#!/usr/bin/env python3
"""
Ultra-Mega Reconciliation: Keep unparseable End Dates, remove only older ones
 - If End Date is parseable & < today => drop row
 - If blank => keep
 - If unparseable => keep row, label it as (unparseable: ???), log it
 - If parseable & future => keep
 - Pivot aggregator merges duplicates so we avoid "cannot reshape" error
 - No "Select All" checkboxes in preview
"""

import os
import sys
import json
import logging
import zipfile
import shutil
import io
from pathlib import Path
from datetime import datetime
from typing import Dict, Set, List, Tuple

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import customtkinter as ctk

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
from matplotlib.backends.backend_pdf import PdfPages

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------------------------------------------------------
# DEFAULT CONFIG & SAVE/LOAD
# ----------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/missing_items.xlsx",
    "PDF_EXPORT_PATH": "output/dashboard_report.pdf",
    "PARAMETER_PATH": "data/parameters.xlsx",

    "CONFIG_PATH": "config/ui_config.json",
    "MASTER_CSV_OUTPUT": "temp_master_csv",
    "LOGO_PATH": "images/company_logo.png",
    "HISTORY_PATH": "history_runs",
    "BAND_CHART_JSON_PATH": "data/bollinger_data.json"
}

def default_config() -> Dict:
    return {
        "paths": DEFAULT_PATHS.copy(),
        "erp_grid": {"filters": {}},
        "master_grid": {"filters": {}},
        "dashboard": {
            "selected_dims": [],
            "selected_attrs": [],
            "top_n": 10
        }
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config => {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        # sets->lists in erp_grid
        if "erp_grid" in cfg and "filters" in cfg["erp_grid"]:
            newf = {}
            for col, svals in cfg["erp_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["erp_grid"]["filters"] = newf

        # sets->lists in master_grid
        if "master_grid" in cfg and "filters" in cfg["master_grid"]:
            newf = {}
            for col, svals in cfg["master_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["master_grid"]["filters"] = newf

        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config => {path}")
    except Exception as e:
        logging.error(f"Error saving config => {e}")


# ----------------------------------------------------------------------------
# TEXT LOGGER
# ----------------------------------------------------------------------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget= widget
    def emit(self, record):
        msg= self.format(record)+ "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")


# ----------------------------------------------------------------------------
# READ PARAM
# ----------------------------------------------------------------------------
def read_param_file(path: Path)-> Dict[str,object]:
    param= {
        "dim_erp_keep": set(),
        "dim_erp_map": {},
        "dim_master_map": {},
        "attr_erp_map": {},
        "attr_master_map": {}
    }
    if not path.is_file():
        logging.warning(f"Param file not found => {path}")
        return param
    try:
        dim_df= pd.read_excel(path, sheet_name="Dimension Parameters")
        dim_df.columns= dim_df.columns.astype(str).str.strip()
        def s(x): return str(x).strip() if pd.notna(x) else ""
        for _, row in dim_df.iterrows():
            fn= s(row.get("FileName",""))
            vsc= s(row.get("V S C",""))
            dim= s(row.get("Dimension",""))
            ev= s(row.get("ERP Values",""))
            if ev.lower()=="x" and vsc and dim:
                param["dim_erp_keep"].add(vsc)
            if vsc and dim:
                param["dim_erp_map"][vsc]= dim
            if fn and dim and ev.lower()=="x":
                param["dim_master_map"][fn]= dim

        attr_df= pd.read_excel(path, sheet_name="Attribute Parameters")
        attr_df.columns= attr_df.columns.astype(str).str.strip()
        for _, row in attr_df.iterrows():
            e_orig= s(row.get("ERP Original Attributes",""))
            m_orig= s(row.get("Master Original Attributes",""))
            final_= s(row.get("Attribute",""))
            onoff= s(row.get("On/Off",""))
            if onoff.lower()=="x" and final_:
                if e_orig:
                    param["attr_erp_map"][e_orig]= final_
                if m_orig:
                    param["attr_master_map"][m_orig]= final_
        return param
    except Exception as e:
        logging.error(f"Error reading param => {e}")
        return param


# ----------------------------------------------------------------------------
# ERP
# ----------------------------------------------------------------------------
def read_erp_excel(path: Path)-> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"ERP Excel not found => {path}")
        return pd.DataFrame()
    try:
        df= pd.read_excel(path, skiprows=3)
        df.columns= df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df= df[df["Enabled_Flag"]=="Enabled"]
        return df
    except Exception as e:
        logging.error(f"Error reading ERP => {e}")
        return pd.DataFrame()


# ----------------------------------------------------------------------------
# MASTER
# ----------------------------------------------------------------------------
def try_read_csv_bytes(raw: bytes)-> pd.DataFrame:
    encs= ["utf-8-sig","utf-16-le","utf-16-be","cp1252","latin-1","ascii"]
    for enc in encs:
        try:
            buf= io.BytesIO(raw)
            df= pd.read_csv(buf, encoding=enc, on_bad_lines="skip", engine="python")
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            df.columns= df.columns.astype(str).str.strip()
            if "Name" not in df.columns and len(df.columns)>0:
                fc= df.columns[0]
                df.rename(columns={fc:"Name"}, inplace=True)
            return df
        except Exception as e:
            logging.debug(f"try_read_csv_bytes => enc={enc} => fail => {e}")
    logging.error("All encodings failed => empty DF.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path)-> List[Path]:
    if not zip_path.is_file():
        logging.warning(f"[Master] ZIP not found => {zip_path}")
        return []
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)

    csvs=[]
    with zipfile.ZipFile(zip_path,"r") as z:
        txt_files= [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            bn= os.path.basename(txt_file)
            if not bn:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw= fo.read()
                df= try_read_csv_bytes(raw)
                if df.empty:
                    continue
                df["RawFileName"]= bn
                out_csv= out_dir / (bn.replace(".txt",".csv"))
                df.to_csv(out_csv, index=False, encoding="utf-8")
                csvs.append(out_csv)
            except Exception as e:
                logging.error(f"[Master] reading {txt_file} => {e}")
    return csvs

def unify_master_csvs(csvs: List[Path])-> pd.DataFrame:
    frames=[]
    for cp in csvs:
        if not cp.is_file():
            continue
        try:
            df= pd.read_csv(cp, encoding="utf-8", on_bad_lines="skip")
            df.columns= df.columns.str.strip()
            frames.append(df)
        except Exception as e:
            logging.error(f"[unify_master_csvs] => {cp} => {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()


# ----------------------------------------------------------------------------
# END DATE FILTERING
# ----------------------------------------------------------------------------
def parse_future_or_keep_unparseable(raw_val: str)-> str:
    """
    Returns:
     - "" if blank
     - "YYYY-MM-DD" if parseable & future
     - "(unparseable: VAL)" if we can't parse => keep
     - "(past: 2023-02-19)" if parseable but older => a sentinel so we can remove row
    """
    s= raw_val.strip()
    if not s:
        return ""  # blank => keep
    try:
        dt= pd.to_datetime(s, errors="raise")
        # if dt < today => we'll mark as (past: iso)
        today= datetime.now().replace(hour=0,minute=0,second=0,microsecond=0)
        if dt<= today:
            # older => remove
            return f"(past: {dt.strftime('%Y-%m-%d')})"
        else:
            return dt.strftime("%Y-%m-%d")
    except Exception as e:
        # can't parse => keep => label
        logging.info(f"[parse_future_or_keep_unparseable] cannot parse '{s}' => keep => (unparseable: {s})")
        return f"(unparseable: {s})"

def remove_past_end_date_rows(df: pd.DataFrame)-> pd.DataFrame:
    """
    If 'End Date' in columns => parse it => if result => (past: ???) => remove row
    otherwise keep (including unparseable or future or blank)
    """
    if "End Date" not in df.columns:
        return df
    newvals=[]
    for val in df["End Date"]:
        if pd.isna(val):
            val_str= ""
        else:
            val_str= str(val)
        new_end= parse_future_or_keep_unparseable(val_str)
        newvals.append(new_end)
    df["End Date"]= newvals

    # remove rows with End Date startswith "(past:"
    def is_past(x):
        return isinstance(x,str) and x.startswith("(past:")
    df_f= df[~df["End Date"].apply(is_past)]
    return df_f


# ----------------------------------------------------------------------------
# MELTDOWN => aggregator => pivot
# ----------------------------------------------------------------------------
def meltdown_and_filter(df: pd.DataFrame,
                        keep_map: Dict[str,str],
                        attr_map: Dict[str,str],
                        dim_col: str,
                        filename_col: str=None):
    """
    1) Filter rows if 'filename_col' given => keep only in keep_map
    2) meltdown => (Dimension, Name, Attribute, Value)
    3) aggregator => merge duplicates => pivot
    4) remove older end date => keep future or unparseable
    """
    if filename_col and filename_col not in df.columns:
        return pd.DataFrame()
    if dim_col not in df.columns:
        return pd.DataFrame()

    if filename_col:
        df2= df[df[filename_col].isin(keep_map.keys())].copy()
    else:
        df2= df.copy()
    if df2.empty:
        return pd.DataFrame()

    if filename_col:
        df2["DimRaw"]= df2[filename_col]
    else:
        df2["DimRaw"]= df2[dim_col]

    meltdown_cols=[]
    skip= {filename_col,dim_col,"DimRaw"}
    if "Name" in df2.columns:
        skip.add("Name")
        id_vars= ["DimRaw","Name"]
    else:
        id_vars= ["DimRaw"]
    for c in df2.columns:
        if c not in skip:
            meltdown_cols.append(c)

    melted= df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    def rename_dim(x):
        if filename_col:
            return keep_map.get(x, x)
        else:
            return keep_map.get(x, x)
    melted["Dimension"]= melted["DimRaw"].apply(rename_dim)
    if "Name" not in id_vars:
        melted["Name"]= ""

    melted= melted[melted["OrigAttr"].isin(attr_map.keys())].copy()
    melted["Attribute"]= melted["OrigAttr"].map(attr_map)
    melted["Value"]= melted["ValX"].fillna("").astype(str).str.strip()

    # aggregator step => group by (Dimension, Name, Attribute) & unify "Value"
    # to avoid pivot duplicates
    group_cols= ["Dimension","Name","Attribute"]
    def unify_values(vals: pd.Series)-> str:
        # remove duplicates, join by "; "
        uniq= vals.dropna().unique()
        if len(uniq)<=1:
            return uniq[0] if len(uniq)>0 else ""
        return "; ".join(uniq)

    grouped= melted.groupby(group_cols, as_index=False).agg({"Value": unify_values})

    # pivot
    try:
        wide= grouped.pivot(index=["Dimension","Name"], columns="Attribute", values="Value").reset_index()
    except Exception as e:
        logging.error(f"[meltdown_and_filter] pivot error => {e}")
        return pd.DataFrame()

    # remove older end date => keep future/unparseable/blank
    wide= remove_past_end_date_rows(wide)
    return wide


# ----------------------------------------------------------------------------
# MAIN meltdown ERP / meltdown MASTER
# ----------------------------------------------------------------------------
def meltdown_erp_for_preview(df: pd.DataFrame, param: Dict[str,object]) -> pd.DataFrame:
    keep= param["dim_erp_keep"]
    dmap= param["dim_erp_map"]
    amap= param["attr_erp_map"]

    # We must have a column "V_S_C" => filter => meltdown
    if "V_S_C" not in df.columns:
        return pd.DataFrame()
    df2= df[df["V_S_C"].isin(keep)].copy()
    if df2.empty:
        return pd.DataFrame()

    # We'll rename "V_S_C" => "DimRaw" in aggregator, so pass keep_map & attr_map
    # But aggregator function expects: meltdown_and_filter(df, keep_map, attr_map, dim_col, filename_col=None)
    # We'll pass dim_col="V_S_C", keep_map=dmap. That means it looks up dmap.get(V_S_C)
    # so we convert V_S_C => the final dimension name
    # aggregator merges duplicates => pivot => remove older End Date
    out_df= meltdown_and_filter(
        df2, dmap, amap,
        dim_col="V_S_C",
        filename_col=None
    )
    return out_df

def meltdown_master_for_preview(df: pd.DataFrame, param: Dict[str,object]) -> pd.DataFrame:
    keep_map= param["dim_master_map"]
    amap= param["attr_master_map"]

    # pass meltdown_and_filter => with dim_col= "RawFileName", filename_col= "RawFileName"
    if "RawFileName" not in df.columns:
        return pd.DataFrame()
    out_df= meltdown_and_filter(
        df, keep_map, amap,
        dim_col="RawFileName",
        filename_col="RawFileName"
    )
    return out_df


# ----------------------------------------------------------------------------
# COMPARISON => 2 sheet
# ----------------------------------------------------------------------------
def meltdown_to_long(df_wide: pd.DataFrame)-> pd.DataFrame:
    if df_wide.empty or {"Dimension","Name"}.difference(df_wide.columns):
        return pd.DataFrame()
    meltdown_cols= [c for c in df_wide.columns if c not in ("Dimension","Name")]
    melted= df_wide.melt(
        id_vars=["Dimension","Name"],
        value_vars= meltdown_cols,
        var_name="Attribute",
        value_name="Value"
    )
    melted["Value"]= melted["Value"].fillna("")
    return melted

from typing import Tuple

def compare_2sheet(
    erp_long: pd.DataFrame,
    mast_long: pd.DataFrame
)-> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    mismatch => Gap In= MASTER/ERP/BOTH
    case_diff => Gap In= CASE
    """
    def to_dict(d):
        out={}
        for (dim,nm), grp in d.groupby(["Dimension","Name"]):
            a_map={}
            for _, row in grp.iterrows():
                attr= row["Attribute"].strip()
                val= str(row["Value"]).strip()
                a_map[attr]= val
            out[(dim,nm)]= a_map
        return out

    e_dict= to_dict(erp_long)
    m_dict= to_dict(mast_long)
    all_dimname= set(e_dict.keys())| set(m_dict.keys())
    mismatch_records=[]
    case_records=[]
    for dm in all_dimname:
        dim,nm= dm
        e_attrs= e_dict.get(dm,{})
        m_attrs= m_dict.get(dm,{})
        all_attrs= set(e_attrs.keys())| set(m_attrs.keys())
        for at in all_attrs:
            e_val= e_attrs.get(at,"")
            m_val= m_attrs.get(at,"")
            if m_val== e_val:
                continue
            # check case diff
            if m_val.lower()== e_val.lower() and m_val and e_val:
                key= f"{dim}|{nm}|{at}|{m_val}|{e_val}".upper()
                case_records.append({
                    "Key": key,
                    "Dimension": dim,
                    "Name": nm,
                    "Attribute": at,
                    "Master": m_val,
                    "ERP": e_val,
                    "Comments_1": "",
                    "Comments_2": "",
                    "Gap In": "CASE"
                })
                continue
            # mismatch
            if not m_val and e_val:
                gap_in= "MASTER"
                ms= ""
                es= e_val
            elif m_val and not e_val:
                gap_in= "ERP"
                ms= m_val
                es= ""
            else:
                gap_in= "BOTH"
                ms= m_val
                es= e_val
            key= f"{dim}|{nm}|{at}|{ms}|{es}".upper()
            mismatch_records.append({
                "Key": key,
                "Dimension": dim,
                "Name": nm,
                "Attribute": at,
                "Master": ms,
                "ERP": es,
                "Comments_1": "",
                "Comments_2": "",
                "Gap In": gap_in
            })
    mismatch_df= pd.DataFrame(mismatch_records)
    case_df= pd.DataFrame(case_records)
    return mismatch_df, case_df


def read_exception_table(path: Path)-> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"Exception table not found => {path}")
        return pd.DataFrame()
    try:
        df= pd.read_excel(path)
        df.columns= df.columns.astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"Error reading exception => {e}")
        return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame)-> pd.DataFrame:
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep= [c for c in ["Key","Comments_1","Comments_2","hide exception"] if c in df_exc.columns]
    if not keep:
        return df
    exc= df_exc[keep].copy()
    exc["Key"]= exc["Key"].astype(str).str.strip()
    merged= df.merge(exc, on="Key", how="left", suffixes=("","_exc"))
    merged["hide exception"]= merged.get("hide exception","").fillna("").str.lower()
    final= merged[merged["hide exception"]!="yes"].copy()

    if "Comments_1_exc" in final.columns:
        final["Comments_1"]= np.where(final["Comments_1_exc"].notna(),
                                      final["Comments_1_exc"],
                                      final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"]= np.where(final["Comments_2_exc"].notna(),
                                      final["Comments_2_exc"],
                                      final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)
    return final

def write_2sheet_excel(mismatch_df: pd.DataFrame, case_df: pd.DataFrame, out_path: Path):
    if mismatch_df.empty and case_df.empty:
        logging.info("No mismatches => skip writing excel.")
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)
    columns= ["Key","Dimension","Name","Attribute","Master","ERP","Comments_1","Comments_2","Gap In"]
    wb= Workbook()
    ws_m= wb.active
    ws_m.title="Mismatch"
    ws_m.append(columns)
    for rowvals in mismatch_df[columns].itertuples(index=False):
        ws_m.append(rowvals)

    ws_c= wb.create_sheet("Case_Differences")
    ws_c.append(columns)
    for rowvals in case_df[columns].itertuples(index=False):
        ws_c.append(rowvals)

    header_font= Font(bold=True)
    fill= PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
    for sheet in [ws_m, ws_c]:
        for cell in sheet[1]:
            cell.font= header_font
            cell.fill= fill
            cell.alignment= Alignment(horizontal="center")

        for col in sheet.columns:
            max_len=0
            letter= col[0].column_letter
            for cell in col:
                val= str(cell.value) if cell.value else ""
                max_len= max(max_len, len(val))
            sheet.column_dimensions[letter].width= max_len+2

        sheet.freeze_panes="A2"

    wb.save(out_path)
    logging.info(f"Missing items => {out_path}")


# ----------------------------------------------------------------------------
# SIMPLE PREVIEW
# ----------------------------------------------------------------------------
class SimplePreview(ctk.CTkFrame):
    """
    Just a read-only table => we show the final meltdown data
    """
    def __init__(self, parent, name: str, filters_dict=None):
        super().__init__(parent)
        self.name= name
        self.df= pd.DataFrame()
        self.filters = {}  # not used now

        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        bar= ctk.CTkFrame(self, corner_radius=10, fg_color="#f0f0f0")
        bar.pack(fill="x", padx=5, pady=5)
        ctk.CTkLabel(
            bar, text=f"{self.name} Preview",
            fg_color="#800020", corner_radius=8,
            text_color="white",
            font=ctk.CTkFont(size=14, weight="bold")
        ).pack(side="left", padx=5)

    def create_table(self):
        container= ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree= ttk.Treeview(container, show="headings")
        vsb= ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb= ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")
        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

    def create_statusbar(self):
        self.status_label= ctk.CTkLabel(self, text="0 rows", fg_color="#f0f0f0", text_color="black")
        self.status_label.pack(fill="x")

    def set_data(self, df: pd.DataFrame):
        self.df= df.copy()
        self.df= self.df.fillna("")
        self.refresh_table()

    def refresh_table(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if self.df.empty:
            self.tree["columns"]=[]
            self.status_label.configure(text="0 rows")
            return

        cols= list(self.df.columns)
        self.tree["columns"]= cols
        for c in cols:
            self.tree.heading(c, text=c, anchor="w")
            self.tree.column(c, anchor="w", width=150)

        for _, row in self.df.iterrows():
            rowvals= [row[c] for c in cols]
            self.tree.insert("", "end", values=rowvals)
        self.status_label.configure(text=f"{len(self.df)} rows")

    def get_filtered_df(self)-> pd.DataFrame:
        return self.df.copy()


# ----------------------------------------------------------------------------
# PDF & Dashboard
# ----------------------------------------------------------------------------
class EnhancedPDFReport:
    def __init__(self, df_current: pd.DataFrame, df_history: pd.DataFrame, config: Dict):
        self.df_current = df_current
        self.df_history = df_history
        self.config = config
        self.page_count = 0
        self.colors = {
            'primary': '#800020',
            'text': '#2C1810',
            'background': '#FFFFFF'
        }
        self.logo_path = self.config["paths"].get("LOGO_PATH","images/company_logo.png")

        self.PAGE_WIDTH= 8.5
        self.PAGE_HEIGHT= 11
        self.CHART_SCALE= 0.7

    def generate(self)-> Path:
        pdf_path= self._get_pdf_path()
        with PdfPages(pdf_path) as pdf:
            self._cover_page(pdf)
            self._summary_page(pdf)
            self._topdimsattrs_page(pdf)
            self._all_charts(pdf)
        logging.info(f"PDF => {pdf_path}")
        return pdf_path

    def _get_pdf_path(self)-> Path:
        stamp= datetime.now().strftime("%Y%m%d_%H%M%S")
        out_dir= Path("Reconciliation_pdf")
        out_dir.mkdir(parents=True, exist_ok=True)
        fname= f"Reconciliationpdf_{stamp}.pdf"
        return out_dir / fname

    def _new_page(self)-> plt.Figure:
        self.page_count+=1
        fig= plt.figure(figsize=(self.PAGE_WIDTH,self.PAGE_HEIGHT))
        fig.patch.set_facecolor(self.colors['background'])
        plt.axis('off')
        if self.logo_path and os.path.exists(self.logo_path):
            try:
                import matplotlib.image as mpimg
                img= mpimg.imread(self.logo_path)
                ax_img= fig.add_axes([0.65,0.65,0.25,0.25], anchor='NE', zorder=10)
                ax_img.imshow(img, alpha=0.2)
                ax_img.axis('off')
            except Exception as e:
                logging.error(f"Logo => {e}")

        fig.text(0.5,0.97,"Reconciliation Report", ha='center', fontsize=10, color='gray')
        fig.text(0.9,0.03,f"Page {self.page_count}", ha='right', fontsize=8, color='gray')
        fig.text(0.5,0.02,"© Ultra-Mega Reconciliation", ha='center', fontsize=8, color='gray')
        return fig

    def _cover_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5,0.7,"Reconciliation Analysis Report",
                 ha='center', fontsize=24, fontweight='bold', color=self.colors['primary'],
                 transform=fig.transFigure)
        plt.text(0.5,0.6,f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                 ha='center', fontsize=12, color=self.colors['text'],
                 transform=fig.transFigure)
        plt.text(0.5,0.1,"CONFIDENTIAL",
                 ha='center',fontsize=9,color=self.colors['text'],
                 transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _summary_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5,0.92,"Reconciliation Summary", ha='center',
                 fontsize=18,fontweight='bold',color=self.colors['primary'],
                 transform=fig.transFigure)
        y=0.75
        if self.df_current.empty:
            plt.text(0.5,y,"No mismatches found this run.",
                     ha='center', fontsize=14,color=self.colors['text'],
                     transform=fig.transFigure)
        else:
            total= len(self.df_current)
            c_master= (self.df_current["Gap In"]=="MASTER").sum()
            c_erp= (self.df_current["Gap In"]=="ERP").sum()
            c_both= (self.df_current["Gap In"]=="BOTH").sum()
            summary= f"Total: {total}\nMaster: {c_master}\nERP: {c_erp}\nBoth: {c_both}"
            plt.text(0.5,y,summary,ha='center',fontsize=14,color=self.colors['text'],
                     transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _topdimsattrs_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5,0.92,"Top Dimensions & Attributes", ha='center',
                 fontsize=18,fontweight='bold',color=self.colors['primary'],
                 transform=fig.transFigure)
        if self.df_current.empty:
            plt.text(0.5,0.7,"No data available.",ha='center',fontsize=12,color=self.colors['text'],
                     transform=fig.transFigure)
        else:
            if "Dimension" in self.df_current.columns:
                dims= self.df_current["Dimension"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in dims.items()]
                plt.text(0.2,0.7,"Top Dimensions:\n"+ "\n".join(lines),
                         fontsize=12, color=self.colors['text'], transform=fig.transFigure)
            if "Attribute" in self.df_current.columns:
                attrs= self.df_current["Attribute"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in attrs.items()]
                plt.text(0.6,0.7,"Top Attributes:\n"+ "\n".join(lines),
                         fontsize=12, color=self.colors['text'], transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _chart_page(self,pdf: PdfPages, title:str, plot_func, **kwargs):
        fig= self._new_page()
        fig.suptitle(title, fontsize=14,fontweight='bold', color=self.colors['primary'], y=0.93)
        w_inch= self.PAGE_WIDTH* self.CHART_SCALE
        if "Heatmap" in title:
            h_inch= w_inch*1.4
            left_margin= (self.PAGE_WIDTH- w_inch)*0.5+0.4
        else:
            h_inch= w_inch*(9.0/16.0)
            left_margin= (self.PAGE_WIDTH- w_inch)*0.5+0.2
        bottom_margin= (self.PAGE_HEIGHT- h_inch)*0.5
        left_rel= left_margin/ self.PAGE_WIDTH
        bottom_rel= bottom_margin/ self.PAGE_HEIGHT
        width_rel= w_inch/ self.PAGE_WIDTH
        height_rel= h_inch/ self.PAGE_HEIGHT
        ax_rect= [left_rel, bottom_rel, width_rel, height_rel]
        ax= fig.add_axes(ax_rect)
        try:
            plot_func(ax, **kwargs)
            pdf.savefig(fig)
        except Exception as e:
            logging.error(f"{title} => {e}")
        finally:
            plt.close(fig)

    def _all_charts(self,pdf: PdfPages):
        dfc= self.df_current.copy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        # Heatmap
        if not df_m.empty and {"Dimension","Attribute"}.issubset(df_m.columns):
            pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
            if not pivot.empty:
                self._chart_page(pdf, "Heatmap", self._plot_heatmap, pivot=pivot)
        # Lollipop
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim.empty:
            self._chart_page(pdf,"Lollipop", self._plot_lollipop, cdim=cdim)
        # Circular
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr.empty:
            self._chart_page(pdf,"Circular", self._plot_circular, cattr=cattr)
        # Scatter
        cdim_sc= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim_sc.sort_values("Count", ascending=False, inplace=True)
        cdim_sc= cdim_sc.head(10)
        if not cdim_sc.empty:
            self._chart_page(pdf,"Scatter", self._plot_scatter, cdim=cdim_sc)
        # Radar
        cdim_ra= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim_ra.empty and len(cdim_ra)>1:
            self._chart_page(pdf,"Radar", self._plot_radar, cdim=cdim_ra)
        # Pie
        dist= df_m["Gap In"].value_counts()
        if not dist.empty:
            self._chart_page(pdf,"Pie: Gap In distribution", self._plot_pie, dist=dist)
        # Bar
        cattr_b= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr_b.empty:
            self._chart_page(pdf,"Bar: Mismatch attributes", self._plot_bar, cattr=cattr_b)
        # Bollinger
        if not self.df_history.empty and "RunDate" in self.df_history.columns:
            date_ct= self.df_history.groupby("RunDate")["Key"].count().reset_index(name="Count")
            date_ct.sort_values("RunDate", inplace=True)
            if not date_ct.empty:
                self._chart_page(pdf,"Bollinger Band Over Time", self._plot_bollinger, date_ct=date_ct)

    # plot helpers
    def _plot_heatmap(self, ax, pivot):
        im= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=45, ha="right")
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        plt.colorbar(im, ax=ax)

    def _plot_lollipop(self, ax, cdim):
        ax.hlines(y= cdim.index, xmin=0, xmax= cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_xlabel("Count")

    def _plot_circular(self, ax, cattr):
        angles= np.linspace(0,2*np.pi, len(cattr), endpoint=False)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)

    def _plot_scatter(self, ax, cdim):
        xvals= np.arange(len(cdim))
        yvals= cdim.values
        ax.scatter(xvals, yvals, color="green")
        for i, idx in enumerate(cdim.index):
            ax.text(xvals[i], yvals[i], idx, ha="center", va="bottom", rotation=60, fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("Mismatch Count")

    def _plot_radar(self, ax, cdim):
        cat= cdim.index.tolist()
        val= cdim.values.tolist()
        angles= np.linspace(0, 2*np.pi, len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)

    def _plot_pie(self, ax, dist):
        ax.pie(dist.values, labels= dist.index, autopct="%.1f%%", startangle=140)

    def _plot_bar(self, ax, cattr):
        bars= ax.bar(range(len(cattr)), cattr.values, color="blue")
        ax.set_xticks(range(len(cattr)))
        ax.set_xticklabels(cattr.index, rotation=45, ha="right", fontsize=8)
        ax.set_ylabel("Count")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, f"{int(h)}", ha="center", va="bottom")

    def _plot_bollinger(self, ax, date_ct):
        date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)
        date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
        date_ct["rolling_std"] = date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]  = date_ct["rolling_mean"]+ 2* date_ct["rolling_std"]
        date_ct["lower_band"]  = date_ct["rolling_mean"]- 2* date_ct["rolling_std"]

        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"],
                        color="blue", alpha=0.2, label="±2σ Band")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Actual Count")
        ax.set_xticks(xvals)
        xlabels= [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Mismatch Count")
        ax.set_title("Bollinger Band Over Time")
        ax.legend()


# ----------------------------------------------------------------------------
# ADVANCED DASHBOARD
# ----------------------------------------------------------------------------
class AdvancedDashboard(ctk.CTkFrame):
    def __init__(self, parent, config: Dict):
        super().__init__(parent)
        self.config= config
        dash_cfg= config.get("dashboard",{})
        self.selected_dims: Set[str]= set(dash_cfg.get("selected_dims",[]))
        self.selected_attrs: Set[str]= set(dash_cfg.get("selected_attrs",[]))
        self.top_n= dash_cfg.get("top_n",10)

        self.df_current= pd.DataFrame()
        self.df_history= pd.DataFrame()

        self.topbar= ctk.CTkScrollableFrame(self, orientation="horizontal", height=60)
        self.topbar.pack(fill="x", pady=5)
        self.metric_label= ctk.CTkLabel(self.topbar, text="Metrics: 0 mismatch, 0 dimension", width=300)
        self.metric_label.pack(side="left", padx=5)

        ctk.CTkButton(
            self.topbar, text="Filter Dimension", command=self.show_dimension_filter,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)
        ctk.CTkButton(
            self.topbar, text="Filter Attribute", command=self.show_attribute_filter,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)
        ctk.CTkButton(
            self.topbar, text="Toggle Top 10 / All", command=self.toggle_top_n,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)
        self.frames= {}
        chart_names= [
            "Heatmap","Lollipop","Circular","Scatter","Radar",
            "Normal Pie","Normal Bar","Bollinger Chart"
        ]
        for lbl in chart_names:
            fr= ctk.CTkFrame(self.notebook)
            fr.pack(fill="both", expand=True)
            self.notebook.add(fr, text=lbl)
            self.frames[lbl]= fr

    def show_dimension_filter(self):
        self.show_filter_popup("Dimension")

    def show_attribute_filter(self):
        self.show_filter_popup("Attribute")

    def toggle_top_n(self):
        if self.top_n==10:
            self.top_n= None
        else:
            self.top_n= 10
        self.update_data_filters()

    def show_filter_popup(self, col:str):
        base_df= self.df_history if not self.df_history.empty else self.df_current
        if base_df.empty or col not in base_df.columns:
            return
        popup= tk.Toplevel(self)
        popup.title(f"Filter: {col}")
        popup.geometry("300x400")
        frame= ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)
        unique_vals= base_df[col].dropna().unique()
        sorted_vals= sorted(unique_vals, key=lambda x: str(x).lower())
        if col=="Dimension":
            curr= self.selected_dims
        else:
            curr= self.selected_attrs
        if not curr:
            curr= set(unique_vals)
        all_vals= set(sorted_vals)
        selall_var= tk.BooleanVar(value=(curr==all_vals or not curr))
        def toggle_all():
            check= selall_var.get()
            for vb in var_dict.values():
                vb.set(check)
        ctk.CTkCheckBox(
            frame, text="Select All", variable=selall_var, command=toggle_all,
            fg_color="#800020", hover_color="#a52a2a", text_color="black"
        ).pack(anchor="w", pady=5)

        scroll= ctk.CTkScrollableFrame(frame, width=250, height=250)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)
        var_dict={}
        for rv in sorted_vals:
            in_filter= (rv in curr) or (not curr)
            bvar= tk.BooleanVar(value=in_filter)
            var_dict[rv]= bvar
            ctk.CTkCheckBox(
                scroll, text=str(rv),
                variable=bvar,
                fg_color="#800020", hover_color="#a52a2a", text_color="black"
            ).pack(anchor="w")

        def apply_():
            sel= {rv for rv,bv in var_dict.items() if bv.get()}
            if col=="Dimension":
                self.selected_dims= sel
            else:
                self.selected_attrs= sel
            popup.destroy()
            self.update_data_filters()

        bf= ctk.CTkFrame(frame)
        bf.pack(fill="x", pady=5)
        ctk.CTkButton(
            bf, text="Apply", command=apply_,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)
        ctk.CTkButton(
            bf, text="Cancel", command=popup.destroy,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

    def update_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current= df_current.copy()
        self.df_history= df_history.copy()
        self.update_data_filters()

    def update_data_filters(self):
        dfc= self.df_current.copy()
        if not dfc.empty:
            if self.selected_dims:
                dfc= dfc[dfc["Dimension"].isin(self.selected_dims)]
            if self.selected_attrs:
                dfc= dfc[dfc["Attribute"].isin(self.selected_attrs)]

        mism= len(dfc)
        dims= dfc["Dimension"].nunique() if not dfc.empty and "Dimension" in dfc.columns else 0
        self.metric_label.configure(text=f"Mismatches: {mism}, Dims: {dims}")

        self.plotHeatmap(dfc)
        self.plotLollipop(dfc)
        self.plotCircular(dfc)
        self.plotScatter(dfc)
        self.plotRadar(dfc)
        self.plotNormalPie(dfc)
        self.plotNormalBar(dfc)
        self.plotBollinger()

    def plot_chart(self, frame, fig):
        for w in frame.winfo_children():
            w.destroy()
        canvas= FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        toolbar_frame= ctk.CTkFrame(frame)
        toolbar_frame.pack(side="top", fill="x")
        toolbar= NavigationToolbar2Tk(canvas, toolbar_frame)
        toolbar.update()
        canvas.get_tk_widget().pack(fill="both", expand=True)
        plt.close(fig)

    def _limit_if_needed(self, series: pd.Series)-> pd.Series:
        if self.top_n==10:
            return series.head(10)
        else:
            return series

    def plotHeatmap(self, dfc: pd.DataFrame):
        fr= self.frames["Heatmap"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty or "Gap In" not in dfc.columns:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty or not {"Dimension","Attribute"}.issubset(df_m.columns):
            return
        pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
        if pivot.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        cax= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=90)
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        fig.colorbar(cax, ax=ax)
        ax.set_title("Heatmap: Mismatch Count")
        self.plot_chart(fr, fig)

    def plotLollipop(self, dfc: pd.DataFrame):
        fr= self.frames["Lollipop"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        cdim= self._limit_if_needed(cdim)
        if cdim.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        ax.hlines(y= cdim.index, xmin=0, xmax= cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_title("Lollipop: Missing by Dimension")
        ax.set_xlabel("Count")
        self.plot_chart(fr, fig)

    def plotCircular(self, dfc: pd.DataFrame):
        fr= self.frames["Circular"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False)
        cattr= self._limit_if_needed(cattr)
        if cattr.empty:
            return
        fig= plt.figure(figsize=(6,6))
        ax= fig.add_subplot(111, polar=True)
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular: Missing Attributes")
        self.plot_chart(fr, fig)

    def plotScatter(self, dfc: pd.DataFrame):
        fr= self.frames["Scatter"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cdim= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim.sort_values("Count", ascending=False, inplace=True)
        if self.top_n==10:
            cdim= cdim.head(10)
        if cdim.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        xvals= np.arange(len(cdim))
        yvals= cdim["Count"].values
        labels= cdim["Dimension"].values
        ax.scatter(xvals,yvals, color="green")
        for i, txt in enumerate(labels):
            ax.text(xvals[i], yvals[i], txt, ha="center", va="bottom", rotation=60)
        ax.set_xticks([])
        ax.set_ylabel("Mismatch Count")
        ax.set_title("Scatter: Mismatch by Dimension")
        self.plot_chart(fr, fig)

    def plotRadar(self, dfc: pd.DataFrame):
        fr= self.frames["Radar"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        if self.top_n==10:
            cdim= cdim.head(10)
        if cdim.empty or len(cdim)<2:
            return
        cat= cdim.index.tolist()
        val= cdim.values.tolist()
        fig= plt.figure(figsize=(6,6))
        angles= np.linspace(0,2*np.pi,len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax= fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)
        ax.set_title("Radar: Mismatch Dims", y=1.08)
        self.plot_chart(fr, fig)

    def plotNormalPie(self, dfc: pd.DataFrame):
        fr= self.frames["Normal Pie"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        dist= df_m["Gap In"].value_counts()
        fig, ax= plt.subplots(figsize=(5,5))
        ax.pie(dist.values, labels= dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Pie: Gap In distribution")
        self.plot_chart(fr, fig)

    def plotNormalBar(self, dfc: pd.DataFrame):
        fr= self.frames["Normal Bar"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cattr= df_m["Attribute"].value_counts().sort_values(ascending=False)
        if self.top_n==10:
            cattr= cattr.head(10)
        if cattr.empty:
            return
        fig, ax= plt.subplots(figsize=(6,4))
        bars= ax.bar(range(len(cattr)), cattr.values, color="blue")
        ax.set_xticks(range(len(cattr)))
        ax.set_xticklabels(cattr.index, rotation=45, ha="right")
        ax.set_ylabel("Count")
        ax.set_title("Bar: Mismatch attributes")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, str(int(h)), ha="center", va="bottom")
        plt.tight_layout()
        self.plot_chart(fr, fig)

    def plotBollinger(self):
        fr= self.frames["Bollinger Chart"]
        for w in fr.winfo_children():
            w.destroy()
        df_hist= self.df_history.copy()
        if df_hist.empty or "RunDate" not in df_hist.columns:
            return
        date_ct= df_hist.groupby("RunDate")["Key"].count().reset_index(name="Count")
        date_ct.sort_values("RunDate", inplace=True)
        if date_ct.empty:
            return
        fig, ax= plt.subplots(figsize=(12,4))
        date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)
        date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
        date_ct["rolling_std"] = date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]  = date_ct["rolling_mean"]+ 2*date_ct["rolling_std"]
        date_ct["lower_band"]  = date_ct["rolling_mean"]- 2*date_ct["rolling_std"]
        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"], color="blue", alpha=0.2, label="±2σ Band")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Actual Count")
        ax.set_xticks(xvals)
        xlabels= [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Mismatch Count")
        ax.set_title("Bollinger Band Over Time (GUI)")
        ax.legend()
        self.plot_chart(fr, fig)


# ----------------------------------------------------------------------------
# HISTORY TAB
# ----------------------------------------------------------------------------
class HistoryTab(ctk.CTkFrame):
    def __init__(self, parent, hist_dir: Path):
        super().__init__(parent)
        self.history_dir= hist_dir
        self.tree= None
        self.build_ui()

    def build_ui(self):
        lbl= ctk.CTkLabel(self, text="Reconciliation Runs History", font=("Arial",16))
        lbl.pack(pady=5)
        self.tree= ttk.Treeview(self, columns=("Filename",), show="headings", height=15)
        self.tree.heading("Filename", text="History File")
        self.tree.pack(fill="both", expand=True, padx=10, pady=10)
        self.tree.bind("<Double-1>", self.on_double_click)
        refresh_btn= ctk.CTkButton(
            self, text="Refresh", command=self.refresh_history,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        )
        refresh_btn.pack(pady=5)
        self.refresh_history()

    def refresh_history(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if not self.history_dir.is_dir():
            self.history_dir.mkdir(parents=True, exist_ok=True)
        files= sorted(self.history_dir.glob("run_*.json"), reverse=True)
        for f in files:
            self.tree.insert("", "end", values=(f.name,))

    def on_double_click(self, event):
        item_id= self.tree.focus()
        if not item_id:
            return
        filename= self.tree.item(item_id,"values")[0]
        path= self.history_dir / filename
        if not path.is_file():
            return
        try:
            with open(path, "r", encoding="utf-8") as ff:
                content= ff.read()
            popup= tk.Toplevel(self)
            popup.title(f"Viewing {filename}")
            txt= ctk.CTkTextbox(popup, width=800, height=600)
            txt.pack(fill="both", expand=True)
            txt.insert("end", content)
            txt.configure(state="disabled")
        except Exception as e:
            logging.error(f"Error opening {path} => {e}")


# ----------------------------------------------------------------------------
# MAIN APP
# ----------------------------------------------------------------------------
class MainApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation => Keep unparseable EndDate, remove older")
        self.geometry("1600x900")
        ctk.set_appearance_mode("light")

        self.protocol("WM_DELETE_WINDOW", self.on_close)

        self.config_dict= load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.param_dict= read_param_file(Path(self.config_dict["paths"].get("PARAMETER_PATH",DEFAULT_PATHS["PARAMETER_PATH"])))
        self.history_df= pd.DataFrame()

        self.tabs= ttk.Notebook(self)
        self.tabs.pack(fill="both", expand=True)

        # 1) Paths
        self.tab_paths= ctk.CTkFrame(self.tabs)
        self.build_paths_tab(self.tab_paths)
        self.tabs.add(self.tab_paths, text="Paths")

        # 2) ERP preview
        self.tab_erp= ctk.CTkFrame(self.tabs)
        erp_filters= self.config_dict.get("erp_grid",{}).get("filters",{})
        self.erp_preview= SimplePreview(self.tab_erp, "ERP", filters_dict=erp_filters)
        self.erp_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_erp, text="ERP Preview")

        # 3) Master preview
        self.tab_master= ctk.CTkFrame(self.tabs)
        mast_filters= self.config_dict.get("master_grid",{}).get("filters",{})
        self.master_preview= SimplePreview(self.tab_master, "Master", filters_dict=mast_filters)
        self.master_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_master, text="Master Preview")

        # 4) Compare
        self.tab_compare= ctk.CTkFrame(self.tabs)
        self.build_compare_tab(self.tab_compare)
        self.tabs.add(self.tab_compare, text="Compare")

        # 5) Dashboard
        self.dashboard_tab= AdvancedDashboard(self.tabs, self.config_dict)
        self.tabs.add(self.dashboard_tab, text="Dashboard")

        # 6) History
        hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))
        self.history_tab= HistoryTab(self.tabs, hist_path)
        self.tabs.add(self.history_tab, text="History")

        # logger
        self.log_box= ctk.CTkTextbox(self, height=120)
        self.log_box.pack(fill="both", side="bottom")
        self.log_box.configure(state="disabled")
        handler= TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        self.temp_csv_dir= Path(self.config_dict["paths"].get("MASTER_CSV_OUTPUT","temp_master_csv"))
        self.temp_csv_dir.mkdir(parents=True, exist_ok=True)

        self.load_all_runs()
        self.refresh_erp()
        self.refresh_master()
        # update dash with old history
        self.dashboard_tab.update_data(pd.DataFrame(), self.history_df)

        ctk.CTkButton(
            self, text="Close", command=self.on_close,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(pady=5)

    def build_paths_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        self.erp_var= tk.StringVar(value= self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mast_var= tk.StringVar(value= self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_var= tk.StringVar(value= self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_var= tk.StringVar(value= self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))
        self.par_var= tk.StringVar(value= self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"]))
        self.pdf_var= tk.StringVar(value= self.config_dict["paths"].get("PDF_EXPORT_PATH", DEFAULT_PATHS["PDF_EXPORT_PATH"]))

        def mkrow(lbl, var, is_dir=False):
            rowf= ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=220).pack(side="left", padx=5)
            e= ctk.CTkEntry(rowf, textvariable=var, width=600)
            e.pack(side="left", padx=5)
            def br():
                if is_dir:
                    p= filedialog.askdirectory()
                else:
                    p= filedialog.askopenfilename()
                if p:
                    var.set(p)
            ctk.CTkButton(rowf, text="Browse", command=br,
                          fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)

        mkrow("ERP Excel:", self.erp_var)
        mkrow("Master ZIP:", self.mast_var)
        mkrow("Exception Path:", self.exc_var)
        mkrow("Missing Items Output:", self.out_var)
        mkrow("Parameter File:", self.par_var)
        mkrow("PDF Export Path:", self.pdf_var)

        bf= ctk.CTkFrame(frm)
        bf.pack(fill="x", pady=10)
        ctk.CTkButton(bf, text="Save Config", command=self.save_all_config,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Refresh ERP", command=self.refresh_erp,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Refresh Master", command=self.refresh_master,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)

    def load_all_runs(self):
        hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))
        if not hist_path.is_dir():
            return
        frames=[]
        for jf in hist_path.glob("run_*.json"):
            try:
                jdata= pd.read_json(jf, orient="records")
                frames.append(jdata)
            except Exception as e:
                logging.error(f"Error reading {jf} => {e}")
        if frames:
            big= pd.concat(frames, ignore_index=True)
            big.drop_duplicates(inplace=True)
            # exclude "CASE" if we only want real mismatch
            big= big[ big["Gap In"]!="CASE" ]
            if self.history_df.empty:
                self.history_df= big
            else:
                self.history_df= pd.concat([self.history_df,big], ignore_index=True)
                self.history_df.drop_duplicates(inplace=True)
            logging.info(f"Loaded all runs => total {len(self.history_df)} mismatch recs.")

    def refresh_erp(self):
        erp_path= Path(self.erp_var.get().strip())
        raw_erp= read_erp_excel(erp_path)
        if raw_erp.empty:
            self.erp_preview.set_data(pd.DataFrame())
            return
        out_df= meltdown_erp_for_preview(raw_erp, self.param_dict)
        self.erp_preview.set_data(out_df)

    def refresh_master(self):
        zip_path= Path(self.mast_var.get().strip())
        out_dir= self.temp_csv_dir
        csvs= convert_master_txt_to_csv(zip_path, out_dir)
        raw_mast= unify_master_csvs(csvs)
        if raw_mast.empty:
            self.master_preview.set_data(pd.DataFrame())
            return
        out_df= meltdown_master_for_preview(raw_mast, self.param_dict)
        self.master_preview.set_data(out_df)

    def build_compare_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Generate Missing Items", font=("Arial",16)).pack(pady=5)
        ctk.CTkButton(
            frm, text="Run Reconciliation", command=self.run_comparison,
            fg_color="#800020", hover_color="#a52a2a"
        ).pack(pady=10)
        ctk.CTkButton(
            frm, text="Export PDF Report", command=self.export_pdf,
            fg_color="#800020", hover_color="#a52a2a"
        ).pack(pady=10)

    def run_comparison(self):
        df_erp_wide= self.erp_preview.get_filtered_df()
        df_mast_wide= self.master_preview.get_filtered_df()
        erp_long= meltdown_to_long(df_erp_wide)
        mast_long= meltdown_to_long(df_mast_wide)
        mismatch_df, case_df= compare_2sheet(erp_long, mast_long)

        exc_path= Path(self.exc_var.get().strip())
        df_exc= read_exception_table(exc_path)
        mismatch_df= merge_exceptions(mismatch_df, df_exc)
        case_df= merge_exceptions(case_df, df_exc)

        out_path= Path(self.out_var.get().strip())
        write_2sheet_excel(mismatch_df, case_df, out_path)

        run_timestamp= datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        mismatch_df["RunDate"]= run_timestamp
        dash_df= mismatch_df.copy()

        if self.history_df.empty:
            self.history_df= dash_df.copy()
        else:
            self.history_df= pd.concat([self.history_df, dash_df], ignore_index=True)
            self.history_df.drop_duplicates(inplace=True)

        hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))
        hist_path.mkdir(parents=True, exist_ok=True)
        run_file= hist_path / f"run_{run_timestamp.replace(':','-').replace(' ','_')}.json"
        try:
            dash_df.to_json(run_file, orient="records", indent=2)
            logging.info(f"Saved run => {run_file}")
        except Exception as e:
            logging.error(f"Error writing JSON => {e}")

        self.dashboard_tab.update_data(dash_df, self.history_df)
        self.history_tab.refresh_history()
        self.tabs.select(self.dashboard_tab)
        messagebox.showinfo("Done", f"Missing items => {out_path}")

    def export_pdf(self):
        if self.history_df.empty:
            messagebox.showinfo("PDF Export","No mismatch => history empty.")
            return
        if "RunDate" in self.history_df.columns:
            last_run= self.history_df["RunDate"].max()
            df_current= self.history_df[self.history_df["RunDate"]== last_run].copy()
        else:
            df_current= self.history_df.copy()
        df_history= self.history_df.copy()
        rep= EnhancedPDFReport(df_current, df_history, self.config_dict)
        pdf_path= rep.generate()
        messagebox.showinfo("PDF Export", f"PDF => {pdf_path}")

    def save_all_config(self):
        self.config_dict["paths"]["ERP_EXCEL_PATH"]= self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mast_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"]= self.par_var.get().strip()
        self.config_dict["paths"]["PDF_EXPORT_PATH"]= self.pdf_var.get().strip()

        self.config_dict.setdefault("erp_grid", {})
        self.config_dict["erp_grid"]["filters"]= self.erp_preview.filters
        self.config_dict.setdefault("master_grid", {})
        self.config_dict["master_grid"]["filters"]= self.master_preview.filters

        dash_cfg= self.config_dict.setdefault("dashboard", {})
        dash_cfg["selected_dims"]= list(self.dashboard_tab.selected_dims)
        dash_cfg["selected_attrs"]= list(self.dashboard_tab.selected_attrs)
        dash_cfg["top_n"]= self.dashboard_tab.top_n

        cfg_path= Path(self.config_dict["paths"]["CONFIG_PATH"])
        save_config(self.config_dict, cfg_path)

    def on_close(self):
        self.save_all_config()
        band_path= self.config_dict["paths"].get("BAND_CHART_JSON_PATH","")
        if band_path and not self.history_df.empty and "RunDate" in self.history_df.columns:
            try:
                outp= Path(band_path)
                date_ct= self.history_df.groupby("RunDate")["Key"].count().reset_index(name="Count")
                date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
                date_ct.sort_values("RunDate_dt", inplace=True)
                date_ct.reset_index(drop=True, inplace=True)
                date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
                date_ct["rolling_std"]= date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
                date_ct["upper_band"]= date_ct["rolling_mean"]+ 2* date_ct["rolling_std"]
                date_ct["lower_band"]= date_ct["rolling_mean"]- 2* date_ct["rolling_std"]
                date_ct["RunDate"]= date_ct["RunDate_dt"].dt.strftime("%Y-%m-%d %H:%M:%S")
                date_ct.drop(columns=["RunDate_dt"], inplace=True)
                date_ct.to_json(outp, orient="records", indent=2)
                logging.info(f"Bollinger => {outp}")
            except Exception as e:
                logging.error(f"Bollinger => {e}")
        self.destroy()


def main():
    app= MainApp()
    app.mainloop()

if __name__=="__main__":
    main()
