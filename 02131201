"""
Ultra-Mega Reconciliation: Mode=2, Parameter-based with two-sheet Parameters

Key Fixes Implemented:
• ERP Processing: Reads starting at row 4; uses V_S_C (or V S C) as dimension; uses Value as record name; melts attributes.
• Master Processing: Uses full filename as dimension; first column is Name; applies parameter filtering/renaming.
• Parameter Handling: Two sheets – “Dimension Parameters” and “Attribute Parameters” – filter and rename dimensions and attributes.
• GUI: The ExcelGrid now only allows filtering on Start Date and End Date (with multi-select including NaN), and the main window uses a burgundy, translucent background.
"""

import os, json, logging, zipfile, shutil
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime

import tkinter as tk
from tkinter import ttk, filedialog, messagebox, simpledialog
import customtkinter as ctk
import pandas as pd
import numpy as np

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# ---------------- LOGGING ----------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ---------------- DEFAULT CONFIG ----------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/Reconciliation.xlsx",
    "CONFIG_PATH": "config/ui_config.json",
    "PARAMETER_PATH": "data/parameters.xlsx",
    "MASTER_CSV_OUTPUT": "temp_master_csv"
}

def default_config() -> Dict:
    return {
        "paths": DEFAULT_PATHS.copy(),
        "comparison_option": 2
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config: {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# ---------------- TEXT LOGGER HANDLER ----------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget
    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")

# ---------------- PARAMETERS FILE READING ----------------
def read_parameter_file(path: Path) -> Dict[str, object]:
    """
    Expects an Excel file with two sheets:
    1) "Dimension Parameters": columns: FileName, V S C, Dimension, ERP Values.
       - For rows where ERP Values == 'x', builds:
            • dimension_params.erp_vsc_map (for ERP, key=V S C)
            • dimension_params.master_file_map (for Master, key=FileName)
    2) "Attribute Parameters": columns: ERP Original Attributes, Master Original Attributes, Attribute, On/Off.
       - For rows where On/Off == 'x', builds:
            • attribute_params.erp_attr_map (for ERP)
            • attribute_params.master_attr_map (for Master)
    """
    param = {
        "dimension_params": {
            "erp_vsc_map": {},
            "master_file_map": {}
        },
        "attribute_params": {
            "erp_attr_map": {},
            "master_attr_map": {}
        }
    }
    if not path.is_file():
        logging.warning(f"Parameter file not found: {path}")
        return param

    try:
        # Dimension Parameters
        df_dim = pd.read_excel(path, sheet_name="Dimension Parameters")
        df_dim.columns = df_dim.columns.astype(str).str.strip()
        for _, row in df_dim.iterrows():
            file_ = str(row.get("FileName", "")).strip()
            vsc = str(row.get("V S C", "")).strip()
            final_dim = str(row.get("Dimension", "")).strip()
            erp_val = str(row.get("ERP Values", "")).strip()
            if erp_val.lower() == "x" and final_dim:
                if vsc:
                    param["dimension_params"]["erp_vsc_map"][vsc] = final_dim
                if file_:
                    param["dimension_params"]["master_file_map"][file_] = final_dim

        # Attribute Parameters
        df_attr = pd.read_excel(path, sheet_name="Attribute Parameters")
        df_attr.columns = df_attr.columns.astype(str).str.strip()
        for _, row in df_attr.iterrows():
            erp_orig = str(row.get("ERP Original Attributes", "")).strip()
            master_orig = str(row.get("Master Original Attributes", "")).strip()
            final_attr = str(row.get("Attribute", "")).strip()
            on_off = str(row.get("On/Off", "")).strip()
            if on_off.lower() == "x" and final_attr:
                if erp_orig:
                    param["attribute_params"]["erp_attr_map"][erp_orig] = final_attr
                if master_orig:
                    param["attribute_params"]["master_attr_map"][master_orig] = final_attr
        return param
    except Exception as e:
        logging.error(f"Error reading parameter file: {e}")
        return param

# ---------------- ERP READING & MELTDOWN ----------------
def read_erp_enabled(path: Path) -> pd.DataFrame:
    """Read ERP file starting from row 4 and filter for Enabled rows."""
    if not path.is_file():
        logging.warning(f"ERP file not found: {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path, skiprows=3)
        df.columns = df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df = df[df["Enabled_Flag"] == "Enabled"]
        return df
    except Exception as e:
        logging.error(f"Error reading ERP: {e}")
        return pd.DataFrame()

def meltdown_erp(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    """
    Process ERP data:
      1. Identify the ERP dimension column ("V_S_C" or "V S C")
      2. Filter rows to only those allowed by the parameter file (using erp_vsc_map)
      3. Use the Value column as the record's name (RefName)
      4. Melt remaining columns as attributes and filter/rename them per the attribute parameters.
    """
    # Identify ERP dimension column
    vsc_col = None
    for col in ["V_S_C", "V S C"]:
        if col in df.columns:
            vsc_col = col
            break
    if not vsc_col:
        logging.warning("[ERP meltdown] V_S_C column not found")
        return pd.DataFrame()

    # Filter rows using allowed ERP dimensions from parameters
    allowed_dims = set(param["dimension_params"]["erp_vsc_map"].keys())
    df2 = df[df[vsc_col].isin(allowed_dims)].copy()
    if df2.empty:
        logging.warning("[ERP meltdown] No rows after dimension filtering")
        return pd.DataFrame()

    # Prepare for melting: use vsc_col and Value column as id_vars
    id_vars = [vsc_col]
    if "Value" in df2.columns:
        id_vars.append("Value")
    skip_cols = {"Enabled_Flag", vsc_col}
    value_vars = [c for c in df2.columns if c not in skip_cols and c not in id_vars]

    melted = df2.melt(
        id_vars=id_vars,
        value_vars=value_vars,
        var_name="Attribute",
        value_name="AttributeValue"
    )

    # Map ERP dimension
    melted["Dimension"] = melted[vsc_col].map(param["dimension_params"]["erp_vsc_map"])
    melted["RefName"] = melted["Value"] if "Value" in melted.columns else ""

    # Filter and rename attributes based on Attribute Parameters (ERP)
    allowed_attrs = set(param["attribute_params"]["erp_attr_map"].keys())
    melted = melted[melted["Attribute"].isin(allowed_attrs)]
    melted["Attribute"] = melted["Attribute"].map(param["attribute_params"]["erp_attr_map"])

    # Handle dates: strip any trailing 'T' portion if present
    def strip_t(val):
        if isinstance(val, str) and "T" in val:
            return val.split("T")[0]
        return val
    melted["AttributeValue"] = melted.apply(
        lambda row: strip_t(row["AttributeValue"]) if row["Attribute"] in {"Start Date", "End Date"} else row["AttributeValue"],
        axis=1
    )

    return melted[["Dimension", "RefName", "Attribute", "AttributeValue"]].rename(columns={"AttributeValue": "Value"})

# ---------------- MASTER READING & MELTDOWN ----------------
def read_txt_robust_in_memory(raw: bytes) -> pd.DataFrame:
    import csv, io
    encs = ['utf-8-sig','utf-8','utf-16','latin1']
    for enc in encs:
        try:
            buf = io.BytesIO(raw)
            df = pd.read_csv(buf, encoding=enc, sep=",", on_bad_lines="skip", engine="python")
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            return df
        except Exception:
            pass
    logging.error("[Master] Could not parse .txt with known encodings.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path) -> List[Path]:
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    csvs = []
    if not zip_path.is_file():
        logging.warning(f"[Master] ZIP not found: {zip_path}")
        return csvs
    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name = os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw = fo.read()
                if not raw:
                    continue
                df = read_txt_robust_in_memory(raw)
                df.columns = df.columns.str.strip()
                # Store full filename as RawFileName for later mapping
                df["RawFileName"] = base_name
                # Ensure first column is named "Name"
                if "Name" not in df.columns and len(df.columns) > 0:
                    firstcol = df.columns[0]
                    df.rename(columns={firstcol: "Name"}, inplace=True)
                out_csv = out_dir / base_name.replace(".txt", ".csv")
                df.to_csv(out_csv, index=False, encoding="utf-8")
                csvs.append(out_csv)
            except Exception as e:
                logging.error(f"[Master] Error reading {txt_file}: {e}")
    return csvs

def unify_master_csvs(csvs: List[Path]) -> pd.DataFrame:
    frames = []
    for cp in csvs:
        if not cp.is_file():
            continue
        try:
            df = pd.read_csv(cp, encoding="utf-8", on_bad_lines="skip")
            df.columns = df.columns.str.strip()
            frames.append(df)
        except Exception as e:
            logging.error(f"[Master unify] Error reading {cp}: {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()

def meltdown_master(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    """
    Process Master data:
      1. Use full filename (RawFileName) as dimension.
      2. First column "Name" holds the record's name.
      3. Melt the data and filter/rename attributes per the parameter file.
    """
    if df.empty or "RawFileName" not in df.columns:
        logging.warning("[Master meltdown] No RawFileName column")
        return pd.DataFrame()
    
    allowed_dims = set(param["dimension_params"]["master_file_map"].keys())
    df2 = df[df["RawFileName"].isin(allowed_dims)].copy()
    if df2.empty:
        logging.warning("[Master meltdown] No rows after dimension filtering")
        return pd.DataFrame()
    
    df2["Dimension"] = df2["RawFileName"].map(param["dimension_params"]["master_file_map"])
    id_vars = ["Dimension"]
    if "Name" in df2.columns:
        id_vars.append("Name")
    value_vars = [c for c in df2.columns if c not in id_vars and c != "RawFileName"]
    
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=value_vars,
        var_name="Attribute",
        value_name="Value"
    )
    if "Name" in melted.columns:
        melted.rename(columns={"Name": "RefName"}, inplace=True)
    
    allowed_attrs = set(param["attribute_params"]["master_attr_map"].keys())
    melted = melted[melted["Attribute"].isin(allowed_attrs)]
    melted["Attribute"] = melted["Attribute"].map(param["attribute_params"]["master_attr_map"])
    
    def strip_t(val):
        if isinstance(val, str) and "T" in val:
            return val.split("T")[0]
        return val
    melted["Value"] = melted.apply(
        lambda row: strip_t(row["Value"]) if row["Attribute"] in {"Start Date", "End Date"} else row["Value"],
        axis=1
    )
    return melted[["Dimension", "RefName", "Attribute", "Value"]]

# ---------------- BUILDING KEYS & COMPARISON ----------------
def build_keys(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in ["Dimension", "RefName", "Attribute", "Value"]:
        if c not in df.columns:
            df[c] = ""
        df[c] = df[c].fillna("").astype(str).str.strip()
    df["GroupKey"] = df["Dimension"] + " | " + df["RefName"]
    df["Key"] = df["Dimension"] + " | " + df["RefName"] + " | " + df["Attribute"] + " | " + df["Value"]
    df["Comments_1"] = ""
    df["Comments_2"] = ""
    df["Action Item"] = ""
    df["Missing In"] = ""
    return df

def compare_mode2(df_erp: pd.DataFrame, df_mst: pd.DataFrame) -> pd.DataFrame:
    erp_dict = {}
    mst_dict = {}
    for gk, grp in df_erp.groupby("GroupKey"):
        rec = {}
        name_ = grp["RefName"].iloc[0] if not grp.empty else ""
        rec["Name"] = name_
        for _, row in grp.iterrows():
            rec[row["Attribute"]] = row["Value"]
        erp_dict[gk] = rec
    for gk, grp in df_mst.groupby("GroupKey"):
        rec = {}
        name_ = grp["RefName"].iloc[0] if not grp.empty else ""
        rec["Name"] = name_
        for _, row in grp.iterrows():
            rec[row["Attribute"]] = row["Value"]
        mst_dict[gk] = rec

    all_keys = set(erp_dict.keys()) | set(mst_dict.keys())
    results = []
    for gk in all_keys:
        dim = gk.split(" | ")[0]
        a_data = erp_dict.get(gk, {})
        b_data = mst_dict.get(gk, {})
        name_a = a_data.get("Name", "")
        name_b = b_data.get("Name", "")
        if name_a and name_b and (name_a == name_b):
            all_attrs = (set(a_data.keys()) | set(b_data.keys())) - {"Name"}
            for attr in all_attrs:
                va = a_data.get(attr, "")
                vb = b_data.get(attr, "")
                if va != vb:
                    if va and not vb:
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
                    elif vb and not va:
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
                    else:
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
        else:
            if name_a and not name_b:
                results.append({"Dimension": dim, "Name": name_a, "Attribute": "Name", "Value": name_a, "Missing In": "MASTER"})
            elif name_b and not name_a:
                results.append({"Dimension": dim, "Name": name_b, "Attribute": "Name", "Value": name_b, "Missing In": "ERP"})
    df_diff = pd.DataFrame(results)
    if not df_diff.empty:
        df_diff["Key"] = (df_diff["Dimension"].str.strip() + " | " +
                          df_diff["Name"].str.strip() + " | " +
                          df_diff["Attribute"].str.strip() + " | " +
                          df_diff["Value"].str.strip())
    return df_diff

# ---------------- EXCEPTIONS & WRITE RESULTS ----------------
def read_exception_table(path: Path) -> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"Exception table not found: {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path)
        df.columns = df.columns.astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"Error reading exception table: {e}")
        return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame) -> pd.DataFrame:
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep = [c for c in df_exc.columns if c in ["Key", "Comments_1", "Comments_2", "hide exception"]]
    if not keep:
        return df
    exc = df_exc[keep].copy()
    exc["Key"] = exc["Key"].astype(str).str.strip()
    merged = df.merge(exc, on="Key", how="left", suffixes=("", "_exc"))
    merged["hide exception"] = merged.get("hide exception", "").fillna("").str.lower()
    final = merged[merged["hide exception"] != "yes"].copy()
    if "Comments_1_exc" in final.columns:
        final["Comments_1"] = np.where(final["Comments_1_exc"].notna(), final["Comments_1_exc"], final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"] = np.where(final["Comments_2_exc"].notna(), final["Comments_2_exc"], final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)
    return final

def write_results(df: pd.DataFrame, out_path: Path):
    if df.empty:
        logging.info("No differences found; skipping write.")
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)
    final_cols = ["Key", "Dimension", "Name", "Attribute", "Value", "Comments_1", "Comments_2", "Action Item", "Missing In"]
    for c in final_cols:
        if c not in df.columns:
            df[c] = ""
    df = df[final_cols]
    wb = Workbook()
    ws = wb.active
    ws.title = "Results"
    ws.append(final_cols)
    for rowvals in df.itertuples(index=False):
        ws.append(rowvals)
    header_font = Font(bold=True)
    fill = PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
    for cell in ws[1]:
        cell.font = header_font
        cell.fill = fill
        cell.alignment = Alignment(horizontal="center")
    for col in ws.columns:
        max_len = 0
        col_letter = col[0].column_letter
        for cell in col:
            val = str(cell.value) if cell.value else ""
            max_len = max(max_len, len(val))
        ws.column_dimensions[col_letter].width = max_len + 2
    ws.freeze_panes = "A2"
    wb.save(out_path)
    logging.info(f"Results saved to {out_path}")

# ---------------- DASHBOARD (8 Charts) ----------------
class Dashboard(ctk.CTkFrame):
    def __init__(self, parent):
        super().__init__(parent)
        self.df_current = pd.DataFrame()
        self.df_history = pd.DataFrame()
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)
        self.frames = {}
        for label in ["Heatmap", "Lollipop", "Circular", "Scatter", "Radar", "Normal Pie", "Normal Bar", "Band Chart"]:
            frame = ctk.CTkFrame(self.notebook)
            self.notebook.add(frame, text=label)
            self.frames[label] = frame

    def update_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current = df_current.copy()
        self.df_history = df_history.copy()
        self.plot_heatmap()
        self.plot_lollipop()
        self.plot_circular()
        self.plot_scatter()
        self.plot_radar()
        self.plot_normal_pie()
        self.plot_normal_bar()
        self.plot_band_chart()

    def plot_heatmap(self):
        frame = self.frames["Heatmap"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        if df_m.empty:
            return
        pivoted = df_m.groupby(["Dimension", "Attribute"]).size().unstack(fill_value=0)
        fig, ax = plt.subplots(figsize=(6, 5))
        cax = ax.imshow(pivoted, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivoted.columns)))
        ax.set_xticklabels(pivoted.columns, rotation=90)
        ax.set_yticks(range(len(pivoted.index)))
        ax.set_yticklabels(pivoted.index)
        fig.colorbar(cax, ax=ax)
        ax.set_title("Heatmap: Missing Items")
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_lollipop(self):
        frame = self.frames["Lollipop"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        if df_m.empty:
            return
        count_dim = df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        fig, ax = plt.subplots(figsize=(6, 5))
        ax.hlines(y=count_dim.index, xmin=0, xmax=count_dim.values, color="skyblue")
        ax.plot(count_dim.values, count_dim.index, "o", color="skyblue")
        ax.set_title("Lollipop: Missing Dimensions")
        ax.set_xlabel("Missing Count")
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_circular(self):
        frame = self.frames["Circular"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        if df_m.empty:
            return
        count_attr = df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        categories = count_attr.index.tolist()
        values = count_attr.values
        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)
        fig = plt.figure(figsize=(6, 6))
        ax = fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(categories, fontsize=9)
        ax.bar(angles, values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular Barplot: Missing Attributes", y=1.05)
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_scatter(self):
        frame = self.frames["Scatter"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        count_dim = df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        count_dim.sort_values("Count", ascending=False, inplace=True)
        xvals = np.arange(len(count_dim))
        yvals = count_dim["Count"].values
        labels = count_dim["Dimension"].values
        fig, ax = plt.subplots(figsize=(6, 5))
        ax.scatter(xvals, yvals, color="green")
        for i, txt in enumerate(labels):
            ax.text(xvals[i], yvals[i], txt, ha="center", va="bottom", rotation=60)
        ax.set_xticks([])
        ax.set_ylabel("Missing Count")
        ax.set_title("Scatter: Missing by Dimension")
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_radar(self):
        frame = self.frames["Radar"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        count_dim = df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(5)
        categories = count_dim.index.tolist()
        values = count_dim.values.tolist()
        N = len(categories)
        angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()
        angles += angles[:1]
        values += values[:1]
        fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=9)
        ax.plot(angles, values, color="red", linewidth=2)
        ax.fill(angles, values, color="red", alpha=0.3)
        ax.set_title("Radar: Top 5 Missing Dimensions", y=1.08)
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_normal_pie(self):
        frame = self.frames["Normal Pie"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        dist = df_m["Missing In"].value_counts()
        fig, ax = plt.subplots(figsize=(5, 5))
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Normal Pie: Missing In Distribution")
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_normal_bar(self):
        frame = self.frames["Normal Bar"]
        for w in frame.winfo_children():
            w.destroy()
        df_m = self.df_current[self.df_current["Missing In"] != ""]
        count_attr = df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        fig, ax = plt.subplots(figsize=(6, 4))
        count_attr.plot(kind="bar", ax=ax, color="blue")
        ax.set_ylabel("Missing Count")
        ax.set_title("Normal Bar: Top 10 Missing Attributes")
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_band_chart(self):
        frame = self.frames["Band Chart"]
        for w in frame.winfo_children():
            w.destroy()
        if self.df_history.empty or "RunDate" not in self.df_history.columns:
            return
        df_m = self.df_history[self.df_history["Missing In"] != ""]
        date_counts = df_m.groupby("RunDate")["Key"].count().reset_index()
        date_counts.sort_values("RunDate", inplace=True)
        date_counts["Count_min"] = date_counts["Key"] * 0.9
        date_counts["Count_max"] = date_counts["Key"] * 1.1
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(date_counts["RunDate"], date_counts["Key"], color="purple", label="Missing Count")
        ax.fill_between(date_counts["RunDate"], date_counts["Count_min"], date_counts["Count_max"],
                        color="purple", alpha=0.2, label="±10% band")
        ax.set_title("Band Chart Over Days")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Missing Count")
        plt.xticks(rotation=45)
        ax.legend()
        for i, row in date_counts.iterrows():
            ax.text(row["RunDate"], row["Key"], str(row["Key"]), ha="center", va="bottom")
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

# ---------------- SIMPLIFIED EXCELGRID (ONLY DATE FILTERING) ----------------
class ExcelGrid(ctk.CTkFrame):
    """
    Simplified grid for displaying DataFrames. Only the "Start Date" and "End Date" columns are filterable.
    A multi-select popup (with search capability) lets the user filter these date columns (including NaN).
    """
    FILTERABLE_COLS = {"Start Date", "End Date"}
    
    def __init__(self, parent, config_block: Dict, name: str):
        super().__init__(parent)
        self.name = name
        self.df = pd.DataFrame()
        self.filters: Dict[str, Set] = {}  # Store selected values for date columns
        self.create_table()
        self.create_statusbar()
    
    def create_table(self):
        container = ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree = ttk.Treeview(container, show="headings")
        vsb = ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb = ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")
        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)
        self.tree.bind("<ButtonRelease-1>", self.on_header_click)
    
    def create_statusbar(self):
        self.status_label = ctk.CTkLabel(self, text="Ready")
        self.status_label.pack(fill="x", padx=5, pady=2)
    
    def set_data(self, df: pd.DataFrame):
        self.df = df.copy()
        self.refresh_table()
    
    def on_header_click(self, event):
        region = self.tree.identify("region", event.x, event.y)
        if region == "heading":
            column = self.tree.identify_column(event.x)
            col_index = int(column[1:]) - 1
            if col_index < len(self.tree["columns"]):
                col_name = self.tree["columns"][col_index]
                if col_name in self.FILTERABLE_COLS:
                    self.show_filter_popup(col_name)
    
    def show_filter_popup(self, column_name: str):
        if self.df.empty or column_name not in self.df.columns:
            return
        popup = tk.Toplevel(self)
        popup.title(f"Filter: {column_name}")
        popup.geometry("300x400")
        popup.transient(self)
        popup.grab_set()
        frame = ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)
        unique_vals = self.df[column_name].unique()
        display_map = {}
        for val in unique_vals:
            if pd.isna(val):
                display_map[val] = "(Blank)"
            elif isinstance(val, str) and not val.strip():
                display_map[val] = "(Empty)"
            else:
                display_map[val] = str(val)
        # Sort values (NaN will be added at the end)
        sorted_vals = sorted([v for v in unique_vals if not pd.isna(v)], key=lambda x: x)
        if any(pd.isna(v) for v in unique_vals):
            sorted_vals.append(np.nan)
        current_filter = self.filters.get(column_name, set())
        if not current_filter:
            current_filter = set(unique_vals)
        select_all_var = tk.BooleanVar(value=True)
        def toggle_all():
            check = select_all_var.get()
            for var in var_dict.values():
                var.set(check)
        select_all_frame = ctk.CTkFrame(frame)
        select_all_frame.pack(fill="x", pady=5)
        ctk.CTkCheckBox(select_all_frame, text="Select All", variable=select_all_var, command=toggle_all).pack(side="left", padx=5)
        search_var = tk.StringVar()
        def on_search(*args):
            search_text = search_var.get().lower()
            for val, chk in checkboxes.items():
                txt = display_map[val].lower()
                if search_text in txt:
                    chk.pack(anchor="w")
                else:
                    chk.pack_forget()
        search_frame = ctk.CTkFrame(frame)
        search_frame.pack(fill="x", pady=5)
        ctk.CTkLabel(search_frame, text="Search:").pack(side="left", padx=5)
        ctk.CTkEntry(search_frame, textvariable=search_var).pack(side="left", fill="x", expand=True, padx=5)
        search_var.trace_add("write", on_search)
        scroll = ctk.CTkScrollableFrame(frame, width=250, height=250)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)
        var_dict = {}
        checkboxes = {}
        for val in sorted_vals:
            disp = display_map[val]
            var = tk.BooleanVar(value=(val in current_filter))
            var_dict[val] = var
            chk = ctk.CTkCheckBox(scroll, text=disp, variable=var)
            chk.pack(anchor="w")
            checkboxes[val] = chk
        button_frame = ctk.CTkFrame(frame)
        button_frame.pack(fill="x", pady=5)
        def apply_filter():
            selected = {val for val, var in var_dict.items() if var.get()}
            if selected:
                if selected == set(unique_vals):
                    self.filters.pop(column_name, None)
                else:
                    self.filters[column_name] = selected
            else:
                self.filters.pop(column_name, None)
            popup.destroy()
            self.refresh_table()
        def clear_filter():
            self.filters.pop(column_name, None)
            popup.destroy()
            self.refresh_table()
        ctk.CTkButton(button_frame, text="Apply", command=apply_filter).pack(side="left", padx=5)
        ctk.CTkButton(button_frame, text="Clear Filter", command=clear_filter).pack(side="left", padx=5)
        ctk.CTkButton(button_frame, text="Cancel", command=popup.destroy).pack(side="left", padx=5)
    
    def get_filtered_df(self) -> pd.DataFrame:
        if self.df.empty:
            return self.df
        df_filtered = self.df.copy()
        for col, allowed_vals in self.filters.items():
            if col in df_filtered.columns and allowed_vals:
                mask = pd.Series(False, index=df_filtered.index)
                for val in allowed_vals:
                    if pd.isna(val):
                        mask |= df_filtered[col].isna()
                    else:
                        mask |= (df_filtered[col] == val)
                df_filtered = df_filtered[mask]
        return df_filtered
    
    def refresh_table(self):
        for item in self.tree.get_children():
            self.tree.delete(item)
        if self.df.empty:
            return
        cols = list(self.df.columns)
        self.tree["columns"] = cols
        for col in cols:
            header_text = col
            if col in self.filters:
                header_text = f"🔍 {col}"
            self.tree.heading(col, text=header_text)
            self.tree.column(col, width=150)
        df_filtered = self.get_filtered_df()
        for _, row in df_filtered.iterrows():
            values = [row[col] if col in df_filtered.columns else "" for col in cols]
            display_values = []
            for val in values:
                if pd.isna(val):
                    display_values.append("(Blank)")
                elif isinstance(val, str) and not val.strip():
                    display_values.append("(Empty)")
                else:
                    display_values.append(str(val))
            self.tree.insert("", "end", values=display_values)
        self.status_label.configure(text=f"{len(df_filtered)} rows")

    def get_config_block(self) -> Dict:
        return {"filters": {col: list(vals) for col, vals in self.filters.items()}}

# ---------------- MAIN APP ----------------
class MainApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation: Mode=2")
        self.geometry("1600x900")
        # Set transparency and a burgundy background
        self.attributes("-alpha", 0.95)
        self.configure(bg="#800020")  # Burgundy color

        self.config_dict = load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.history_df = pd.DataFrame()
        self.param_dict = read_parameter_file(Path(self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"])))
        
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True, padx=10, pady=10)
        
        # Paths Tab
        self.tab_paths = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_paths, text="Paths")
        self.build_paths_tab(self.tab_paths)
        
        # ERP Tab
        self.tab_erp = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_erp, text="ERP")
        self.erp_grid = ExcelGrid(self.tab_erp, {}, "ERP")
        self.erp_grid.pack(fill="both", expand=True)
        
        # Master Tab
        self.tab_master = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_master, text="Master")
        self.master_grid = ExcelGrid(self.tab_master, {}, "Master")
        self.master_grid.pack(fill="both", expand=True)
        
        # Compare & Exceptions Tab
        self.tab_compare = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_compare, text="Compare & Exceptions")
        self.build_compare_tab(self.tab_compare)
        
        # Dashboard Tab
        self.tab_dashboard = Dashboard(self.notebook)
        self.notebook.add(self.tab_dashboard, text="Dashboard")
        
        # Log Box
        self.log_box = ctk.CTkTextbox(self, height=120)
        self.log_box.pack(fill="both", expand=False)
        self.log_box.configure(state="disabled")
        handler = TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)
        
        # Master CSV folder
        self.temp_csv_dir = Path(self.config_dict["paths"].get("MASTER_CSV_OUTPUT", "temp_master_csv"))
        self.temp_csv_dir.mkdir(exist_ok=True)
        
        # Load initial data
        self.refresh_erp_data()
        self.refresh_master_data()
    
    def build_paths_tab(self, parent):
        frm = ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        self.erp_var = tk.StringVar(value=self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mst_var = tk.StringVar(value=self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_var = tk.StringVar(value=self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_var = tk.StringVar(value=self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))
        self.cfg_var = tk.StringVar(value=self.config_dict["paths"].get("CONFIG_PATH", DEFAULT_PATHS["CONFIG_PATH"]))
        self.par_var = tk.StringVar(value=self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"]))
        self.csv_var = tk.StringVar(value=self.config_dict["paths"].get("MASTER_CSV_OUTPUT", DEFAULT_PATHS["MASTER_CSV_OUTPUT"]))
        def mkrow(lbl, var, is_dir=False):
            rowf = ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=180).pack(side="left", padx=5)
            e = ctk.CTkEntry(rowf, textvariable=var, width=600)
            e.pack(side="left", padx=5)
            def br():
                if is_dir:
                    p = filedialog.askdirectory()
                else:
                    p = filedialog.askopenfilename()
                if p:
                    var.set(p)
            ctk.CTkButton(rowf, text="Browse", command=br).pack(side="left", padx=5)
        mkrow("ERP Excel Path:", self.erp_var)
        mkrow("Master ZIP Path:", self.mst_var)
        mkrow("Exception Path:", self.exc_var)
        mkrow("Output Excel Path:", self.out_var)
        mkrow("JSON Config Path:", self.cfg_var)
        mkrow("Parameter File Path:", self.par_var)
        mkrow("Master CSV Folder:", self.csv_var, is_dir=True)
    
    def build_compare_tab(self, parent):
        frm = ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Mode=2 Comparison").pack(pady=5)
        btnf = ctk.CTkFrame(frm)
        btnf.pack(fill="x", pady=5)
        ctk.CTkButton(btnf, text="Run Comparison", command=self.run_comparison).pack(side="left", padx=5)
        ctk.CTkButton(btnf, text="Save Config", command=self.save_all_config).pack(side="left", padx=5)
    
    def refresh_erp_data(self):
        df_erp = read_erp_enabled(Path(self.erp_var.get().strip()))
        self.erp_grid.set_data(df_erp)
    
    def refresh_master_data(self):
        zip_path = Path(self.mst_var.get().strip())
        out_dir = Path(self.csv_var.get().strip())
        csvs = convert_master_txt_to_csv(zip_path, out_dir)
        df_m = unify_master_csvs(csvs)
        self.master_grid.set_data(df_m)
    
    def run_comparison(self):
        # Update paths in config
        self.config_dict["paths"]["ERP_EXCEL_PATH"] = self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"] = self.mst_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"] = self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"] = self.out_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"] = self.cfg_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"] = self.par_var.get().strip()
        self.config_dict["paths"]["MASTER_CSV_OUTPUT"] = self.csv_var.get().strip()
        self.config_dict["comparison_option"] = 2
        # Re-read parameter file
        newparam = read_parameter_file(Path(self.par_var.get().strip()))
        # Process ERP data
        df_erp_filt = self.erp_grid.get_filtered_df()
        erp_ready_raw = meltdown_erp(df_erp_filt, newparam)
        erp_ready = build_keys(erp_ready_raw)
        # Process Master data
        df_mst_filt = self.master_grid.get_filtered_df()
        mst_ready_raw = meltdown_master(df_mst_filt, newparam)
        mst_ready = build_keys(mst_ready_raw)
        # Compare (Mode=2)
        df_diff = compare_mode2(erp_ready, mst_ready)
        # Exceptions
        exc_path = Path(self.exc_var.get().strip())
        df_exc = read_exception_table(exc_path)
        final = merge_exceptions(df_diff, df_exc)
        # Write results
        out_path = Path(self.out_var.get().strip())
        write_results(final, out_path)
        # Update dashboard history
        run_date = datetime.now().strftime("%Y-%m-%d")
        final["RunDate"] = run_date
        self.history_df = pd.concat([self.history_df, final], ignore_index=True)
        self.notebook.select(self.tab_dashboard)
        self.tab_dashboard.update_data(final, self.history_df)
        messagebox.showinfo("Done", f"Comparison completed. Output saved to {out_path}")
    
    def save_all_config(self):
        self.config_dict["paths"]["ERP_EXCEL_PATH"] = self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"] = self.mst_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"] = self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"] = self.out_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"] = self.cfg_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"] = self.par_var.get().strip()
        self.config_dict["paths"]["MASTER_CSV_OUTPUT"] = self.csv_var.get().strip()
        self.config_dict["comparison_option"] = 2
        save_config(self.config_dict, Path(self.cfg_var.get()))
        messagebox.showinfo("Saved", "Configuration saved successfully.")

def main():
    app = MainApp()
    app.mainloop()

if __name__ == "__main__":
    main()
