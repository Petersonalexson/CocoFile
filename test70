import pandas as pd
from pathlib import Path

def transform_alfa(
    file_path: Path,
    excluded_keys: set,
    sheet_name: str = "Sheet1",
    skip_rows: int = 3,
    dimension_rename_dict: dict = None,
    attr_rename_dict: dict = None,
    keep_attributes: list = None,
    ignore_attributes: list = None,
    include_filters: list = None,
    exclude_filters: list = None,
    pre_exclude_filters: list = None  # New parameter for pre-melt exclusion
) -> pd.DataFrame:
    """
    Transforms an Excel file (Alfa) into a structured DataFrame with unique NameIDs.

    Parameters:
    - file_path (Path): Path to the Excel file.
    - excluded_keys (set): Set of keys to exclude from the final DataFrame.
    - sheet_name (str): Name of the sheet to read. Default is "Sheet1".
    - skip_rows (int): Number of rows to skip at the start. Default is 3.
    - dimension_rename_dict (dict): Dictionary to rename Dimension values. Default is None.
    - attr_rename_dict (dict): Dictionary to rename Attribute names. Default is None.
    - keep_attributes (list): List of attributes to keep. If None, all are kept.
    - ignore_attributes (list): List of attributes to ignore. If None, none are ignored.
    - include_filters (list): List of tuples for include filtering based on (Attribute, Value).
    - exclude_filters (list): List of tuples for exclude filtering based on (Attribute, Value).
    - pre_exclude_filters (list): List of tuples for pre-melt exclusion based on (Attribute, Value).

    Returns:
    - pd.DataFrame: Transformed DataFrame with columns [Key, Dimension, NameID, Attribute, Value].
    """

    # 1. Read Excel
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        print(f"[Alfa] Initial rows: {len(df)}")
        print(f"[Alfa] Initial columns: {df.columns.tolist()}")
    except Exception as e:
        print(f"[Alfa] Error reading Excel file: {e}")
        return pd.DataFrame(columns=["Key", "Dimension", "NameID", "Attribute", "Value"])

    # 2. Validate the number of columns
    if df.shape[1] < 4:
        print("[Alfa] Warning: fewer than 4 columns in the Excel. Returning empty DataFrame.")
        return pd.DataFrame(columns=["Key", "Dimension", "NameID", "Attribute", "Value"])

    # 3. Rename columns: Column C -> 'Dimension', Column D -> 'NameID'
    original_dimension_col = df.columns[2]  # Column C (0-based index)
    original_nameid_col = df.columns[3]     # Column D
    df.rename(columns={
        original_dimension_col: "Dimension",
        original_nameid_col: "NameID"
    }, inplace=True)
    print(f"[Alfa] Columns after renaming: {df.columns.tolist()}")  # **Debug**

    # 4. Create 'Name' column equal to 'NameID' for inclusion as an attribute
    df['Name'] = df['NameID']
    print(f"[Alfa] Added 'Name' column:\n{df[['NameID', 'Name']].head()}")  # **Debug**

    # 5. Apply pre-exclude filters based on other attributes (excluding 'Name')
    if pre_exclude_filters:
        # pre_exclude_filters: list of tuples (Attribute, [Values])
        combined_pre_exclude_mask = pd.Series([False] * len(df), index=df.index)
        for (attr, val_list) in pre_exclude_filters:
            if attr.lower() == 'name':
                print(f"[Alfa] Skipping pre-exclude filter for 'Name' attribute.")
                continue  # Exclude 'Name' from pre-exclusion
            if attr not in df.columns:
                print(f"[Alfa] Warning: Attribute '{attr}' not found in DataFrame columns for pre-exclude.")
                continue
            sub_mask = df[attr].isin(val_list)
            combined_pre_exclude_mask = combined_pre_exclude_mask | sub_mask
            print(f"[Alfa] Applying pre-exclude filter: {attr} in {val_list} → {sub_mask.sum()} matches.")
        pre_excluded_count = combined_pre_exclude_mask.sum()
        df = df[~combined_pre_exclude_mask]
        print(f"[Alfa] Pre-excluded {pre_excluded_count} rows based on pre_exclude_filters.")  # **Debug**

    # 6. Melt the DataFrame, keeping 'Dimension' and 'NameID' as id_vars
    id_vars = ['Dimension', 'NameID']
    val_vars = [col for col in df.columns if col not in id_vars]
    print(f"[Alfa] id_vars: {id_vars}, val_vars: {val_vars}")  # **Debug**

    # Ensure 'Name' is included as an attribute
    if 'Name' not in val_vars and 'Name' in df.columns:
        val_vars.append('Name')
        print(f"[Alfa] 'Name' added to val_vars for melting.")  # **Debug**

    df_melt = df.melt(
        id_vars=id_vars,
        value_vars=val_vars,
        var_name="Attribute",
        value_name="Value"
    )
    print(f"[Alfa] Rows after melt: {len(df_melt)}")  # **Debug**
    print(f"[Alfa] Sample melted data:\n{df_melt.head()}")  # **Debug**

    # 7. Rename dimensions and attributes if renaming dictionaries are provided
    if dimension_rename_dict:
        df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
        print(f"[Alfa] Dimensions after renaming: {df_melt['Dimension'].unique()}")  # **Debug**
    if attr_rename_dict:
        df_melt["Attribute"] = df_melt["Attribute"].replace(attr_rename_dict)
        print(f"[Alfa] Attributes after renaming: {df_melt['Attribute'].unique()}")  # **Debug**

    # 8. Keep or ignore certain attributes based on provided lists
    if keep_attributes is not None:
        original_count = len(df_melt)
        df_melt = df_melt[df_melt["Attribute"].isin(keep_attributes)]
        dropped = original_count - len(df_melt)
        print(f"[Alfa] Rows after keeping attributes {keep_attributes}: {len(df_melt)} (Dropped {dropped})")  # **Debug**
    if ignore_attributes is not None:
        original_count = len(df_melt)
        df_melt = df_melt[~df_melt["Attribute"].isin(ignore_attributes)]
        dropped = original_count - len(df_melt)
        print(f"[Alfa] Rows after ignoring attributes {ignore_attributes}: {len(df_melt)} (Dropped {dropped})")  # **Debug**

    # 9. Apply include/exclude filters based on (Attribute, Value) pairs
    def match_any_filter(dfa, filters):
        if not filters:
            return pd.Series([False] * len(dfa), index=dfa.index)
        combined_mask = pd.Series([False] * len(dfa), index=dfa.index)
        for (attr, val_list) in filters:
            sub_mask = (dfa["Attribute"] == attr) & (dfa["Value"].isin(val_list))
            combined_mask = combined_mask | sub_mask
            print(f"[Alfa] Applying filter: {attr} in {val_list} → {sub_mask.sum()} matches.")
        return combined_mask

    if include_filters:
        inc_mask = match_any_filter(df_melt, include_filters)
        original_count = len(df_melt)
        df_melt = df_melt[inc_mask]
        kept = len(df_melt) - original_count
        print(f"[Alfa] Rows after include_filters: {len(df_melt)} (Kept {len(df_melt) - original_count})")  # **Debug**

    if exclude_filters:
        exc_mask = match_any_filter(df_melt, exclude_filters)
        original_count = len(df_melt)
        df_melt = df_melt[~exc_mask]
        excluded = original_count - len(df_melt)
        print(f"[Alfa] Rows after exclude_filters: {len(df_melt)} (Excluded {excluded})")  # **Debug**

    # 10. Create 'Key' column
    df_melt["Key"] = df_melt.apply(
        lambda row: f"{row['Dimension']} | {row['NameID']} | {row['Attribute']} | {row['Value']}",
        axis=1
    )
    print(f"[Alfa] Sample Keys:\n{df_melt['Key'].head()}")  # **Debug**

    # 11. Exclude rows based on the excluded_keys set
    before_exclusion = len(df_melt)
    df_melt = df_melt[~df_melt["Key"].isin(excluded_keys)]
    after_exclusion = len(df_melt)
    print(f"[Alfa] Excluded {before_exclusion - after_exclusion} rows based on excluded_keys.")  # **Debug**

    # 12. Reorder columns and drop duplicates by 'Key'
    df_melt = df_melt[["Key", "Dimension", "NameID", "Attribute", "Value"]]
    df_melt.drop_duplicates(subset=["Key"], inplace=True)
    print(f"[Alfa] Final DataFrame has {len(df_melt)} rows after dropping duplicates.")  # **Debug**

    return df_melt
