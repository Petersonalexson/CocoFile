import os
import zipfile
import pandas as pd
from pathlib import Path
import logging
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font

# ---------------------------
# 0) SETUP LOGGING
# ---------------------------
def setup_logging(log_file: Path) -> None:
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    c_handler = logging.StreamHandler()
    f_handler = logging.FileHandler(log_file, mode='w')
    c_handler.setLevel(logging.INFO)
    f_handler.setLevel(logging.DEBUG)
    c_format = logging.Formatter('%(levelname)s: %(message)s')
    f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    c_handler.setFormatter(c_format)
    f_handler.setFormatter(f_format)
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)

# ---------------------------
# 1) PRE-MELT FILTER
# ---------------------------
def filter_pre_melt(df: pd.DataFrame, exclude_rules: list = None) -> pd.DataFrame:
    """
    Apply pre-melt filtering rules.
    
    Parameters:
      - exclude_rules: list of tuples (column_name, [bad_values])
        Any row with a value in the given column equal to one of the bad_values is dropped.
    """
    if not exclude_rules:
        return df
    combined_mask = pd.Series(False, index=df.index)
    for col, bad_values in exclude_rules:
        if col in df.columns:
            mask = df[col].isin(bad_values)
            logging.debug(f"[Pre-Melt] Excluding {mask.sum()} rows from column '{col}' with values in {bad_values}")
            combined_mask |= mask
        else:
            logging.warning(f"[Pre-Melt] Column '{col}' not found in DataFrame.")
    return df[~combined_mask]

# ---------------------------
# 2) POST-MELT FILTER (Exclude Bad Dimensions/Attributes)
# ---------------------------
def exclude_dimension_attribute(df: pd.DataFrame, bad_dimensions: list = None, bad_attributes: list = None) -> pd.DataFrame:
    """
    Exclude rows based on bad dimensions or attributes.
    """
    if bad_dimensions:
        initial = len(df)
        df = df[~df["Dimension"].isin(bad_dimensions)]
        logging.debug(f"[Post-Melt] Excluded {initial - len(df)} rows based on bad dimensions {bad_dimensions}")
    if bad_attributes:
        initial = len(df)
        df = df[~df["Attribute"].isin(bad_attributes)]
        logging.debug(f"[Post-Melt] Excluded {initial - len(df)} rows based on bad attributes {bad_attributes}")
    return df

# ---------------------------
# 3) TRANSFORM ALFA
# ---------------------------
def transform_alfa(file_path: Path,
                   pre_melt_exclude_rules: list = None,
                   bad_dimensions: list = None,
                   bad_attributes: list = None,
                   dimension_rename: dict = None,
                   attribute_rename: dict = None,
                   sheet_name: str = "Sheet1",
                   skip_rows: int = 3) -> pd.DataFrame:
    """
    Reads and transforms Alfa (Excel) data.
    
    Assumes that:
      - The third column is Dimension.
      - The fourth column is NameID.
    A "Name" column is forced (equal to NameID) so that when melting,
    a row with Attribute "Name" is produced.
    """
    try:
        if not file_path.is_file():
            logging.error(f"[Alfa] File not found: {file_path}")
            return pd.DataFrame()
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        logging.info(f"[Alfa] Loaded {len(df)} rows from Excel.")
        
        # Pre-melt filtering.
        df = filter_pre_melt(df, pre_melt_exclude_rules)
        
        # Rename columns: assume third column is Dimension and fourth is NameID.
        df.rename(columns={df.columns[2]: "Dimension", df.columns[3]: "NameID"}, inplace=True)
        
        # Force a "Name" column from NameID.
        df["Name"] = df["NameID"]
        
        # Melt the DataFrame.
        id_vars = ["Dimension", "NameID"]
        value_vars = [col for col in df.columns if col not in id_vars]
        melted = df.melt(id_vars=["Dimension", "NameID"], value_vars=value_vars,
                         var_name="Attribute", value_name="Value")
        
        # Apply renaming (if provided).
        if dimension_rename:
            melted["Dimension"] = melted["Dimension"].replace(dimension_rename)
        if attribute_rename:
            melted["Attribute"] = melted["Attribute"].replace(attribute_rename)
        
        # Exclude rows with bad dimensions or attributes.
        melted = exclude_dimension_attribute(melted, bad_dimensions, bad_attributes)
        
        # Create a grouping key: Dimension | NameID.
        melted["GroupKey"] = (
            melted["Dimension"].astype(str).str.strip() + " | " +
            melted["NameID"].astype(str).str.strip()
        )
        
        # Build a full key string for each row: "Dimension | NameID | Attribute | Value"
        def build_full_key(row):
            return f"{row['Dimension'].strip()} | {row['NameID'].strip()} | {row['Attribute'].strip()} | {str(row['Value']).strip()}"
        melted["Key"] = melted.apply(build_full_key, axis=1)
        
        melted.drop_duplicates(inplace=True)
        logging.info(f"[Alfa] Transformed data to {len(melted)} rows after melting.")
        return melted
    except Exception as e:
        logging.error(f"[Alfa] Error during transformation: {e}")
        return pd.DataFrame()

# ---------------------------
# 4) TRANSFORM GAMMA
# ---------------------------
def transform_gamma(zip_file_path: Path,
                    pre_melt_exclude_rules: list = None,
                    bad_dimensions: list = None,
                    bad_attributes: list = None,
                    dimension_rename: dict = None,
                    attribute_rename: dict = None,
                    delimiter: str = ",",
                    remove_substring: str = "_ceaster.txt") -> pd.DataFrame:
    """
    Reads and transforms Gamma data from a ZIP file containing .txt (CSV) files.
    
    For each file:
      - The filename (with an optional substring removed) defines the Dimension.
      - The first column is renamed to "NameID" and used to force a "Name" attribute.
      - Pre-melt filtering is applied.
      - The DataFrame is melted, then filtered, and renaming dictionaries are applied.
    """
    try:
        if not zip_file_path.is_file():
            logging.error(f"[Gamma] ZIP file not found: {zip_file_path}")
            return pd.DataFrame()
        all_dfs = []
        with zipfile.ZipFile(zip_file_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.endswith(".txt")]
            if not txt_files:
                logging.warning("[Gamma] No .txt files found in the ZIP.")
                return pd.DataFrame()
            for txt_file in txt_files:
                try:
                    # Compute Dimension from the filename.
                    base = os.path.basename(txt_file)
                    if remove_substring in base:
                        base = base.replace(remove_substring, "")
                    else:
                        base, _ = os.path.splitext(base)
                    dimension = base.replace("_", " ").strip()
                    
                    # Read CSV data.
                    with z.open(txt_file) as fo:
                        df = pd.read_csv(fo, delimiter=delimiter)
                    if df.empty:
                        logging.warning(f"[Gamma] File {txt_file} is empty. Skipping.")
                        continue
                    
                    # Rename the first column to "NameID".
                    first_col = df.columns[0]
                    df.rename(columns={first_col: "NameID"}, inplace=True)
                    df["NameID"] = df["NameID"].fillna("Unknown")
                    
                    # Pre-melt filtering.
                    df = filter_pre_melt(df, pre_melt_exclude_rules)
                    
                    # Add Dimension column.
                    df["Dimension"] = dimension
                    
                    # Force a "Name" column.
                    df["Name"] = df["NameID"]
                    
                    # Melt the DataFrame.
                    melted = df.melt(
                        id_vars=["Dimension", "NameID"],
                        var_name="Attribute",
                        value_name="Value"
                    )
                    
                    # Apply renaming if provided.
                    if dimension_rename:
                        melted["Dimension"] = melted["Dimension"].replace(dimension_rename)
                    if attribute_rename:
                        melted["Attribute"] = melted["Attribute"].replace(attribute_rename)
                    
                    # Exclude rows with bad dimensions or attributes.
                    melted = exclude_dimension_attribute(melted, bad_dimensions, bad_attributes)
                    
                    # Create GroupKey: Dimension | NameID.
                    melted["GroupKey"] = (
                        melted["Dimension"].astype(str).str.strip() + " | " +
                        melted["NameID"].astype(str).str.strip()
                    )
                    
                    # Build the full key.
                    def build_full_key(row):
                        return f"{row['Dimension'].strip()} | {row['NameID'].strip()} | {row['Attribute'].strip()} | {str(row['Value']).strip()}"
                    melted["Key"] = melted.apply(build_full_key, axis=1)
                    
                    melted.drop_duplicates(inplace=True)
                    logging.info(f"[Gamma] Processed {txt_file} into {len(melted)} rows.")
                    all_dfs.append(melted)
                except Exception as inner_e:
                    logging.error(f"[Gamma] Error processing file {txt_file}: {inner_e}")
                    continue
        if all_dfs:
            df_gamma = pd.concat(all_dfs, ignore_index=True)
            logging.info(f"[Gamma] Combined transformed data has {len(df_gamma)} rows.")
            return df_gamma
        else:
            logging.warning("[Gamma] No valid data found in ZIP.")
            return pd.DataFrame()
    except Exception as e:
        logging.error(f"[Gamma] Error during transformation: {e}")
        return pd.DataFrame()

# ---------------------------
# 5) CREATE MISSING ITEMS EXCEL
# ---------------------------
def create_missing_items_excel(df_alfa: pd.DataFrame,
                               df_gamma: pd.DataFrame,
                               df_exceptions: pd.DataFrame,
                               output_path: Path) -> None:
    """
    Compares Alfa and Gamma records (grouped by Dimension and NameID) and outputs only keys
    (rows) that are missing in one source.
    """
    try:
        logging.info("[Missing Items] Starting comparison between Alfa and Gamma.")
        
        # Ensure key columns are strings (but do not force them to lower case).
        for df in [df_alfa, df_gamma]:
            df["Dimension"] = df["Dimension"].fillna("").astype(str)
            df["NameID"] = df["NameID"].fillna("").astype(str)
            df["Attribute"] = df["Attribute"].fillna("").astype(str)
        
        # Get the set of all groups.
        groups_alfa = set(df_alfa["GroupKey"].unique())
        groups_gamma = set(df_gamma["GroupKey"].unique())
        all_groups = groups_alfa.union(groups_gamma)
        
        missing_items = []
        for group in all_groups:
            alfa_group = df_alfa[df_alfa["GroupKey"] == group]
            gamma_group = df_gamma[df_gamma["GroupKey"] == group]
            
            # If one source is missing the entire group, report the "Name" row from the other.
            if alfa_group.empty and not gamma_group.empty:
                name_rows = gamma_group[gamma_group["Attribute"].str.lower() == "name"]
                if not name_rows.empty:
                    row = name_rows.iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Alfa"
                    })
                continue
            if gamma_group.empty and not alfa_group.empty:
                name_rows = alfa_group[alfa_group["Attribute"].str.lower() == "name"]
                if not name_rows.empty:
                    row = name_rows.iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Gamma"
                    })
                continue
            
            # Both groups exist. Check for presence of the "Name" attribute.
            alfa_has_name = not alfa_group[alfa_group["Attribute"].str.lower() == "name"].empty
            gamma_has_name = not gamma_group[gamma_group["Attribute"].str.lower() == "name"].empty
            if not alfa_has_name or not gamma_has_name:
                if not alfa_has_name and gamma_has_name:
                    row = gamma_group[gamma_group["Attribute"].str.lower() == "name"].iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Alfa"
                    })
                if not gamma_has_name and alfa_has_name:
                    row = alfa_group[alfa_group["Attribute"].str.lower() == "name"].iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Gamma"
                    })
                continue  # Skip further attribute comparisons for this group.
            
            # Both groups have a "Name" row; compare non-"Name" attributes.
            alfa_attrs = set(alfa_group[alfa_group["Attribute"].str.lower() != "name"]["Attribute"].str.lower())
            gamma_attrs = set(gamma_group[gamma_group["Attribute"].str.lower() != "name"]["Attribute"].str.lower())
            missing_in_gamma = alfa_attrs - gamma_attrs
            missing_in_alfa = gamma_attrs - alfa_attrs
            for attr in missing_in_gamma:
                row = alfa_group[alfa_group["Attribute"].str.lower() == attr].iloc[0]
                missing_items.append({
                    "Key": row["Key"],
                    "Dimension": row["Dimension"],
                    "NameID": row["NameID"],
                    "Attribute": row["Attribute"],
                    "Value": row["Value"],
                    "Missing In": "Gamma"
                })
            for attr in missing_in_alfa:
                row = gamma_group[gamma_group["Attribute"].str.lower() == attr].iloc[0]
                missing_items.append({
                    "Key": row["Key"],
                    "Dimension": row["Dimension"],
                    "NameID": row["NameID"],
                    "Attribute": row["Attribute"],
                    "Value": row["Value"],
                    "Missing In": "Alfa"
                })
        
        df_missing = pd.DataFrame(missing_items)
        logging.info(f"[Missing Items] Total missing items: {len(df_missing)}")
        
        if df_missing.empty:
            logging.info("[Missing Items] No missing items found.")
            return
        
        # Merge with exceptions table if provided.
        if not df_exceptions.empty:
            # Only strip; do not lower-case the keys.
            df_missing["Key"] = df_missing["Key"].astype(str).str.strip()
            df_exceptions["Key"] = df_exceptions["Key"].astype(str).str.strip()
            
            df_missing = df_missing.merge(df_exceptions, on="Key", how="left")
            
            # If you rely on the 'hide exception' column being recognized in a case-insensitive
            # way, you can keep `.str.lower()`. Otherwise, remove it.
            df_missing["hide exception"] = df_missing["hide exception"].astype(str).str.lower().fillna("no")
            df_missing = df_missing[df_missing["hide exception"] != "yes"]
        
        # Add an Action Item column.
        df_missing["Action Item"] = ""
        final_columns = ["Key", "Dimension", "NameID", "Attribute", "Value",
                         "Comments_1", "Comments_2", "Action Item", "Missing In"]
        df_missing = df_missing.reindex(columns=final_columns)
        
        # Write to Excel.
        df_missing.to_excel(output_path, sheet_name="Missing_Items", index=False)
        logging.info(f"[Missing Items] Written output to {output_path}")
        
        # --- Apply Color Formatting ---
        try:
            wb = load_workbook(output_path)
            ws = wb["Missing_Items"]
            header_font = Font(bold=True)
            fill_header = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
            fill_gamma = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # green
            fill_alfa = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")   # blue

            header_row = next(ws.iter_rows(min_row=1, max_row=1))
            headers = {cell.value: cell.column for cell in header_row}
            for cell in header_row:
                cell.font = header_font
                cell.fill = fill_header

            required_cols = ["Dimension", "NameID", "Attribute", "Value", "Missing In"]
            if not all(col in headers for col in required_cols):
                logging.warning("[Missing Items] One or more required columns for coloring are missing.")
            else:
                for row_idx in range(2, ws.max_row + 1):
                    missing_in = str(ws.cell(row=row_idx, column=headers["Missing In"]).value).strip().lower()
                    if missing_in == "gamma":
                        fill_color = fill_gamma
                    elif missing_in == "alfa":
                        fill_color = fill_alfa
                    else:
                        continue
                    ws.cell(row=row_idx, column=headers["Dimension"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["NameID"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Attribute"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Value"]).fill = fill_color

            ws.freeze_panes = ws["A2"]
            wb.save(output_path)
            logging.info("[Missing Items] Applied color formatting to Excel.")
        except Exception as e:
            logging.error(f"[Missing Items] Error applying color formatting: {e}")
    
    except Exception as e:
        logging.error(f"[Missing Items] Error during creation of missing items Excel: {e}")

# ---------------------------
# 6) READ EXCEPTION TABLE
# ---------------------------
def read_exception_table(exception_file: Path) -> pd.DataFrame:
    """
    Reads an Excel file containing exceptions.
    Expected columns: Key, Comments_1, Comments_2, hide exception.
    """
    try:
        if not exception_file.is_file():
            logging.warning(f"[Exception] Exception file {exception_file} not found.")
            return pd.DataFrame()
        df = pd.read_excel(exception_file, sheet_name="Sheet1")
        required_cols = {"Key", "Comments_1", "Comments_2", "hide exception"}
        if not required_cols.issubset(set(df.columns)):
            missing = required_cols - set(df.columns)
            logging.warning(f"[Exception] Missing required columns: {missing}")
            return pd.DataFrame()
        logging.info(f"[Exception] Loaded {len(df)} exception rows.")
        return df
    except Exception as e:
        logging.error(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame()

# ---------------------------
# 7) MAIN FUNCTION
# ---------------------------
def main() -> None:
    try:
        log_file = Path("script.log")
        setup_logging(log_file)
        logging.info("Script started.")
        
        # Example: Read an exception table (optional).
        exception_file = Path("Exception_Table.xlsx")
        df_exceptions = read_exception_table(exception_file)
        
        # ALFA configuration (sample)
        alfa_file = Path("AlfaData.xlsx")
        alfa_pre_exclude = [("ColA", ["X"]), ("ColB", ["Y"])]  
        alfa_bad_dims = ["UnwantedDim"]
        alfa_bad_attrs = ["Debug"]
        alfa_dimension_rename = {"DimOld": "DimNew"}
        alfa_attribute_rename = {"First": "Name"}
        
        df_alfa = transform_alfa(
            alfa_file,
            pre_melt_exclude_rules=alfa_pre_exclude,
            bad_dimensions=alfa_bad_dims,
            bad_attributes=alfa_bad_attrs,
            dimension_rename=alfa_dimension_rename,
            attribute_rename=alfa_attribute_rename,
            sheet_name="Sheet1",
            skip_rows=3
        )
        logging.info(f"[Alfa] Transformed data contains {len(df_alfa)} rows.")
        
        # GAMMA configuration (sample)
        gamma_zip = Path("GammaData.zip")
        gamma_pre_exclude = [("RawCol", ["Z"])]
        gamma_bad_dims = ["TestDim"]
        gamma_bad_attrs = ["BadAttr"]
        gamma_dimension_rename = {"GammaOld": "GammaNew"}
        gamma_attribute_rename = {"First": "Name"}
        
        df_gamma = transform_gamma(
            gamma_zip,
            pre_melt_exclude_rules=gamma_pre_exclude,
            bad_dimensions=gamma_bad_dims,
            bad_attributes=gamma_bad_attrs,
            dimension_rename=gamma_dimension_rename,
            attribute_rename=gamma_attribute_rename,
            delimiter=",",
            remove_substring="_ceaster.txt"
        )
        logging.info(f"[Gamma] Transformed data contains {len(df_gamma)} rows.")
        
        # Create Missing Items Excel Report
        output_file = Path("Missing_Items.xlsx")
        create_missing_items_excel(df_alfa, df_gamma, df_exceptions, output_file)
        
        logging.info("Script completed successfully.")
    except Exception as e:
        logging.critical(f"[Main] Critical error: {e}")

if __name__ == "__main__":
    main()
