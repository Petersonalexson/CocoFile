#!/usr/bin/env python3
"""
Ultra-Mega Reconciliation (Mode=2 only, Exclude Disabled in ERP, Param-based Renames, 8-Charts Dashboard)
---------------------------------------------------------------------------------------------------------
Features:
- ERP:
  * Reads Excel, excludes Enabled_Flag == 'Disabled'
  * Meltdown with param-based dimension/attribute rename + allowed sets
  * If attribute becomes "Start Date" or "End Date" in meltdown => strip T... => YYYY-MM-DD
- Master:
  * Reads .txt from a ZIP robustly => .csv => unify => meltdown with param-based rename + allowed sets
  * Dimension set from filename, rename first col => "Name" if missing
- Compare => Mode=2 only
- Exceptions => merges an Exception_Table
- Dashboard => 8 chart frames (Heatmap, Lollipop, Circular, Scatter, Radar, Normal Pie, Normal Bar, Band Chart)
- UI => filter popups only for columns named "Start Date" or "End Date" (all other headings ignore clicks)
Author: Al Pacinohhh
Date: 2025
"""

import os
import json
import logging
import zipfile
import shutil
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime

import tkinter as tk
from tkinter import ttk, filedialog, messagebox

import customtkinter as ctk
import pandas as pd
import numpy as np

# Attempt advanced detection
try:
    import chardet
except ImportError:
    chardet = None

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# ------------------------------------------------------------------------------
# 1) LOGGING
# ------------------------------------------------------------------------------
def setup_logger():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
setup_logger()

# ------------------------------------------------------------------------------
# 2) DEFAULT CONFIG
# ------------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/Reconciliation.xlsx",
    "CONFIG_PATH": "config/ui_config.json",
    "PARAMETER_PATH": "data/parameters.xlsx",
    "MASTER_CSV_OUTPUT": "temp_master_csv"
}

def default_config() -> Dict:
    return {
        "paths": {
            "ERP_EXCEL_PATH": DEFAULT_PATHS["ERP_EXCEL_PATH"],
            "MASTER_ZIP_PATH": DEFAULT_PATHS["MASTER_ZIP_PATH"],
            "EXCEPTION_PATH": DEFAULT_PATHS["EXCEPTION_PATH"],
            "OUTPUT_PATH": DEFAULT_PATHS["OUTPUT_PATH"],
            "CONFIG_PATH": DEFAULT_PATHS["CONFIG_PATH"],
            "PARAMETER_PATH": DEFAULT_PATHS["PARAMETER_PATH"],
            "MASTER_CSV_OUTPUT": DEFAULT_PATHS["MASTER_CSV_OUTPUT"]
        },
        "erp_grid": {
            "columns": [],
            "filters": {}
        },
        "master_grid": {
            "columns": [],
            "filters": {}
        },
        "comparison_option": 2  # Mode=2 only by default
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config: {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# ------------------------------------------------------------------------------
# 3) TEXT LOGGER HANDLER
# ------------------------------------------------------------------------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget

    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)

    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")

# ------------------------------------------------------------------------------
# 4) PARAMETER FILE => dimension/attribute rename + allowed sets
# ------------------------------------------------------------------------------
def read_parameter_file(path: Path) -> Dict[str,object]:
    """
    Expects columns:
     "ERP Original Dimension", "ERP Renamed Dimension"
     "ERP Original Attribute", "ERP Renamed Attribute"
     "ERP Allowed Dimensions", "ERP Allowed Attributes"
     "Master Original Dimension", "Master Renamed Dimension"
     "Master Original Attribute", "Master Renamed Attribute"
     "Master Allowed Dimensions", "Master Allowed Attributes"
    We'll parse them into rename dict + allowed sets:
      param["erp_dim_map"], param["erp_attr_map"], param["erp_dim_allow"], param["erp_attr_allow"]
      param["master_dim_map"], param["master_attr_map"], param["master_dim_allow"], param["master_attr_allow"]
    """
    import pandas as pd
    if not path.is_file():
        logging.warning(f"Parameter file not found: {path}")
        return {
            "erp_dim_map": {},
            "erp_attr_map": {},
            "erp_dim_allow": set(),
            "erp_attr_allow": set(),
            "master_dim_map": {},
            "master_attr_map": {},
            "master_dim_allow": set(),
            "master_attr_allow": set()
        }
    try:
        dfp= pd.read_excel(path)
        dfp.columns= dfp.columns.astype(str).str.strip()

        param= {
            "erp_dim_map": {},
            "erp_attr_map": {},
            "erp_dim_allow": set(),
            "erp_attr_allow": set(),
            "master_dim_map": {},
            "master_attr_map": {},
            "master_dim_allow": set(),
            "master_attr_allow": set()
        }

        def s(x):
            return str(x).strip() if pd.notna(x) else ""

        # parse row by row
        for idx, row in dfp.iterrows():
            e_od= s(row.get("ERP Original Dimension",""))
            e_rd= s(row.get("ERP Renamed Dimension",""))
            if e_od and e_rd and e_od!=e_rd:
                param["erp_dim_map"][e_od]= e_rd

            e_oa= s(row.get("ERP Original Attribute",""))
            e_ra= s(row.get("ERP Renamed Attribute",""))
            if e_oa and e_ra and e_oa!=e_ra:
                param["erp_attr_map"][e_oa]= e_ra

            m_od= s(row.get("Master Original Dimension",""))
            m_rd= s(row.get("Master Renamed Dimension",""))
            if m_od and m_rd and m_od!=m_rd:
                param["master_dim_map"][m_od]= m_rd

            m_oa= s(row.get("Master Original Attribute",""))
            m_ra= s(row.get("Master Renamed Attribute",""))
            if m_oa and m_ra and m_oa!=m_ra:
                param["master_attr_map"][m_oa]= m_ra

            # allowed sets
            ead= s(row.get("ERP Allowed Dimensions",""))
            if ead:
                param["erp_dim_allow"].add(ead)
            eaa= s(row.get("ERP Allowed Attributes",""))
            if eaa:
                param["erp_attr_allow"].add(eaa)

            mad= s(row.get("Master Allowed Dimensions",""))
            if mad:
                param["master_dim_allow"].add(mad)
            maa= s(row.get("Master Allowed Attributes",""))
            if maa:
                param["master_attr_allow"].add(maa)

        return param

    except Exception as e:
        logging.error(f"Error reading param file: {e}")
        return {
            "erp_dim_map": {},
            "erp_attr_map": {},
            "erp_dim_allow": set(),
            "erp_attr_allow": set(),
            "master_dim_map": {},
            "master_attr_map": {},
            "master_dim_allow": set(),
            "master_attr_allow": set()
        }

# ------------------------------------------------------------------------------
# 5) ERP => Exclude 'Disabled', strip date
# ------------------------------------------------------------------------------
def read_erp_excluding_disabled(path: Path)-> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"ERP not found: {path}")
        return pd.DataFrame()
    import pandas as pd
    try:
        df= pd.read_excel(path, skiprows=3)
        df.columns= df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df= df[df["Enabled_Flag"]=="Enabled"]  # keep only 'Enabled'
        # strip date columns
        for c in df.columns:
            if "Date" in c:
                df[c]= df[c].astype(str).apply(lambda x: x.split("T")[0] if "T" in x else x)
        return df
    except Exception as e:
        logging.error(f"Error reading ERP: {e}")
        return pd.DataFrame()

# ------------------------------------------------------------------------------
# 6) MASTER .TXT => CSV => unify
# ------------------------------------------------------------------------------
def read_txt_robust_in_memory(raw: bytes)-> pd.DataFrame:
    import csv
    import io
    encs= [
        'utf-8-sig','utf-8','utf-16','utf-16-le','utf-16-be','utf-32','utf-32-le','utf-32-be',
        'cp1250','cp1251','cp1252','cp1254','cp1256','cp932','cp949','latin1','iso-8859-1','iso-8859-2',
        'windows-1250','windows-1251','windows-1252','windows-1254','windows-1256','shift_jis','euc_jp','euc_kr',
        'big5','big5hkscs','gb2312','gbk','gb18030'
    ]
    import pandas as pd
    for enc in encs:
        try:
            buf= io.BytesIO(raw)
            df= pd.read_csv(
                buf,
                encoding=enc,
                sep=",",
                on_bad_lines="skip",
                quoting=csv.QUOTE_MINIMAL,
                engine="python"
            )
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            return df
        except:
            pass
    logging.error("Could not parse Master .txt with known encodings.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path)-> List[Path]:
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    created_csvs=[]
    if not zip_path.is_file():
        logging.warning(f"Master ZIP not found: {zip_path}")
        return created_csvs

    import pandas as pd
    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files= [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name= os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw= fo.read()
                if not raw:
                    continue
                df= read_txt_robust_in_memory(raw)
                df.columns= df.columns.str.strip()
                # strip T in date
                for c in df.columns:
                    if "Date" in c:
                        df[c]= df[c].astype(str).apply(lambda x: x.split("T")[0] if "T" in x else x)

                # dimension from filename
                low= base_name.lower()
                if "_master.txt" in low:
                    dim_part= low.replace("_master.txt","")
                else:
                    dim_part, _= os.path.splitext(low)
                dim_part= dim_part.replace("_"," ").title()

                df["Dimension"]= dim_part
                if "Name" not in df.columns and len(df.columns)>0:
                    firstcol= df.columns[0]
                    df.rename(columns={firstcol:"Name"}, inplace=True)

                out_csv= out_dir / f"{dim_part}.csv"
                df.to_csv(out_csv, index=False, encoding="utf-8")
                created_csvs.append(out_csv)
            except Exception as e:
                logging.error(f"Error reading {txt_file}: {e}")
    return created_csvs

def unify_master_csvs(csv_files: List[Path])-> pd.DataFrame:
    import pandas as pd
    frames=[]
    for cp in csv_files:
        if not cp.is_file():
            continue
        try:
            df= pd.read_csv(cp, encoding="utf-8", on_bad_lines="skip")
            df.columns= df.columns.str.strip()
            frames.append(df)
        except Exception as e:
            logging.error(f"Error reading {cp}: {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()

# ------------------------------------------------------------------------------
# 7) MELTDOWN => rename, filter => mode=2 compare
# ------------------------------------------------------------------------------
def meltdown_erp(df: pd.DataFrame,
                 erp_dim_map: Dict[str,str],
                 erp_attr_map: Dict[str,str],
                 erp_dim_allow: Set[str],
                 erp_attr_allow: Set[str]) -> pd.DataFrame:
    """
    - meltdown => dimension=Dimension_Name => rename => erp_dim_map
    - attribute => rename => erp_attr_map
    - filter out dimension not in erp_dim_allow if set is non-empty
    - filter out attribute not in erp_attr_allow if set is non-empty
    - If attribute becomes "Start Date"/"End Date", strip T... from the meltdown Value
    """
    if df.empty:
        return df
    skip_cols= {"Col1","Col2","Enabled_Flag"}
    keep= [c for c in df.columns if c not in skip_cols]

    id_vars=[]
    if "Dimension_Name" in keep:
        id_vars.append("Dimension_Name")
    if "Value" in keep:
        id_vars.append("Value")
    value_vars= [c for c in keep if c not in id_vars]

    melted= df.melt(id_vars=id_vars, value_vars=value_vars,
                    var_name="Attribute", value_name="Value")

    melted.rename(columns={"Dimension_Name":"Dimension","Value":"RefName"}, inplace=True)
    # rename dimension
    melted["Dimension"]= melted["Dimension"].replace(erp_dim_map)
    # rename attribute
    melted["Attribute"]= melted["Attribute"].replace(erp_attr_map)

    # if attribute is "Start Date"/"End Date", strip T
    # We'll do that after meltdown
    def strip_t(x):
        # keep only up to first T
        return x.split("T")[0] if "T" in x else x

    melted["Value"]= melted.apply(
        lambda row: strip_t(row["Value"]) if row["Attribute"] in ["Start Date","End Date"] else row["Value"],
        axis=1
    )

    # filter dimension
    if erp_dim_allow:
        melted= melted[melted["Dimension"].isin(erp_dim_allow)]
    # filter attribute
    if erp_attr_allow:
        melted= melted[melted["Attribute"].isin(erp_attr_allow)]

    return melted[["Dimension","RefName","Attribute","Value"]].copy()

def meltdown_master(df: pd.DataFrame,
                    mst_dim_map: Dict[str,str],
                    mst_attr_map: Dict[str,str],
                    mst_dim_allow: Set[str],
                    mst_attr_allow: Set[str]) -> pd.DataFrame:
    """
    meltdown => dimension=Dimension => rename => mst_dim_map
    => attribute => rename => mst_attr_map
    => filter out dimension/attribute if not in allowed sets
    => if attribute => "Start Date"/"End Date", also strip T... from the meltdown Value
    """
    if df.empty:
        return df
    keep= df.columns.tolist()
    id_vars= [c for c in ["Dimension","Name"] if c in keep]
    value_vars= [c for c in keep if c not in id_vars]

    melted= df.melt(id_vars=id_vars, value_vars=value_vars,
                    var_name="Attribute", value_name="Value")
    melted.rename(columns={"Name":"RefName"}, inplace=True)

    # rename dimension
    melted["Dimension"]= melted["Dimension"].replace(mst_dim_map)
    # rename attribute
    melted["Attribute"]= melted["Attribute"].replace(mst_attr_map)

    # strip T if attribute => "Start Date"/"End Date"
    def strip_t(x):
        return x.split("T")[0] if "T" in x else x

    melted["Value"]= melted.apply(
        lambda row: strip_t(row["Value"]) if row["Attribute"] in ["Start Date","End Date"] else row["Value"],
        axis=1
    )

    if mst_dim_allow:
        melted= melted[melted["Dimension"].isin(mst_dim_allow)]
    if mst_attr_allow:
        melted= melted[melted["Attribute"].isin(mst_attr_allow)]

    return melted[["Dimension","RefName","Attribute","Value"]].copy()

def build_keys(df: pd.DataFrame)-> pd.DataFrame:
    df= df.copy()
    for c in ["Dimension","RefName","Attribute","Value"]:
        if c not in df.columns:
            df[c]=""
        df[c]= df[c].fillna("").astype(str).str.strip()
    df["GroupKey"]= df["Dimension"]+" | "+df["RefName"]
    df["Key"]= df["Dimension"]+" | "+df["RefName"]+" | "+df["Attribute"]+" | "+df["Value"]
    df["Comments_1"]=""
    df["Comments_2"]=""
    df["Action Item"]=""
    df["Missing In"]=""
    return df

def build_lookup_dict(df: pd.DataFrame)-> Dict[str,Dict[str,str]]:
    lookup={}
    for gk, grp in df.groupby("GroupKey"):
        rec={}
        if not grp.empty:
            ref= grp["RefName"].iloc[0]
        else:
            ref=""
        rec["Name"]= ref
        for _, row in grp.iterrows():
            rec[row["Attribute"]]= row["Value"]
        lookup[gk]= rec
    return lookup

def compare_mode2(df_erp: pd.DataFrame, df_mst: pd.DataFrame)-> pd.DataFrame:
    """
    If name missing => entire record missing
    else compare attributes
    """
    erp_dict= build_lookup_dict(df_erp)
    mst_dict= build_lookup_dict(df_mst)
    all_keys= set(erp_dict.keys())| set(mst_dict.keys())
    results=[]
    for gk in all_keys:
        dim= gk.split(" | ")[0]
        a_data= erp_dict.get(gk,{})
        b_data= mst_dict.get(gk,{})
        name_a= a_data.get("Name", a_data.get("RefName",""))
        name_b= b_data.get("Name", b_data.get("RefName",""))
        if name_a and name_b and (name_a==name_b):
            # partial attr compare
            all_attrs= (set(a_data.keys())| set(b_data.keys()))-{"Name"}
            for attr in all_attrs:
                va= a_data.get(attr,"")
                vb= b_data.get(attr,"")
                if va!= vb:
                    if va and not vb:
                        results.append({"Dimension":dim, "Name":name_a,"Attribute":attr,"Value":va,"Missing In":"MASTER"})
                    elif vb and not va:
                        results.append({"Dimension":dim, "Name":name_a,"Attribute":attr,"Value":vb,"Missing In":"ERP"})
                    else:
                        results.append({"Dimension":dim, "Name":name_a,"Attribute":attr,"Value":va,"Missing In":"MASTER"})
                        results.append({"Dimension":dim, "Name":name_a,"Attribute":attr,"Value":vb,"Missing In":"ERP"})
        else:
            # entire record missing
            if name_a and not name_b:
                results.append({"Dimension":dim, "Name":name_a,"Attribute":"Name","Value":name_a,"Missing In":"MASTER"})
            elif name_b and not name_a:
                results.append({"Dimension":dim, "Name":name_b,"Attribute":"Name","Value":name_b,"Missing In":"ERP"})

    df_diff= pd.DataFrame(results)
    if not df_diff.empty:
        df_diff["Key"]= (
            df_diff["Dimension"].str.strip()+" | "+
            df_diff["Name"].str.strip()+" | "+
            df_diff["Attribute"].str.strip()+" | "+
            df_diff["Value"].str.strip()
        )
    return df_diff

# ------------------------------------------------------------------------------
# 8) EXCEPTIONS
# ------------------------------------------------------------------------------
def read_exception_table(path: Path)-> pd.DataFrame:
    import pandas as pd
    if not path.is_file():
        logging.warning(f"Exception file not found: {path}")
        return pd.DataFrame()
    try:
        df= pd.read_excel(path)
        df.columns= df.columns.astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"Error reading exceptions: {e}")
        return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame)-> pd.DataFrame:
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep= [c for c in df_exc.columns if c in ["Key","Comments_1","Comments_2","hide exception"]]
    if not keep:
        return df
    exc= df_exc[keep].copy()
    exc["Key"]= exc["Key"].astype(str).str.strip()

    merged= df.merge(exc, on="Key", how="left", suffixes=("", "_exc"))
    merged["hide exception"]= merged.get("hide exception","").fillna("").str.lower()
    final= merged[merged["hide exception"]!="yes"].copy()

    if "Comments_1_exc" in final.columns:
        final["Comments_1"]= np.where(final["Comments_1_exc"].notna(), final["Comments_1_exc"], final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"]= np.where(final["Comments_2_exc"].notna(), final["Comments_2_exc"], final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)
    return final

# ------------------------------------------------------------------------------
# 9) WRITE RESULTS
# ------------------------------------------------------------------------------
def write_results(df: pd.DataFrame, out_path: Path):
    import pandas as pd
    if df.empty:
        logging.info("No differences => skip writing.")
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)

    final_cols= ["Key","Dimension","Name","Attribute","Value","Comments_1","Comments_2","Action Item","Missing In"]
    for c in final_cols:
        if c not in df.columns:
            df[c]=""
    df= df[final_cols]

    from openpyxl import Workbook
    from openpyxl.styles import PatternFill, Font, Alignment

    wb= Workbook()
    ws= wb.active
    ws.title= "Results"
    ws.append(final_cols)
    for rowvals in df.itertuples(index=False):
        ws.append(rowvals)
    header_font= Font(bold=True)
    fill= PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
    for cell in ws[1]:
        cell.font= header_font
        cell.fill= fill
        cell.alignment= Alignment(horizontal="center")

    for col in ws.columns:
        max_len=0
        col_letter= col[0].column_letter
        for cell in col:
            val= str(cell.value) if cell.value else ""
            max_len= max(max_len, len(val))
        ws.column_dimensions[col_letter].width= max_len+2

    ws.freeze_panes= "A2"
    wb.save(out_path)
    logging.info(f"Results => {out_path}")

# ------------------------------------------------------------------------------
# 10) DASHBOARD
# ------------------------------------------------------------------------------
class Dashboard(ctk.CTkFrame):
    """
    8 chart frames: Heatmap, Lollipop, Circular, Scatter, Radar, Normal Pie, Normal Bar, Band Chart
    """
    def __init__(self, parent):
        super().__init__(parent)
        self.df_current= pd.DataFrame()
        self.df_history= pd.DataFrame()

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        self.frame_heatmap   = ctk.CTkFrame(self.notebook)
        self.frame_lollipop  = ctk.CTkFrame(self.notebook)
        self.frame_circular  = ctk.CTkFrame(self.notebook)
        self.frame_scatter   = ctk.CTkFrame(self.notebook)
        self.frame_radar     = ctk.CTkFrame(self.notebook)
        self.frame_normalpie = ctk.CTkFrame(self.notebook)
        self.frame_normalbar = ctk.CTkFrame(self.notebook)
        self.frame_bandchart = ctk.CTkFrame(self.notebook)

        self.notebook.add(self.frame_heatmap,   text="Heatmap")
        self.notebook.add(self.frame_lollipop,  text="Lollipop Dim")
        self.notebook.add(self.frame_circular,  text="Circular Attr")
        self.notebook.add(self.frame_scatter,   text="Scatter")
        self.notebook.add(self.frame_radar,     text="Radar")
        self.notebook.add(self.frame_normalpie, text="Normal Pie")
        self.notebook.add(self.frame_normalbar, text="Normal Bar")
        self.notebook.add(self.frame_bandchart, text="Band Chart")

    def update_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current= df_current.copy()
        self.df_history= df_history.copy()

        self.plot_heatmap()
        self.plot_lollipop()
        self.plot_circular()
        self.plot_scatter()
        self.plot_radar()
        self.plot_normal_pie()
        self.plot_normal_bar()
        self.plot_band_chart()

    def plot_heatmap(self):
        for w in self.frame_heatmap.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        pivoted= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
        if pivoted.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        cax= ax.imshow(pivoted, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivoted.columns)))
        ax.set_yticks(range(len(pivoted.index)))
        ax.set_xticklabels(pivoted.columns, rotation=90)
        ax.set_yticklabels(pivoted.index)
        fig.colorbar(cax, ax=ax)
        ax.set_title("Heatmap: Missing (Dim x Attr)")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_heatmap)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_lollipop(self):
        for w in self.frame_lollipop.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_dim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        fig, ax= plt.subplots(figsize=(6,5))
        ax.hlines(y=count_dim.index, xmin=0, xmax=count_dim.values, color="skyblue")
        ax.plot(count_dim.values, count_dim.index, "o", color="skyblue")
        ax.set_title("Lollipop: Missing Dimensions")
        ax.set_xlabel("Missing Count")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_lollipop)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_circular(self):
        for w in self.frame_circular.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_attr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if count_attr.empty:
            return

        categories= count_attr.index.tolist()
        values= count_attr.values
        N= len(categories)
        angles= np.linspace(0, 2*np.pi, N, endpoint=False).tolist()

        fig= plt.figure(figsize=(6,6))
        ax= fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(categories, fontsize=9)
        ax.bar(angles, values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular Barplot: Missing Attributes", y=1.05)
        canvas= FigureCanvasTkAgg(fig, master=self.frame_circular)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_scatter(self):
        for w in self.frame_scatter.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_dim= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        if count_dim.empty:
            return
        count_dim.sort_values("Count", ascending=False, inplace=True)
        xvals= np.arange(len(count_dim))
        yvals= count_dim["Count"].values
        labels= count_dim["Dimension"].values

        fig, ax= plt.subplots(figsize=(6,5))
        ax.scatter(xvals, yvals, color="green")
        for i, txt in enumerate(labels):
            ax.text(xvals[i], yvals[i], txt, ha="center", va="bottom", rotation=60)
        ax.set_xticks([])
        ax.set_ylabel("Missing Count")
        ax.set_title("Scatter: Missing by Dimension")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_scatter)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_radar(self):
        for w in self.frame_radar.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_dim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(5)
        if count_dim.empty:
            return
        categories= count_dim.index.tolist()
        values= count_dim.values.tolist()
        N= len(categories)
        angles= np.linspace(0,2*np.pi,N,endpoint=False).tolist()
        angles+= angles[:1]
        values+= values[:1]

        fig= plt.figure(figsize=(6,6))
        ax= fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=9)
        ax.plot(angles, values, color="red", linewidth=2)
        ax.fill(angles, values, color="red", alpha=0.3)
        ax.set_title("Radar: Top 5 Missing Dimensions", y=1.08)
        canvas= FigureCanvasTkAgg(fig, master=self.frame_radar)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_normal_pie(self):
        for w in self.frame_normalpie.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        dist= df_m["Missing In"].value_counts()
        fig, ax= plt.subplots(figsize=(5,5))
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Normal Pie: Missing In Distribution")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_normalpie)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_normal_bar(self):
        for w in self.frame_normalbar.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_attr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        fig, ax= plt.subplots(figsize=(6,4))
        count_attr.plot(kind="bar", ax=ax, color="blue")
        ax.set_ylabel("Missing Count")
        ax.set_title("Normal Bar: Top 10 Missing Attributes")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_normalbar)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_band_chart(self):
        for w in self.frame_bandchart.winfo_children():
            w.destroy()
        if self.df_history.empty or "RunDate" not in self.df_history.columns:
            return
        df_m= self.df_history[self.df_history["Missing In"]!=""]
        if df_m.empty:
            return
        date_counts= df_m.groupby("RunDate")["Key"].count().reset_index()
        date_counts.sort_values("RunDate", inplace=True)
        date_counts["Count_min"]= date_counts["Key"]*0.9
        date_counts["Count_max"]= date_counts["Key"]*1.1

        fig, ax= plt.subplots(figsize=(6,4))
        ax.plot(date_counts["RunDate"], date_counts["Key"], color="purple", label="Missing Count")
        ax.fill_between(date_counts["RunDate"], date_counts["Count_min"], date_counts["Count_max"],
                        color="purple", alpha=0.2, label="±10% band")
        ax.set_title("Band Chart Over Days")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Missing Count")
        plt.xticks(rotation=45)
        ax.legend()

        for i, row in date_counts.iterrows():
            ax.text(row["RunDate"], row["Key"], str(row["Key"]), ha="center", va="bottom")

        canvas= FigureCanvasTkAgg(fig, master=self.frame_bandchart)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

# ------------------------------------------------------------------------------
# 11) EXCEL GRID (ONLY START/END DATE FILTERABLE)
# ------------------------------------------------------------------------------
class ExcelGrid(ctk.CTkFrame):
    """
    All columns displayed, but only columns named "Start Date" or "End Date" allow a filter popup.
    The user can rename/hide columns in "Manage Columns" but won't get KeyErrors because
    we only open the filter popup if heading is "Start Date"/"End Date".
    """
    FILTERABLE_COLS= {"Start Date","End Date"}

    def __init__(self, parent, config_block: Dict, name: str):
        super().__init__(parent)
        self.name= name
        self.col_defs= config_block.get("columns", [])
        self.filters: Dict[str,Set]= {k:set(v) for k,v in config_block.get("filters",{}).items()}
        self.df= pd.DataFrame()

        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        tb= ctk.CTkFrame(self)
        tb.pack(fill="x", padx=5, pady=5)
        ctk.CTkButton(tb, text="Manage Columns", command=self.show_column_manager).pack(side="left", padx=5)
        ctk.CTkButton(tb, text="Clear Filters", command=self.clear_filters).pack(side="left", padx=5)

    def create_table(self):
        container= ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)

        self.tree= ttk.Treeview(container, show="headings")
        vsb= ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb= ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)

        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")

        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

    def create_statusbar(self):
        self.status_label= ctk.CTkLabel(self, text="Ready")
        self.status_label.pack(fill="x")

    def set_data(self, df: pd.DataFrame):
        self.df= df.copy(deep=True)
        existing_ids= [c["id"] for c in self.col_defs]
        for col in self.df.columns:
            if col not in existing_ids:
                self.col_defs.append({
                    "id":col,
                    "name":col,
                    "locked":False,
                    "visible":True,
                    "renameable":True
                })
        self.refresh_table()

    def get_config_block(self)-> Dict:
        return {
            "columns": self.col_defs,
            "filters": {cid: sorted(list(vals)) for cid, vals in self.filters.items()}
        }

    def get_filtered_df(self)-> pd.DataFrame:
        if self.df.empty:
            return self.df
        df_f= self.df.copy()

        def passes_filter(x, allowed):
            if pd.isna(x):
                return any(pd.isna(a) for a in allowed)
            else:
                return x in allowed

        for col_id, allowed_vals in self.filters.items():
            if col_id in df_f.columns and allowed_vals:
                df_f= df_f[df_f[col_id].apply(lambda x: passes_filter(x, allowed_vals))]

        vis_ids= [c["id"] for c in self.col_defs if c.get("visible",True)]
        vis_ids= [c for c in vis_ids if c in df_f.columns]
        return df_f[vis_ids]

    def refresh_table(self):
        for item in self.tree.get_children():
            self.tree.delete(item)

        visible_cols= [c for c in self.col_defs if c.get("visible",True)]
        self.tree["columns"]= [c["id"] for c in visible_cols]

        for col_def in visible_cols:
            self.tree.heading(
                col_def["id"],
                text= self.heading_text(col_def),
                anchor="w",
                command=lambda c=col_def: self.on_heading_click(c)
            )
            self.tree.column(col_def["id"], anchor="w", width=150)

        df_f= self.get_filtered_df()
        for idx, row in df_f.iterrows():
            vals= []
            for c in visible_cols:
                cid= c["id"]
                if cid in df_f.columns:
                    vals.append(row[cid])
                else:
                    vals.append("")
            self.tree.insert("", "end", values=vals)

        self.status_label.configure(text=f"{len(df_f)} rows")

    def heading_text(self, col_def: Dict)-> str:
        txt= col_def["name"]
        if col_def.get("locked",False):
            txt+= " \U0001F512"
        return txt

    def on_heading_click(self, col_def: Dict):
        # Only open filter if col_def["name"] in FILTERABLE_COLS
        if col_def["name"] in self.FILTERABLE_COLS:
            self.show_filter_popup(col_def)
        else:
            # ignore
            pass

    def show_filter_popup(self, col_def: Dict):
        col_id= col_def["id"]
        if self.df.empty or col_id not in self.df.columns:
            return
        popup= tk.Toplevel(self)
        popup.title(f"Filter: {col_def['name']}")
        popup.geometry("320x500")

        frame= ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)

        unique_vals= self.df[col_id].unique()
        display_map={}
        for v in unique_vals:
            if pd.isna(v):
                dsp= "(NaN)"
            elif isinstance(v,str) and not v.strip():
                dsp= "(blank)"
            else:
                dsp= str(v)
            display_map[v]= dsp

        # sort by display_map
        sorted_vals= sorted(display_map.keys(), key=lambda x: display_map[x].lower())
        curr_filter= self.filters.get(col_id, set())
        if not curr_filter:
            curr_filter= set(unique_vals)

        all_var= tk.BooleanVar(value=True)
        def toggle_all():
            check= all_var.get()
            for vb in var_dict.values():
                vb.set(check)

        ctk.CTkCheckBox(frame, text="Select All", variable=all_var, command=toggle_all).pack(anchor="w", pady=5)

        scroll= ctk.CTkScrollableFrame(frame, width=280, height=320)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)

        var_dict={}
        for rv in sorted_vals:
            if pd.isna(rv):
                in_filter= any(pd.isna(a) for a in curr_filter)
            else:
                in_filter= (rv in curr_filter)
            bvar= tk.BooleanVar(value=in_filter)
            var_dict[rv]= bvar
            ctk.CTkCheckBox(scroll, text= display_map[rv], variable=bvar).pack(anchor="w")

        def apply_():
            sel= set()
            for rv, vb in var_dict.items():
                if vb.get():
                    sel.add(rv)
            self.filters[col_id]= sel
            popup.destroy()
            self.refresh_table()

        bf= ctk.CTkFrame(frame)
        bf.pack(fill="x", pady=5)
        ctk.CTkButton(bf, text="Apply", command=apply_).pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Cancel", command=popup.destroy).pack(side="left", padx=5)

    def show_column_manager(self):
        cm= tk.Toplevel(self)
        cm.title(f"{self.name} Column Manager")
        scrolled= ctk.CTkScrollableFrame(cm, width=600, height=500)
        scrolled.pack(fill="both", expand=True)

        for i, col_def in enumerate(self.col_defs):
            rowf= ctk.CTkFrame(scrolled)
            rowf.pack(fill="x", pady=2)

            if col_def.get("locked",False):
                txt= col_def["name"]+" \U0001F512"
                ctk.CTkLabel(rowf, text=txt).pack(side="left", padx=5)
                continue

            var_vis= tk.BooleanVar(value= col_def.get("visible",True))
            def toggler(c=col_def,v=var_vis):
                c["visible"]= v.get()
                self.refresh_table()

            ctk.CTkCheckBox(rowf, text="", variable=var_vis, command=toggler).pack(side="left")

            if col_def.get("renameable",True):
                ctk.CTkButton(rowf, text=col_def["name"], command=lambda c=col_def: self.rename_column(c)).pack(side="left", padx=5)
            else:
                ctk.CTkLabel(rowf, text=col_def["name"]).pack(side="left", padx=5)

            ctk.CTkButton(rowf, text="↑", width=30, command=lambda idx=i: self.move_column(idx, -1)).pack(side="right", padx=2)
            ctk.CTkButton(rowf, text="↓", width=30, command=lambda idx=i: self.move_column(idx, 1)).pack(side="right", padx=2)

    def rename_column(self, col_def: Dict):
        old_name= col_def["name"]
        new_name= simpledialog.askstring("Rename Column", f"New name for {old_name}:", initialvalue= old_name)
        if new_name:
            col_def["name"]= new_name
            self.refresh_table()

    def move_column(self, idx: int, delta: int):
        new_idx= idx+ delta
        if 0 <= new_idx< len(self.col_defs):
            self.col_defs[idx], self.col_defs[new_idx]= self.col_defs[new_idx], self.col_defs[idx]
            self.refresh_table()

    def clear_filters(self):
        self.filters.clear()
        self.refresh_table()

# ------------------------------------------------------------------------------
# 12) MAIN APP => MODE=2
# ------------------------------------------------------------------------------
class MainApp(ctk.CTk):
    """
    Steps:
    - ERP => read Excel => only 'Enabled', meltdown => rename/filter from param => compare_mode_2
    - Master => .txt => robust => meltdown => rename/filter from param => compare
    - 8-charts dashboard
    - Filter popups => only "Start Date"/"End Date"
    """
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation: Mode=2 only, Param-based rename, Exclude Disabled, 8 Charts")
        self.geometry("1600x900")

        self.config_dict= load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.history_df= pd.DataFrame()

        self.param_dict= read_parameter_file(Path(self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"])))

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        # 1) Paths
        self.tab_paths= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_paths, text="Paths")
        self.build_paths_tab(self.tab_paths)

        # 2) ERP
        self.tab_erp= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_erp, text="ERP")
        self.erp_grid= ExcelGrid(self.tab_erp, self.config_dict["erp_grid"], "ERP")
        self.erp_grid.pack(fill="both", expand=True)

        # 3) Master
        self.tab_master= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_master, text="Master")
        self.master_grid= ExcelGrid(self.tab_master, self.config_dict["master_grid"], "Master")
        self.master_grid.pack(fill="both", expand=True)

        # 4) Compare & Exceptions
        self.tab_compare= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_compare, text="Compare & Exceptions")
        self.build_compare_tab(self.tab_compare)

        # 5) Dashboard
        self.tab_dashboard= Dashboard(self.notebook)
        self.notebook.add(self.tab_dashboard, text="Dashboard")

        # Logging
        self.log_box= ctk.CTkTextbox(self, height=120)
        self.log_box.pack(fill="both", expand=False)
        self.log_box.configure(state="disabled")
        handler= TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        # local var for Master CSV
        self.temp_csv_dir= Path(self.config_dict["paths"].get("MASTER_CSV_OUTPUT","temp_master_csv"))
        self.temp_csv_dir.mkdir(exist_ok=True)

        # load data
        self.refresh_erp_data()
        self.refresh_master_data()

    def build_paths_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        self.erp_var= tk.StringVar(value=self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mst_var= tk.StringVar(value=self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_var= tk.StringVar(value=self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_var= tk.StringVar(value=self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))
        self.cfg_var= tk.StringVar(value=self.config_dict["paths"].get("CONFIG_PATH", DEFAULT_PATHS["CONFIG_PATH"]))
        self.par_var= tk.StringVar(value=self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"]))
        self.csv_var= tk.StringVar(value=self.config_dict["paths"].get("MASTER_CSV_OUTPUT", DEFAULT_PATHS["MASTER_CSV_OUTPUT"]))

        def mkrow(lbl, var, is_dir=False):
            rowf= ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=180).pack(side="left", padx=5)
            e= ctk.CTkEntry(rowf, textvariable=var, width=600)
            e.pack(side="left", padx=5)
            def br():
                if is_dir:
                    p= filedialog.askdirectory()
                else:
                    p= filedialog.askopenfilename()
                if p:
                    var.set(p)
            ctk.CTkButton(rowf, text="Browse", command=br).pack(side="left", padx=5)

        mkrow("ERP Excel Path:", self.erp_var)
        mkrow("Master ZIP Path:", self.mst_var)
        mkrow("Exception Path:", self.exc_var)
        mkrow("Output Excel Path:", self.out_var)
        mkrow("JSON Config Path:", self.cfg_var)
        mkrow("Parameter File Path:", self.par_var)
        mkrow("Master CSV Folder:", self.csv_var, is_dir=True)

    def build_compare_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Mode=2 Only").pack(pady=5)

        btnf= ctk.CTkFrame(frm)
        btnf.pack(fill="x", pady=5)
        ctk.CTkButton(btnf, text="Run Comparison", command=self.run_comparison).pack(side="left", padx=5)
        ctk.CTkButton(btnf, text="Save Config", command=self.save_all_config).pack(side="left", padx=5)

    def refresh_erp_data(self):
        df_erp= read_erp_excluding_disabled(Path(self.erp_var.get().strip()))
        self.erp_grid.set_data(df_erp)

    def refresh_master_data(self):
        zip_path= Path(self.mst_var.get().strip())
        out_dir= Path(self.csv_var.get().strip())
        csvs= convert_master_txt_to_csv(zip_path, out_dir)
        df_master= unify_master_csvs(csvs)
        self.master_grid.set_data(df_master)

    def run_comparison(self):
        # update config paths
        self.config_dict["paths"]["ERP_EXCEL_PATH"]= self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mst_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"]= self.cfg_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"]= self.par_var.get().strip()
        self.config_dict["paths"]["MASTER_CSV_OUTPUT"]= self.csv_var.get().strip()

        self.config_dict["comparison_option"]= 2  # Mode=2 only

        # re-read param in case changed
        param= read_parameter_file(Path(self.par_var.get().strip()))

        # meltdown ERP
        df_erp_filt= self.erp_grid.get_filtered_df()
        erp_melt= meltdown_erp(
            df_erp_filt,
            erp_dim_map= param["erp_dim_map"],
            erp_attr_map= param["erp_attr_map"],
            erp_dim_allow= param["erp_dim_allow"],
            erp_attr_allow= param["erp_attr_allow"]
        )
        erp_ready= build_keys(erp_melt)

        # meltdown Master
        df_mst_filt= self.master_grid.get_filtered_df()
        mst_melt= meltdown_master(
            df_mst_filt,
            mst_dim_map= param["master_dim_map"],
            mst_attr_map= param["master_attr_map"],
            mst_dim_allow= param["master_dim_allow"],
            mst_attr_allow= param["master_attr_allow"]
        )
        mst_ready= build_keys(mst_melt)

        # compare => mode2
        df_diff= compare_mode2(erp_ready, mst_ready)

        # exceptions
        exc_path= Path(self.exc_var.get().strip())
        df_exc= read_exception_table(exc_path)
        final= merge_exceptions(df_diff, df_exc)

        # write results
        out_path= Path(self.out_var.get().strip())
        write_results(final, out_path)

        # add to dashboard
        run_date= datetime.now().strftime("%Y-%m-%d")
        final["RunDate"]= run_date
        self.history_df= pd.concat([self.history_df, final], ignore_index=True)

        self.notebook.select(self.tab_dashboard)
        self.tab_dashboard.update_data(final, self.history_df)

        messagebox.showinfo("Done", f"Comparison => mode=2 done. Output => {out_path}")

    def save_all_config(self):
        self.config_dict["erp_grid"]= self.erp_grid.get_config_block()
        self.config_dict["master_grid"]= self.master_grid.get_config_block()

        self.config_dict["paths"]["ERP_EXCEL_PATH"]= self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mst_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"]= self.cfg_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"]= self.par_var.get().strip()
        self.config_dict["paths"]["MASTER_CSV_OUTPUT"]= self.csv_var.get().strip()

        self.config_dict["comparison_option"]= 2

        save_config(self.config_dict, Path(self.cfg_var.get().strip()))
        messagebox.showinfo("Saved", "All config saved successfully.")

# ------------------------------------------------------------------------------
# 13) MAIN
# ------------------------------------------------------------------------------
def main():
    app= MainApp()
    app.mainloop()

if __name__=="__main__":
    main()
