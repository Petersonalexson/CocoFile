#!/usr/bin/env python3
"""
Ultra-Mega Reconciliation (Extract Master as CSV, Only 'Enabled', Mode=2, Filter Start/End Date, Param-based, 8 Charts)
---------------------------------------------------------------------------------------------------------------
1) Master .txt -> .csv extraction:
   - For each .txt in ZIP, robust read -> we create a .csv in a temp folder,
   - We derive "Dimension" from the filename, e.g. removing "_master.csv" or underscores, etc.
   - Then we unify them into one Master DataFrame.
2) ERP:
   - Excludes Enabled_Flag == 'Disabled',
   - Strips any date columns => 'YYYY-MM-DD'.
3) Parameter File:
   - Columns exactly:
       "ERP Original Dimension", "ERP Renamed Dimension"
       "Master Original Dimension", "Master Renamed Dimension"
       "ERP Original Attribute", "ERP Renamed Attribute"
       "Master Original Attribute", "Master Renamed Attribute"
       "ERP Allowed Dimensions", "ERP Allowed Attributes"
       "Master Allowed Dimensions", "Master Allowed Attributes"
   - Used to rename dimension/attribute + filter dimension/attribute not in "allowed" sets.
4) Compare => Mode=2 (Name missing => entire record missing, else compare attributes).
5) Dashboard => 8 chart frames (Heatmap, Lollipop, Circular, Scatter, Radar, Normal Pie, Normal Bar, Band Chart).
6) GUI:
   - No rename/hide columns from user.
   - Only "Start Date" or "End Date" columns are filterable in both ERP & Master. All other headings ignore filter clicks.
7) Output => an Excel with the differences, plus we have a dashboard for analysis.

Author: Al Pacino
Date: 2025
"""

import os
import json
import logging
import zipfile
import shutil
import time
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime

import tkinter as tk
from tkinter import ttk, filedialog, messagebox

import customtkinter as ctk
import pandas as pd
import numpy as np

# Attempt advanced encoding detection
try:
    import chardet
except ImportError:
    chardet = None

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# ------------------------------------------------------------------------------
# 1) LOGGING
# ------------------------------------------------------------------------------
def setup_logger():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
setup_logger()

# ------------------------------------------------------------------------------
# 2) DEFAULT CONFIG
# ------------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/Reconciliation.xlsx",
    "PARAMETER_PATH": "data/parameters.xlsx",
    "CONFIG_PATH": "config/ui_config.json",
    "MASTER_CSV_OUTPUT": "temp_master_csv"  # where we store .csv from .txt
}

def default_config() -> Dict:
    return {
        "paths": {
            "ERP_EXCEL_PATH": DEFAULT_PATHS["ERP_EXCEL_PATH"],
            "MASTER_ZIP_PATH": DEFAULT_PATHS["MASTER_ZIP_PATH"],
            "EXCEPTION_PATH": DEFAULT_PATHS["EXCEPTION_PATH"],
            "OUTPUT_PATH": DEFAULT_PATHS["OUTPUT_PATH"],
            "PARAMETER_PATH": DEFAULT_PATHS["PARAMETER_PATH"],
            "CONFIG_PATH": DEFAULT_PATHS["CONFIG_PATH"],
            "MASTER_CSV_OUTPUT": DEFAULT_PATHS["MASTER_CSV_OUTPUT"]
        },
        "erp_grid": {
            "columns": [],
            "filters": {}
        },
        "master_grid": {
            "columns": [],
            "filters": {}
        },
        "comparison_option": 2
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config: {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# ------------------------------------------------------------------------------
# 3) TEXT LOGGER HANDLER
# ------------------------------------------------------------------------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget

    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)

    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")

# ------------------------------------------------------------------------------
# 4) PARAMETER FILE
# ------------------------------------------------------------------------------
def read_parameters_excel(path: Path) -> Dict[str, object]:
    """
    Expects columns:
     - "ERP Original Dimension", "ERP Renamed Dimension"
     - "Master Original Dimension", "Master Renamed Dimension"
     - "ERP Original Attribute",  "ERP Renamed Attribute"
     - "Master Original Attribute","Master Renamed Attribute"
     - "ERP Allowed Dimensions", "ERP Allowed Attributes"
     - "Master Allowed Dimensions","Master Allowed Attributes"
    Returns rename dicts + allowed sets.
    """
    if not path.is_file():
        logging.warning(f"Parameter file not found: {path}")
        return {
            "erp_dim_map": {},
            "master_dim_map": {},
            "erp_attr_map": {},
            "master_attr_map": {},
            "erp_dim_allow": set(),
            "erp_attr_allow": set(),
            "master_dim_allow": set(),
            "master_attr_allow": set()
        }
    try:
        dfp = pd.read_excel(path)
        dfp.columns = dfp.columns.str.strip()

        param = {
            "erp_dim_map": {},
            "master_dim_map": {},
            "erp_attr_map": {},
            "master_attr_map": {},
            "erp_dim_allow": set(),
            "erp_attr_allow": set(),
            "master_dim_allow": set(),
            "master_attr_allow": set()
        }

        def s(x):
            return str(x).strip() if pd.notna(x) else ""

        # Allowed sets
        if "ERP Allowed Dimensions" in dfp.columns:
            param["erp_dim_allow"] = set(dfp["ERP Allowed Dimensions"].dropna().unique())
        if "ERP Allowed Attributes" in dfp.columns:
            param["erp_attr_allow"] = set(dfp["ERP Allowed Attributes"].dropna().unique())
        if "Master Allowed Dimensions" in dfp.columns:
            param["master_dim_allow"] = set(dfp["Master Allowed Dimensions"].dropna().unique())
        if "Master Allowed Attributes" in dfp.columns:
            param["master_attr_allow"] = set(dfp["Master Allowed Attributes"].dropna().unique())

        for _, row in dfp.iterrows():
            e_od = s(row.get("ERP Original Dimension",""))
            e_rd = s(row.get("ERP Renamed Dimension",""))
            if e_od and e_rd and e_od!=e_rd:
                param["erp_dim_map"][e_od] = e_rd

            m_od = s(row.get("Master Original Dimension",""))
            m_rd = s(row.get("Master Renamed Dimension",""))
            if m_od and m_rd and m_od!=m_rd:
                param["master_dim_map"][m_od] = m_rd

            e_oa = s(row.get("ERP Original Attribute",""))
            e_ra = s(row.get("ERP Renamed Attribute",""))
            if e_oa and e_ra and e_oa!=e_ra:
                param["erp_attr_map"][e_oa] = e_ra

            m_oa = s(row.get("Master Original Attribute",""))
            m_ra = s(row.get("Master Renamed Attribute",""))
            if m_oa and m_ra and m_oa!=m_ra:
                param["master_attr_map"][m_oa] = m_ra

        return param
    except Exception as e:
        logging.error(f"Error reading param file: {e}")
        return {
            "erp_dim_map": {},
            "master_dim_map": {},
            "erp_attr_map": {},
            "master_attr_map": {},
            "erp_dim_allow": set(),
            "erp_attr_allow": set(),
            "master_dim_allow": set(),
            "master_attr_allow": set()
        }

# ------------------------------------------------------------------------------
# 5) EXTRACT MASTER .TXT -> .CSV, DERIVE DIMENSION FROM FILENAME
# ------------------------------------------------------------------------------
def robust_read_txt_to_df(filebytes: bytes) -> pd.DataFrame:
    """
    Try multiple encodings, reading CSV from .txt bytes in memory.
    """
    import csv
    import io
    encodings = [
        'utf-8-sig','utf-8','utf-16','utf-16-le','utf-16-be','utf-32','utf-32-le','utf-32-be',
        'cp1250','cp1251','cp1252','cp1254','cp1256','cp932','cp949',
        'latin1','iso-8859-1','iso-8859-2','windows-1250','windows-1251',
        'windows-1252','windows-1254','windows-1256','shift_jis',
        'euc_jp','euc_kr','big5','big5hkscs','gb2312','gbk','gb18030'
    ]
    for enc in encodings:
        try:
            buf = io.BytesIO(filebytes)
            df = pd.read_csv(
                buf,
                encoding=enc,
                sep=",",
                on_bad_lines="skip",
                quoting=csv.QUOTE_MINIMAL,
                engine="python"
            )
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            return df
        except:
            pass
    logging.error("Could not parse .txt with known encodings.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path) -> List[Path]:
    """
    Extract each .txt from ZIP, read robustly -> store as .csv in out_dir
    Derive dimension from filename, e.g. removing "_master.txt".
    Then we add that dimension as a column in the DataFrame, then to_csv.
    Returns a list of created CSV paths.
    """
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)

    created_csvs = []
    if not zip_path.is_file():
        logging.warning(f"Master ZIP not found: {zip_path}")
        return created_csvs

    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name = os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw = fo.read()
                if not raw:
                    continue
                df_part = robust_read_txt_to_df(raw)
                df_part.columns = df_part.columns.str.strip()

                # parse date columns => strip T
                for c in df_part.columns:
                    if "Date" in c:
                        df_part[c] = df_part[c].astype(str).apply(lambda x: x.split("T")[0] if "T" in x else x)

                # Derive dimension from filename
                # e.g. if file is "some_dim_master.txt", we remove "_master.txt"
                # or if "some_dim.txt" we remove ".txt" etc.
                name_lower = base_name.lower()
                if name_lower.endswith("_master.txt"):
                    # remove "_master.txt"
                    dimension_part = name_lower.replace("_master.txt","")
                else:
                    dimension_part = os.path.splitext(name_lower)[0]

                # remove underscores => spaces, or just remove them
                dimension_part = dimension_part.replace("_"," ")
                # Title-case it 
                dimension_str = dimension_part.title()

                # Add dimension col
                df_part["Dimension"] = dimension_str

                # If first col is not 'Value', rename e.g. first col => 'Value'
                if "Value" not in df_part.columns and len(df_part.columns)>0:
                    first_col = df_part.columns[0]
                    df_part.rename(columns={first_col:"Value"}, inplace=True)

                # output CSV path
                out_csv_name = dimension_str.replace(" ","_") + ".csv"
                out_csv_path = out_dir / out_csv_name
                df_part.to_csv(out_csv_path, index=False, encoding="utf-8")
                created_csvs.append(out_csv_path)

            except Exception as e:
                logging.error(f"Error reading {txt_file} from ZIP: {e}")
    return created_csvs

def unify_master_csvs(csv_paths: List[Path]) -> pd.DataFrame:
    """
    read each .csv, unify => one DataFrame
    """
    frames=[]
    for cp in csv_paths:
        if not cp.is_file():
            continue
        try:
            dfp = pd.read_csv(cp, encoding="utf-8", on_bad_lines="skip")
            dfp.columns = dfp.columns.str.strip()
            frames.append(dfp)
        except Exception as e:
            logging.error(f"Error reading {cp}: {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    else:
        return pd.DataFrame()

# ------------------------------------------------------------------------------
# 6) ERP READING
# ------------------------------------------------------------------------------
def read_erp_excel(path: Path) -> pd.DataFrame:
    """
    skiprows=3, exclude Enabled_Flag=='Disabled', strip date columns => 'YYYY-MM-DD'
    """
    if not path.is_file():
        logging.warning(f"ERP Excel not found: {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path, skiprows=3)
        df.columns= df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df= df[df["Enabled_Flag"]!="Disabled"]
        # strip T from date
        for c in df.columns:
            if "Date" in c:
                df[c] = df[c].astype(str).apply(lambda x: x.split("T")[0] if "T" in x else x)
        return df
    except Exception as e:
        logging.error(f"Error reading ERP: {e}")
        return pd.DataFrame()

# ------------------------------------------------------------------------------
# 7) MELTDOWN, COMPARE
# ------------------------------------------------------------------------------
def meltdown_erp(df: pd.DataFrame,
                 erp_dim_map: Dict[str,str],
                 erp_attr_map: Dict[str,str],
                 erp_dim_allow: Set[str],
                 erp_attr_allow: Set[str]) -> pd.DataFrame:
    """
    ensure Dimension/Value exist, meltdown => rename dimension => erp_dim_map, rename attribute => erp_attr_map
    filter dimension => erp_dim_allow, attribute => erp_attr_allow
    """
    if df.empty:
        return df
    if "Dimension" not in df.columns:
        df["Dimension"] = ""
    if "Value" not in df.columns:
        for c in ["Name","RefName"]:
            if c in df.columns:
                df.rename(columns={c:"Value"}, inplace=True)
                break
        if "Value" not in df.columns:
            df["Value"]=""

    keep_cols= df.columns.tolist()
    id_vars= ["Dimension","Value"]
    value_vars= [c for c in keep_cols if c not in id_vars]

    melted= df.melt(id_vars=id_vars, value_vars=value_vars,
                    var_name="Attribute", value_name="Value_melted")

    melted["Dimension"] = melted["Dimension"].replace(erp_dim_map)
    melted["Attribute"] = melted["Attribute"].replace(erp_attr_map)

    if erp_dim_allow:
        melted= melted[melted["Dimension"].isin(erp_dim_allow)]
    if erp_attr_allow:
        melted= melted[melted["Attribute"].isin(erp_attr_allow)]

    melted.rename(columns={"Value":"RefName","Value_melted":"Value"}, inplace=True)
    return melted[["Dimension","RefName","Attribute","Value"]].copy()

def meltdown_master(df: pd.DataFrame,
                    mst_dim_map: Dict[str,str],
                    mst_attr_map: Dict[str,str],
                    mst_dim_allow: Set[str],
                    mst_attr_allow: Set[str]) -> pd.DataFrame:
    if df.empty:
        return df
    if "Dimension" not in df.columns:
        df["Dimension"]=""
    if "Value" not in df.columns:
        for c in ["Name","RefName"]:
            if c in df.columns:
                df.rename(columns={c:"Value"}, inplace=True)
                break
        if "Value" not in df.columns:
            df["Value"]=""

    keep_cols= df.columns.tolist()
    id_vars= ["Dimension","Value"]
    value_vars= [c for c in keep_cols if c not in id_vars]

    melted= df.melt(id_vars=id_vars, value_vars=value_vars,
                    var_name="Attribute", value_name="Value_melted")

    melted["Dimension"] = melted["Dimension"].replace(mst_dim_map)
    melted["Attribute"] = melted["Attribute"].replace(mst_attr_map)

    if mst_dim_allow:
        melted= melted[melted["Dimension"].isin(mst_dim_allow)]
    if mst_attr_allow:
        melted= melted[melted["Attribute"].isin(mst_attr_allow)]

    melted.rename(columns={"Value":"RefName","Value_melted":"Value"}, inplace=True)
    return melted[["Dimension","RefName","Attribute","Value"]].copy()

def build_keys(df: pd.DataFrame) -> pd.DataFrame:
    df= df.copy()
    for c in ["Dimension","RefName","Attribute","Value"]:
        if c not in df.columns:
            df[c]=""
        df[c]= df[c].fillna("").astype(str).str.strip()
    df["GroupKey"]= df["Dimension"]+" | "+df["RefName"]
    df["Key"]= df["Dimension"]+" | "+df["RefName"]+" | "+df["Attribute"]+" | "+df["Value"]
    df["Comments_1"]=""
    df["Comments_2"]=""
    df["Action Item"]=""
    df["Missing In"]=""
    return df

def build_lookup_dict(df: pd.DataFrame) -> Dict[str,Dict[str,str]]:
    lookup={}
    for gk,grp in df.groupby("GroupKey"):
        rec={}
        name_= grp["RefName"].iloc[0] if not grp.empty else ""
        rec["Name"] = name_
        for _, row in grp.iterrows():
            rec[row["Attribute"]]= row["Value"]
        lookup[gk]= rec
    return lookup

def compare_mode2(df_erp: pd.DataFrame, df_mst: pd.DataFrame) -> pd.DataFrame:
    """
    if name is missing from one side => entire record missing
    else compare attributes => missing in MASTER or ERP
    """
    erp_dict= build_lookup_dict(df_erp)
    mst_dict= build_lookup_dict(df_mst)
    all_keys= set(erp_dict.keys())|set(mst_dict.keys())
    results=[]
    for gk in all_keys:
        dim = gk.split(" | ")[0]
        a_data= erp_dict.get(gk,{})
        b_data= mst_dict.get(gk,{})
        name_a= a_data.get("Name","")
        name_b= b_data.get("Name","")
        if name_a and name_b and (name_a==name_b):
            # partial compare
            all_attrs= (set(a_data.keys())|set(b_data.keys()))-{"Name"}
            for attr in all_attrs:
                va= a_data.get(attr,"")
                vb= b_data.get(attr,"")
                if va!=vb:
                    if va and not vb:
                        results.append({"Dimension":dim, "Name":name_a, "Attribute":attr, "Value":va, "Missing In":"MASTER"})
                    elif vb and not va:
                        results.append({"Dimension":dim, "Name":name_a, "Attribute":attr, "Value":vb, "Missing In":"ERP"})
                    else:
                        results.append({"Dimension":dim, "Name":name_a, "Attribute":attr, "Value":va, "Missing In":"MASTER"})
                        results.append({"Dimension":dim, "Name":name_a, "Attribute":attr, "Value":vb, "Missing In":"ERP"})
        else:
            # entire record missing
            if name_a and not name_b:
                results.append({"Dimension":dim, "Name":name_a, "Attribute":"Name","Value":name_a,"Missing In":"MASTER"})
            elif name_b and not name_a:
                results.append({"Dimension":dim, "Name":name_b, "Attribute":"Name","Value":name_b,"Missing In":"ERP"})
    df_diff= pd.DataFrame(results)
    if not df_diff.empty:
        df_diff["Key"]= (
            df_diff["Dimension"].str.strip()+" | "+
            df_diff["Name"].str.strip()+" | "+
            df_diff["Attribute"].str.strip()+" | "+
            df_diff["Value"].str.strip()
        )
    return df_diff

# ------------------------------------------------------------------------------
# 8) EXCEPTIONS
# ------------------------------------------------------------------------------
def read_exception_table(path: Path) -> pd.DataFrame:
    if path.is_file():
        try:
            df= pd.read_excel(path)
            df.columns= df.columns.str.strip()
            return df
        except Exception as e:
            logging.error(f"Error reading exception table: {e}")
    return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame) -> pd.DataFrame:
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep = [c for c in df_exc.columns if c in ["Key","Comments_1","Comments_2","hide exception"]]
    if not keep:
        return df
    exc= df_exc[keep].copy()
    exc["Key"]= exc["Key"].astype(str).str.strip()

    merged= df.merge(exc, on="Key", how="left", suffixes=("", "_exc"))
    merged["hide exception"]= merged.get("hide exception","").fillna("").str.lower()
    final= merged[merged["hide exception"]!="yes"].copy()

    if "Comments_1_exc" in final.columns:
        final["Comments_1"]= np.where(final["Comments_1_exc"].notna(), final["Comments_1_exc"], final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"]= np.where(final["Comments_2_exc"].notna(), final["Comments_2_exc"], final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)
    return final

# ------------------------------------------------------------------------------
# 9) WRITE RESULTS
# ------------------------------------------------------------------------------
def write_results(df: pd.DataFrame, out_path: Path):
    if df.empty:
        logging.info("No differences => skip writing.")
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)

    final_cols= ["Key","Dimension","Name","Attribute","Value","Comments_1","Comments_2","Action Item","Missing In"]
    for c in final_cols:
        if c not in df.columns:
            df[c] = ""
    df= df[final_cols]

    wb= Workbook()
    ws= wb.active
    ws.title= "Results"
    ws.append(final_cols)
    for row in df.itertuples(index=False):
        ws.append(row)

    header_font= Font(bold=True)
    fill= PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
    for cell in ws[1]:
        cell.font= header_font
        cell.fill= fill
        cell.alignment= Alignment(horizontal="center")

    for col in ws.columns:
        max_len=0
        col_letter= col[0].column_letter
        for cell in col:
            val= str(cell.value) if cell.value else ""
            max_len= max(max_len, len(val))
        ws.column_dimensions[col_letter].width= max_len+2

    ws.freeze_panes= "A2"
    wb.save(out_path)
    logging.info(f"Results => {out_path}")

# ------------------------------------------------------------------------------
# 10) DASHBOARD
# ------------------------------------------------------------------------------
class Dashboard(ctk.CTkFrame):
    """
    8 chart frames: Heatmap, Lollipop, Circular, Scatter, Radar, Normal Pie, Normal Bar, Band Chart
    """
    def __init__(self, parent):
        super().__init__(parent)
        self.df_current= pd.DataFrame()
        self.df_history= pd.DataFrame()

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        self.frame_heatmap   = ctk.CTkFrame(self.notebook)
        self.frame_lollipop  = ctk.CTkFrame(self.notebook)
        self.frame_circular  = ctk.CTkFrame(self.notebook)
        self.frame_scatter   = ctk.CTkFrame(self.notebook)
        self.frame_radar     = ctk.CTkFrame(self.notebook)
        self.frame_normalpie = ctk.CTkFrame(self.notebook)
        self.frame_normalbar = ctk.CTkFrame(self.notebook)
        self.frame_bandchart = ctk.CTkFrame(self.notebook)

        self.notebook.add(self.frame_heatmap,   text="Heatmap")
        self.notebook.add(self.frame_lollipop,  text="Lollipop Dim")
        self.notebook.add(self.frame_circular,  text="Circular Attr")
        self.notebook.add(self.frame_scatter,   text="Scatter")
        self.notebook.add(self.frame_radar,     text="Radar")
        self.notebook.add(self.frame_normalpie, text="Normal Pie")
        self.notebook.add(self.frame_normalbar, text="Normal Bar")
        self.notebook.add(self.frame_bandchart, text="Band Chart")

    def update_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current= df_current.copy()
        self.df_history= df_history.copy()

        self.plot_heatmap()
        self.plot_lollipop()
        self.plot_circular()
        self.plot_scatter()
        self.plot_radar()
        self.plot_normal_pie()
        self.plot_normal_bar()
        self.plot_band_chart()

    def plot_heatmap(self):
        for w in self.frame_heatmap.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        pivoted= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
        fig, ax= plt.subplots(figsize=(6,5))
        cax= ax.imshow(pivoted, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivoted.columns)))
        ax.set_xticklabels(pivoted.columns, rotation=90)
        ax.set_yticks(range(len(pivoted.index)))
        ax.set_yticklabels(pivoted.index)
        fig.colorbar(cax, ax=ax)
        ax.set_title("Heatmap: Dimension vs. Attribute Missing")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_heatmap)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_lollipop(self):
        for w in self.frame_lollipop.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_dim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        fig, ax= plt.subplots(figsize=(6,5))
        ax.hlines(y=count_dim.index, xmin=0, xmax=count_dim.values, color="skyblue")
        ax.plot(count_dim.values, count_dim.index, "o", color="skyblue")
        ax.set_title("Lollipop: Missing Dimensions")
        ax.set_xlabel("Missing Count")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_lollipop)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_circular(self):
        for w in self.frame_circular.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_attr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if count_attr.empty:
            return
        categories= count_attr.index.tolist()
        values= count_attr.values
        angles= np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()

        fig= plt.figure(figsize=(6,6))
        ax= fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(categories, fontsize=9)
        ax.bar(angles, values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular Barplot: Missing Attributes", y=1.05)
        canvas= FigureCanvasTkAgg(fig, master=self.frame_circular)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_scatter(self):
        for w in self.frame_scatter.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_dim= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        if count_dim.empty:
            return
        count_dim.sort_values("Count", ascending=False, inplace=True)
        xvals= np.arange(len(count_dim))
        yvals= count_dim["Count"].values
        labels= count_dim["Dimension"].values

        fig, ax= plt.subplots(figsize=(6,5))
        ax.scatter(xvals, yvals, color="green")
        for i, txt in enumerate(labels):
            ax.text(xvals[i], yvals[i], txt, ha="center", va="bottom", rotation=60)
        ax.set_xticks([])
        ax.set_ylabel("Missing Count")
        ax.set_title("Scatter: Missing by Dimension")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_scatter)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_radar(self):
        for w in self.frame_radar.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_dim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(5)
        if count_dim.empty:
            return
        categories= count_dim.index.tolist()
        values= count_dim.values.tolist()
        N= len(categories)
        angles= np.linspace(0,2*np.pi,N,endpoint=False).tolist()
        angles+= angles[:1]
        values+= values[:1]

        fig= plt.figure(figsize=(6,6))
        ax= fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=9)
        ax.plot(angles, values, color="red", linewidth=2)
        ax.fill(angles, values, color="red", alpha=0.3)
        ax.set_title("Radar Chart (Top 5 Dimensions Missing)", y=1.08)
        canvas= FigureCanvasTkAgg(fig, master=self.frame_radar)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_normal_pie(self):
        for w in self.frame_normalpie.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        dist= df_m["Missing In"].value_counts()
        fig, ax= plt.subplots(figsize=(5,5))
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Normal Pie: Missing In Distribution")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_normalpie)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_normal_bar(self):
        for w in self.frame_normalbar.winfo_children():
            w.destroy()
        df_m= self.df_current[self.df_current["Missing In"]!=""]
        if df_m.empty:
            return
        count_attr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        fig, ax= plt.subplots(figsize=(6,4))
        count_attr.plot(kind="bar", ax=ax, color="blue")
        ax.set_title("Normal Bar: Top 10 Missing Attributes")
        ax.set_ylabel("Missing Count")
        canvas= FigureCanvasTkAgg(fig, master=self.frame_normalbar)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_band_chart(self):
        for w in self.frame_bandchart.winfo_children():
            w.destroy()
        if self.df_history.empty or "RunDate" not in self.df_history.columns:
            return
        df_m= self.df_history[self.df_history["Missing In"]!=""]
        if df_m.empty:
            return
        date_counts= df_m.groupby("RunDate")["Key"].count().reset_index()
        date_counts.sort_values("RunDate", inplace=True)
        date_counts["Count_min"]= date_counts["Key"]*0.9
        date_counts["Count_max"]= date_counts["Key"]*1.1

        fig, ax= plt.subplots(figsize=(6,4))
        ax.plot(date_counts["RunDate"], date_counts["Key"], color="purple", label="Missing Count")
        ax.fill_between(date_counts["RunDate"], date_counts["Count_min"], date_counts["Count_max"],
                        color="purple", alpha=0.2, label="±10% band")
        ax.set_title("Band Chart Over Runs")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Missing Count")
        plt.xticks(rotation=45)
        ax.legend()

        for i, row in date_counts.iterrows():
            ax.text(row["RunDate"], row["Key"], str(row["Key"]), ha="center", va="bottom")

        canvas= FigureCanvasTkAgg(fig, master=self.frame_bandchart)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

# ------------------------------------------------------------------------------
# 11) EXCEL GRID (ONLY START/END DATE FILTERABLE)
# ------------------------------------------------------------------------------
class ExcelGrid(ctk.CTkFrame):
    """
    All columns displayed, but only columns named exactly "Start Date" or "End Date" allow filter popups.
    """
    ALLOWED_DATE_COLS = {"Start Date","End Date"}

    def __init__(self, parent, config_block: Dict, name: str):
        super().__init__(parent)
        self.name = name
        self.col_defs = config_block.get("columns", [])
        self.filters: Dict[str,Set] = {k:set(v) for k,v in config_block.get("filters",{}).items()}
        self.df = pd.DataFrame()

        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        tb = ctk.CTkFrame(self)
        tb.pack(fill="x", padx=5, pady=5)
        ctk.CTkButton(tb, text="Clear Filters", command=self.clear_filters).pack(side="left", padx=5)

    def create_table(self):
        container= ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)

        self.tree= ttk.Treeview(container, show="headings")
        vsb= ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb= ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)

        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")

        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

    def create_statusbar(self):
        self.status_label= ctk.CTkLabel(self, text="Ready")
        self.status_label.pack(fill="x")

    def set_data(self, df: pd.DataFrame):
        self.df= df.copy(deep=True)
        existing_ids = [c["id"] for c in self.col_defs]
        for col in self.df.columns:
            if col not in existing_ids:
                self.col_defs.append({
                    "id": col,
                    "name": col,
                    "visible": True
                })
        self.refresh_table()

    def refresh_table(self):
        for item in self.tree.get_children():
            self.tree.delete(item)

        visible_cols= [c for c in self.col_defs if c.get("visible",True)]
        self.tree["columns"]= [c["id"] for c in visible_cols]

        for col_def in visible_cols:
            self.tree.heading(
                col_def["id"],
                text=col_def["name"],
                anchor="w",
                command=lambda c=col_def: self.show_filter_popup(c)
            )
            self.tree.column(col_def["id"], anchor="w", width=150)

        df_f= self.get_filtered_df()
        for _, row in df_f.iterrows():
            vals= [row[c["id"]] for c in visible_cols]
            self.tree.insert("", "end", values=vals)

        self.status_label.configure(text=f"{len(df_f)} rows")

    def get_filtered_df(self) -> pd.DataFrame:
        if self.df.empty:
            return self.df
        df_f= self.df.copy()

        def passes(x, allowed):
            if pd.isna(x):
                return any(pd.isna(a) for a in allowed)
            else:
                return x in allowed

        for col_id, allowed_vals in self.filters.items():
            if col_id in df_f.columns and allowed_vals:
                df_f= df_f[df_f[col_id].apply(lambda x: passes(x, allowed_vals))]

        visible_ids= [c["id"] for c in self.col_defs if c.get("visible",True)]
        visible_ids= [c for c in visible_ids if c in df_f.columns]
        return df_f[visible_ids]

    def show_filter_popup(self, col_def: Dict):
        col_id= col_def["id"]
        col_name= col_def["name"]
        # only open filter if col_name in ALLOWED_DATE_COLS
        if col_name not in self.ALLOWED_DATE_COLS:
            return
        if self.df.empty or col_id not in self.df.columns:
            return

        popup= tk.Toplevel(self)
        popup.title(f"Filter: {col_name}")
        popup.geometry("320x500")

        frame= ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)

        unique_vals= self.df[col_id].unique()
        display_map={}
        for v in unique_vals:
            if pd.isna(v):
                dsp= "(NaN)"
            elif isinstance(v,str) and not v.strip():
                dsp= "(blank)"
            else:
                dsp= str(v)
            display_map[v]= dsp

        sorted_vals= sorted(display_map.keys(), key=lambda x: display_map[x].lower())
        current_filter= self.filters.get(col_id, set())
        if not current_filter:
            current_filter= set(unique_vals)

        all_var= tk.BooleanVar(value=True)
        def toggle_all():
            c_ = all_var.get()
            for vb in var_dict.values():
                vb.set(c_)

        ctk.CTkCheckBox(frame, text="Select All", variable=all_var, command=toggle_all).pack(anchor="w", pady=5)

        scroll= ctk.CTkScrollableFrame(frame, width=280, height=320)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)

        var_dict={}
        for rv in sorted_vals:
            if pd.isna(rv):
                in_filter= any(pd.isna(a) for a in current_filter)
            else:
                in_filter= (rv in current_filter)
            bvar= tk.BooleanVar(value=in_filter)
            var_dict[rv]= bvar
            ctk.CTkCheckBox(scroll, text=display_map[rv], variable=bvar).pack(anchor="w")

        def apply_():
            sel= set()
            for rv, vb in var_dict.items():
                if vb.get():
                    sel.add(rv)
            self.filters[col_id] = sel
            popup.destroy()
            self.refresh_table()

        bf= ctk.CTkFrame(frame)
        bf.pack(fill="x", pady=5)
        ctk.CTkButton(bf, text="Apply", command=apply_).pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Cancel", command=popup.destroy).pack(side="left", padx=5)

    def clear_filters(self):
        self.filters.clear()
        self.refresh_table()

    def get_config_block(self)->Dict:
        return {
            "columns": self.col_defs,
            "filters": {col_id: sorted(list(vals)) for col_id, vals in self.filters.items()}
        }

# ------------------------------------------------------------------------------
# 12) MAIN APP
# ------------------------------------------------------------------------------
class MainApp(ctk.CTk):
    """
    Steps:
    1) ERP => read Excel, skip disabled, strip date => one DF
    2) Master => .txt -> .csv in temp folder -> unify -> one DF
    3) Parameter => dimension/attr rename + allowed sets
    4) meltdown => compare mode=2 => exceptions => dashboard
    5) only Start/End Date columns are filterable in both ERP & Master grids
    6) 8-charts in dashboard
    """
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation: Master .txt->.csv, Only Enabled, Mode=2, Filter Start/End Date, 8 Charts")
        self.geometry("1600x900")

        self.config_dict= load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.history_df= pd.DataFrame()

        self.param_dict= read_parameters_excel(Path(self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"])))

        # Notebook
        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        # Tab 1: Paths
        self.tab_paths= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_paths, text="Paths")
        self.build_paths_tab(self.tab_paths)

        # Tab 2: ERP
        self.tab_erp= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_erp, text="ERP")
        self.erp_grid= ExcelGrid(self.tab_erp, self.config_dict["erp_grid"], name="ERP")
        self.erp_grid.pack(fill="both", expand=True)

        # Tab 3: Master
        self.tab_master= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_master, text="Master")
        self.master_grid= ExcelGrid(self.tab_master, self.config_dict["master_grid"], name="Master")
        self.master_grid.pack(fill="both", expand=True)

        # Tab 4: Compare & Exceptions
        self.tab_compare= ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_compare, text="Compare & Exceptions")
        self.build_compare_tab(self.tab_compare)

        # Tab 5: Dashboard
        self.tab_dashboard= Dashboard(self.notebook)
        self.notebook.add(self.tab_dashboard, text="Dashboard")

        # Logging
        self.log_box= ctk.CTkTextbox(self, height=100)
        self.log_box.pack(fill="both", expand=False)
        self.log_box.configure(state="disabled")
        handler= TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        # load initial data
        self.refresh_erp_data()
        self.refresh_master_data()

    def build_paths_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        self.erp_var= tk.StringVar(value=self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mst_var= tk.StringVar(value=self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_var= tk.StringVar(value=self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_var= tk.StringVar(value=self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))
        self.param_var= tk.StringVar(value=self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"]))
        self.cfg_var= tk.StringVar(value=self.config_dict["paths"].get("CONFIG_PATH", DEFAULT_PATHS["CONFIG_PATH"]))
        self.csvdir_var= tk.StringVar(value=self.config_dict["paths"].get("MASTER_CSV_OUTPUT", DEFAULT_PATHS["MASTER_CSV_OUTPUT"]))

        def mkrow(lbl,var, is_dir=False):
            rowf= ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=180).pack(side="left", padx=5)
            e= ctk.CTkEntry(rowf, textvariable=var, width=600)
            e.pack(side="left", padx=5)
            def br():
                if is_dir:
                    p= filedialog.askdirectory()
                else:
                    p= filedialog.askopenfilename()
                if p:
                    var.set(p)
            ctk.CTkButton(rowf, text="Browse", command=br).pack(side="left", padx=5)

        mkrow("ERP Excel Path:", self.erp_var)
        mkrow("Master ZIP Path:", self.mst_var)
        mkrow("Exception Path:", self.exc_var)
        mkrow("Output Path:", self.out_var)
        mkrow("Parameter Path:", self.param_var)
        mkrow("JSON Config Path:", self.cfg_var)
        mkrow("Master CSV Folder:", self.csvdir_var, is_dir=True)

    def build_compare_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Compare => Mode=2 only.").pack(pady=5)

        row= ctk.CTkFrame(frm)
        row.pack(fill="x", pady=5)
        ctk.CTkButton(row, text="Run Comparison", command=self.run_comparison).pack(side="left", padx=5)
        ctk.CTkButton(row, text="Save Config", command=self.save_all_config).pack(side="left", padx=5)

    def refresh_erp_data(self):
        # read ERP
        path= Path(self.erp_var.get().strip())
        df_erp= read_erp_excel(path)
        self.erp_grid.set_data(df_erp)

    def refresh_master_data(self):
        # 1) extract .txt to CSV
        zip_path= Path(self.mst_var.get().strip())
        out_dir= Path(self.csvdir_var.get().strip())
        csv_list= convert_master_txt_to_csv(zip_path, out_dir)
        # 2) unify
        df_master= unify_master_csvs(csv_list)
        self.master_grid.set_data(df_master)

    def run_comparison(self):
        # update config paths
        self.config_dict["paths"]["ERP_EXCEL_PATH"] = self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mst_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"]= self.param_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"]= self.cfg_var.get().strip()
        self.config_dict["paths"]["MASTER_CSV_OUTPUT"]= self.csvdir_var.get().strip()
        self.config_dict["comparison_option"]= 2

        # meltdown ERP
        df_erp_filt= self.erp_grid.get_filtered_df()
        erp_m= meltdown_erp(
            df_erp_filt,
            erp_dim_map=self.param_dict.get("erp_dim_map",{}),
            erp_attr_map=self.param_dict.get("erp_attr_map",{}),
            erp_dim_allow=self.param_dict.get("erp_dim_allow",set()),
            erp_attr_allow=self.param_dict.get("erp_attr_allow",set())
        )
        erp_ready= build_keys(erp_m)

        # meltdown master
        df_mst_filt= self.master_grid.get_filtered_df()
        mst_m= meltdown_master(
            df_mst_filt,
            mst_dim_map=self.param_dict.get("master_dim_map",{}),
            mst_attr_map=self.param_dict.get("master_attr_map",{}),
            mst_dim_allow=self.param_dict.get("master_dim_allow",set()),
            mst_attr_allow=self.param_dict.get("master_attr_allow",set())
        )
        mst_ready= build_keys(mst_m)

        # compare mode2
        df_diff= compare_mode2(erp_ready, mst_ready)

        # exceptions
        exc_path= Path(self.exc_var.get().strip())
        df_exc= read_exception_table(exc_path)
        final= merge_exceptions(df_diff, df_exc)

        # write
        out_path= Path(self.out_var.get().strip())
        write_results(final, out_path)

        # add run date => dashboard
        run_date= datetime.now().strftime("%Y-%m-%d")
        final["RunDate"]= run_date
        self.history_df= pd.concat([self.history_df, final], ignore_index=True)

        self.notebook.select(self.tab_dashboard)
        self.tab_dashboard.update_data(final, self.history_df)

        messagebox.showinfo("Done", f"Comparison run for {run_date}, output => {out_path}")

    def save_all_config(self):
        self.config_dict["erp_grid"]= self.erp_grid.get_config_block()
        self.config_dict["master_grid"]= self.master_grid.get_config_block()

        self.config_dict["paths"]["ERP_EXCEL_PATH"]= self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mst_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"]= self.param_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"]= self.cfg_var.get().strip()
        self.config_dict["paths"]["MASTER_CSV_OUTPUT"]= self.csvdir_var.get().strip()
        self.config_dict["comparison_option"]= 2

        save_config(self.config_dict, Path(self.cfg_var.get()))
        messagebox.showinfo("Saved", "All config saved successfully.")

# ------------------------------------------------------------------------------
# 13) MAIN
# ------------------------------------------------------------------------------
def main():
    app= MainApp()
    app.mainloop()

if __name__=="__main__":
    main()
