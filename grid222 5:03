#!/usr/bin/env python3

"""
Ultra-Mega Reconciliation Script - Multiple Delimiters & Encodings
------------------------------------------------------------------
- For Master .txt files, we try multiple encodings (utf-8-sig, utf-8, utf-16, utf-32,
  cp1252, latin1, iso-8859-1, ascii), fallback to chardet guess if available.
- Multiple delimiters (",", ";", "\t", "|", None for inference).
- Skips bad lines, quotes issues, empties, etc. 
- Fallback for older Pandas if needed.
- Includes Excel-like grids that filter NaN/blank, dimension rename, 3 compare modes,
  merges exception table, daily-run chart, config, etc.

Author: X
Date: 2025
"""

import os
import json
import logging
import zipfile
import time
import copy
import csv
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime, timedelta
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, simpledialog

import customtkinter as ctk
import pandas as pd
import numpy as np

# Attempt optional chardet
try:
    import chardet
except ImportError:
    chardet = None

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# ---------------- LOGGING ----------------
def setup_logger():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
setup_logger()

# -------------- DEFAULTS & CONFIG --------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/Reconciliation.xlsx",
    "CONFIG_PATH": "config/ui_config.json"
}

def default_config() -> Dict:
    return {
        "paths": {
            "ERP_EXCEL_PATH": DEFAULT_PATHS["ERP_EXCEL_PATH"],
            "MASTER_ZIP_PATH": DEFAULT_PATHS["MASTER_ZIP_PATH"],
            "EXCEPTION_PATH": DEFAULT_PATHS["EXCEPTION_PATH"],
            "OUTPUT_PATH": DEFAULT_PATHS["OUTPUT_PATH"],
            "CONFIG_PATH": DEFAULT_PATHS["CONFIG_PATH"]
        },
        "erp_grid": {
            "columns": [
                {"id": "Col1",           "name": "Col1",           "locked": False, "visible": True,  "renameable": True},
                {"id": "Col2",           "name": "Col2",           "locked": False, "visible": True,  "renameable": True},
                {"id": "Enabled_Flag",   "name": "Enabled_Flag",   "locked": False, "visible": True,  "renameable": True},
                {"id": "Dimension_Name", "name": "Dimension_Name", "locked": True,  "visible": True,  "renameable": False},
                {"id": "Value",          "name": "Value",          "locked": True,  "visible": True,  "renameable": False},
            ],
            "filters": {}
        },
        "master_grid": {
            "columns": [
                {"id": "Name",      "name": "Name",      "locked": True,  "visible": True, "renameable": False},
                {"id": "Dimension", "name": "Dimension", "locked": True,  "visible": True, "renameable": False},
            ],
            "filters": {}
        },
        "dimension_renames": {},
        "comparison_option": 1
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config from {path}: {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# -------------- UI LOG HANDLER --------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget
    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")

# -------------- SAFE READ EXCEL --------------
class FileHandler:
    @staticmethod
    def safe_read_excel(path: Path, skiprows: int = 0, retries: int = 2) -> pd.DataFrame:
        import pandas as pd
        for attempt in range(retries):
            try:
                logging.info(f"Reading Excel: {path} (attempt {attempt+1}/{retries})")
                df = pd.read_excel(path, skiprows=skiprows)
                df.columns = df.columns.str.strip()
                return df
            except Exception as e:
                logging.warning(f"Failed reading Excel {path}: {e}")
                if attempt == retries - 1:
                    raise
                time.sleep(1)
        return pd.DataFrame()

def safe_read_erp_excel(path: Path) -> pd.DataFrame:
    try:
        return FileHandler.safe_read_excel(path, skiprows=3)
    except Exception as e:
        logging.error(f"Error reading ERP Excel: {e}")
        return pd.DataFrame()

# -------------- IMPROVED ROBUST CSV READING --------------
def read_csv_robust(filebytes: bytes) -> pd.DataFrame:
    """
    More robust CSV reading:
    1) Try chardet if available & confidence high enough.
    2) Then try multiple encodings in priority order:
       utf-8-sig, utf-16, utf-32, cp1252, latin1, iso-8859-1, ascii
    3) Try multiple delimiters: ',', ';', '\\t', '|', and a final fallback with delimiter=None
       to let pandas attempt auto-inference.
    4) Skip bad lines, handle quoting errors, remove empty rows/cols, clean col names.
    5) Fallback for old Pandas if TypeError with on_bad_lines param.
    """
    import io
    import pandas as pd

    # -- Check for completely empty file --
    if len(filebytes) == 0:
        logging.warning("File is empty; returning empty DataFrame.")
        return pd.DataFrame()

    # 1) Possibly guess with chardet:
    chardet_encoding = None
    if chardet:
        detect_sample = filebytes[:4096]  # small sample
        det = chardet.detect(detect_sample)
        guess_enc = det["encoding"]
        confidence = det.get("confidence", 0)
        if guess_enc and confidence >= 0.75:
            # We'll try chardet guess first if confidence is decent
            chardet_encoding = guess_enc
            logging.info(f"chardet suggests '{guess_enc}' with confidence {confidence}.")

    # 2) Build the encoding priority list:
    encodings_to_try = []
    if chardet_encoding:
        encodings_to_try.append(chardet_encoding)
    encodings_to_try.extend([
        "utf-8-sig",  # handles BOM for UTF-8
        "utf-16",
        "utf-32",
        "cp1252",
        "latin1",
        "iso-8859-1",
        "ascii",
    ])

    # 3) Potential delimiters (plus a final fallback of None)
    delimiters_to_try = [",", ";", "\t", "|", None]

    def try_read(data_bytes: bytes, encoding: str, delim) -> pd.DataFrame:
        """
        Attempt read with a specific encoding & delimiter.
        Try new approach first, if fails (older pandas?), fallback.
        """
        try:
            logging.debug(f"Attempt read with encoding='{encoding}', delimiter='{delim}'")
            return pd.read_csv(
                io.BytesIO(data_bytes),
                encoding=encoding,
                delimiter=delim,
                on_bad_lines="skip",   # introduced in pandas 1.3
                quoting=csv.QUOTE_MINIMAL,
                error_bad_lines=False, # deprecated in newer pandas, but used as fallback
                engine="python"        # 'python' engine handles more exotic quoting issues
            )
        except TypeError:
            # Fallback for older Pandas
            logging.debug("Fallback for older pandas (TypeError with on_bad_lines).")
            text_decoded = data_bytes.decode(encoding, errors="replace")
            buffer = io.StringIO(text_decoded)
            return pd.read_csv(
                buffer,
                delimiter=delim,
                error_bad_lines=False,
                warn_bad_lines=True,
                quoting=csv.QUOTE_MINIMAL,
                engine="python"
            )

    # Try each encoding and delimiter
    for enc in encodings_to_try:
        for delim in delimiters_to_try:
            try:
                df_temp = try_read(filebytes, enc, delim)
                # If we got a DataFrame without catastrophic parse failure, proceed:
                if df_temp is not None:
                    # -- Cleanup --
                    # Drop completely empty rows:
                    df_temp.dropna(how="all", inplace=True)
                    # Drop completely empty columns:
                    df_temp.dropna(axis=1, how="all", inplace=True)
                    # Strip column names:
                    df_temp.columns = df_temp.columns.astype(str).str.strip()
                    # If there's at least 1 column, we treat it as success
                    if not df_temp.empty and len(df_temp.columns) > 0:
                        logging.info(
                            f"Success reading CSV with encoding='{enc}', delimiter='{delim}' "
                            f"=> {len(df_temp)} rows, {len(df_temp.columns)} columns."
                        )
                        return df_temp
                    else:
                        logging.warning(
                            f"Read with encoding='{enc}', delimiter='{delim}' but got empty/no columns, "
                            "continuing tries..."
                        )
            except Exception as e:
                logging.debug(f"Failed with encoding='{enc}', delimiter='{delim}': {e}")

    # If nothing worked:
    logging.error("Could not read CSV with any tried encodings/delimiters. Returning empty DataFrame.")
    return pd.DataFrame()

# -------------- READ MASTER DATA (ZIP .txt) with IMPROVED HANDLING --------------
def read_master_data(zip_path: Path) -> pd.DataFrame:
    """
    Open the ZIP, read each .txt file with robust CSV logic, and combine.
    Cleans up dimension names. Continues reading other files if one fails.
    """
    if not zip_path.is_file():
        logging.warning(f"Master ZIP not found: {zip_path}")
        return pd.DataFrame()

    all_dfs = []
    try:
        with zipfile.ZipFile(zip_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
            if not txt_files:
                logging.warning(f"No .txt files found in ZIP: {zip_path}")
            for txt_file in txt_files:
                base_name = os.path.basename(txt_file)
                try:
                    # Infer dimension from filename:
                    if "_ceaster.txt" in base_name.lower():
                        base_dim = base_name.lower().replace("_ceaster.txt", "")
                    else:
                        base_dim, _ = os.path.splitext(base_name)
                    dimension = base_dim.replace("_", " ").title()

                    with z.open(txt_file) as fo:
                        file_bytes = fo.read()
                        if not file_bytes:
                            logging.warning(f"File {txt_file} is empty; skipping.")
                            continue

                        # Use the robust CSV reader:
                        df_part = read_csv_robust(file_bytes)
                        if df_part.empty:
                            logging.warning(f"File {txt_file} read empty or invalid; skipping.")
                            continue

                    # Ensure columns are stripped:
                    df_part.columns = df_part.columns.str.strip()

                    # If the current columns do not contain "Name", rename the first col to "Name".
                    if "Name" not in df_part.columns and len(df_part.columns) > 0:
                        first_col = df_part.columns[0]
                        df_part.rename(columns={first_col: "Name"}, inplace=True)

                    # Add dimension column
                    df_part["Dimension"] = dimension
                    all_dfs.append(df_part)
                except Exception as e2:
                    logging.error(f"Error reading {txt_file}: {e2}")
    except Exception as e:
        logging.error(f"Error opening zip {zip_path}: {e}")

    if all_dfs:
        combined = pd.concat(all_dfs, ignore_index=True)
        # Final cleanup if needed
        combined.dropna(how="all", inplace=True)
        combined.columns = combined.columns.str.strip()
        return combined
    else:
        return pd.DataFrame()

# -------------- EXCELGRID (with NaN/blank filter) --------------
class ExcelGrid(ctk.CTkFrame):
    """
    Excel-like grid that attempts multiple filters including NaN and blank.
    Also shows lock icon if 'locked': True in col_def.
    """
    LOCK_ICON = " \U0001F512"

    def __init__(self, parent, config_block: Dict, name: str):
        super().__init__(parent)
        self.name = name
        self.col_defs = config_block.get("columns", [])
        self.filters: Dict[str, Set] = {}
        for col_id, val_list in config_block.get("filters", {}).items():
            self.filters[col_id] = set(val_list) if isinstance(val_list, list) else set()

        self.df = pd.DataFrame()

        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        bar = ctk.CTkFrame(self)
        bar.pack(fill="x", padx=5, pady=5)
        ctk.CTkButton(bar, text="Manage Columns", command=self.show_column_manager).pack(side="left", padx=5)
        ctk.CTkButton(bar, text="Clear Filters", command=self.clear_filters).pack(side="left", padx=5)

    def create_table(self):
        container = ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)

        self.tree = ttk.Treeview(container, show="headings")
        vsb = ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb = ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)

        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")

        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

    def create_statusbar(self):
        self.status_label = ctk.CTkLabel(self, text="Ready")
        self.status_label.pack(fill="x", padx=5, pady=2)

    def set_data(self, df: pd.DataFrame):
        """Load a new DataFrame and refresh."""
        self.df = df.copy(deep=True)
        existing_ids = [c["id"] for c in self.col_defs]
        for col in self.df.columns:
            if col not in existing_ids:
                self.col_defs.append({
                    "id": col,
                    "name": col,
                    "locked": False,
                    "visible": True,
                    "renameable": True
                })
        self.refresh_table()

    def get_config_block(self):
        return {
            "columns": self.col_defs,
            "filters": {col_id: sorted(list(vals)) for col_id, vals in self.filters.items()}
        }

    def get_filtered_df(self) -> pd.DataFrame:
        if self.df.empty:
            return self.df
        df_f = self.df.copy()

        def passes_filter(x, allowed_vals):
            if pd.isna(x):
                return any(pd.isna(a) for a in allowed_vals)
            else:
                return x in allowed_vals

        for col_id, allowed_vals in self.filters.items():
            if col_id in df_f.columns and allowed_vals:
                mask = df_f[col_id].apply(lambda v: passes_filter(v, allowed_vals))
                df_f = df_f[mask]

        visible_ids = [c["id"] for c in self.col_defs if c.get("visible", True)]
        visible_ids = [c for c in visible_ids if c in df_f.columns]
        return df_f[visible_ids]

    def refresh_table(self):
        for item in self.tree.get_children():
            self.tree.delete(item)
        visible_cols = [c for c in self.col_defs if c.get("visible", True)]
        self.tree["columns"] = [c["id"] for c in visible_cols]

        for col_def in visible_cols:
            col_name = col_def["name"]
            if col_def.get("locked", False):
                col_name += self.LOCK_ICON

            self.tree.heading(
                col_def["id"],
                text=col_name,
                anchor="w",
                command=lambda c=col_def: self.show_filter_popup(c)
            )
            self.tree.column(col_def["id"], anchor="w", width=col_def.get("width", 150))

        df_f = self.get_filtered_df()
        for idx, row in df_f.iterrows():
            vals = [row[c["id"]] for c in visible_cols]
            self.tree.insert("", "end", values=vals)

        self.status_label.configure(text=f"{len(df_f)} rows")

    def show_filter_popup(self, col_def: Dict):
        col_id = col_def["id"]
        if self.df.empty or col_id not in self.df.columns:
            return

        popup = tk.Toplevel(self)
        popup.title(f"Filter: {col_def['name']}")
        popup.geometry("320x500")

        frame = ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)

        series = self.df[col_id].unique()
        display_map = {}
        for val in series:
            if pd.isna(val):
                display_str = "(NaN)"
            elif isinstance(val, str) and not val.strip():
                display_str = "(blank)"
            else:
                display_str = str(val)
            display_map[val] = display_str

        sorted_vals = sorted(display_map.keys(), key=lambda rv: display_map[rv].lower())
        current_filter = self.filters.get(col_id, set())
        if not current_filter:
            current_filter = set(series)

        select_all_var = tk.BooleanVar(value=True)
        def toggle_select_all():
            new_val = select_all_var.get()
            for vb in var_dict.values():
                vb.set(new_val)

        ctk.CTkCheckBox(frame, text="Select All", variable=select_all_var,
                        command=toggle_select_all).pack(anchor="w", pady=5)

        scroll = ctk.CTkScrollableFrame(frame, width=280, height=320)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)

        var_dict = {}
        for real_val in sorted_vals:
            disp_str = display_map[real_val]
            if pd.isna(real_val):
                in_filter = any(pd.isna(fv) for fv in current_filter)
            else:
                in_filter = (real_val in current_filter)

            var = tk.BooleanVar(value=in_filter)
            var_dict[real_val] = var
            ctk.CTkCheckBox(scroll, text=disp_str, variable=var).pack(anchor="w")

        def apply_filter():
            selected = set()
            for rv, vb in var_dict.items():
                if vb.get():
                    selected.add(rv)
            self.filters[col_id] = selected
            popup.destroy()
            self.refresh_table()

        btn_frame = ctk.CTkFrame(frame)
        btn_frame.pack(fill="x", pady=5)
        ctk.CTkButton(btn_frame, text="Apply", command=apply_filter).pack(side="left", padx=5)
        ctk.CTkButton(btn_frame, text="Cancel", command=popup.destroy).pack(side="left", padx=5)

    def show_column_manager(self):
        cm = tk.Toplevel(self)
        cm.title(f"{self.name} Column Manager")
        scrolled = ctk.CTkScrollableFrame(cm, width=600, height=500)
        scrolled.pack(fill="both", expand=True)

        for i, col_def in enumerate(self.col_defs):
            rowf = ctk.CTkFrame(scrolled)
            rowf.pack(fill="x", pady=2)

            if col_def.get("locked", False):
                txt = col_def["name"] + self.LOCK_ICON
                ctk.CTkLabel(rowf, text=txt).pack(side="left", padx=5)
                continue

            var_vis = tk.BooleanVar(value=col_def.get("visible", True))
            def toggler(c=col_def, v=var_vis):
                c["visible"] = v.get()
                self.refresh_table()

            ctk.CTkCheckBox(rowf, text="", variable=var_vis, command=toggler).pack(side="left")

            if col_def.get("renameable", True):
                ctk.CTkButton(rowf, text=col_def["name"], command=lambda c=col_def: self.rename_column(c)).pack(side="left", padx=5)
            else:
                ctk.CTkLabel(rowf, text=col_def["name"]).pack(side="left", padx=5)

            ctk.CTkButton(rowf, text="↑", width=30, command=lambda idx=i: self.move_column(idx, -1)).pack(side="right", padx=2)
            ctk.CTkButton(rowf, text="↓", width=30, command=lambda idx=i: self.move_column(idx, 1)).pack(side="right", padx=2)

    def rename_column(self, col_def: Dict):
        old_name = col_def["name"]
        new_name = simpledialog.askstring("Rename Column", f"New name for {old_name}:", initialvalue=old_name)
        if new_name:
            col_def["name"] = new_name
            self.refresh_table()

    def move_column(self, idx: int, delta: int):
        new_idx = idx + delta
        if 0 <= new_idx < len(self.col_defs):
            self.col_defs[idx], self.col_defs[new_idx] = self.col_defs[new_idx], self.col_defs[idx]
            self.refresh_table()

    def clear_filters(self):
        self.filters.clear()
        self.refresh_table()

# -------------- MELTDOWN & COMPARISON --------------
def meltdown_erp(df: pd.DataFrame, dim_renames: Dict[str, str]) -> pd.DataFrame:
    if df.empty:
        return df
    skip_cols = {"Col1","Col2","Enabled_Flag"}
    keep_cols = [c for c in df.columns if c not in skip_cols]
    id_vars = []
    if "Dimension_Name" in keep_cols:
        id_vars.append("Dimension_Name")
    if "Value" in keep_cols:
        id_vars.append("Value")
    value_vars = [c for c in keep_cols if c not in id_vars]

    melted = df.melt(
        id_vars=id_vars,
        value_vars=value_vars,
        var_name="Attribute",
        value_name="Value"
    )
    melted.rename(columns={"Dimension_Name":"Dimension","Value":"RefName"}, inplace=True)
    if dim_renames:
        melted["Dimension"] = melted["Dimension"].replace(dim_renames)
    return melted[["Dimension","RefName","Attribute","Value"]].copy()

def meltdown_master(df: pd.DataFrame, dim_renames: Dict[str,str]) -> pd.DataFrame:
    if df.empty:
        return df
    keep_cols = df.columns.tolist()
    id_vars = [c for c in ["Dimension","Name"] if c in keep_cols]
    value_vars = [c for c in keep_cols if c not in id_vars]
    melted = df.melt(
        id_vars=id_vars,
        value_vars=value_vars,
        var_name="Attribute",
        value_name="Value"
    )
    melted.rename(columns={"Name":"RefName"}, inplace=True)
    if dim_renames:
        melted["Dimension"] = melted["Dimension"].replace(dim_renames)
    return melted[["Dimension","RefName","Attribute","Value"]].copy()

def build_keys(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for col in ["Dimension","RefName","Attribute","Value"]:
        if col not in df.columns:
            df[col] = ""
        df[col] = df[col].fillna("").astype(str).str.strip()
    df["GroupKey"] = df["Dimension"] + " | " + df["RefName"]
    df["Key"] = df["Dimension"] + " | " + df["RefName"] + " | " + df["Attribute"] + " | " + df["Value"]
    df["Comments_1"] = ""
    df["Comments_2"] = ""
    df["Action Item"] = ""
    df["Missing In"] = ""
    return df

def build_lookup_dict(df: pd.DataFrame) -> Dict[str, Dict[str, str]]:
    lookup = {}
    for gk, grp in df.groupby("GroupKey"):
        rec = {}
        ref = grp["RefName"].iloc[0] if not grp.empty else ""
        rec["Name"] = ref
        for _, row in grp.iterrows():
            rec[row["Attribute"]] = row["Value"]
        lookup[gk] = rec
    return lookup

def compare_data(df_erp: pd.DataFrame, df_master: pd.DataFrame, mode: int) -> pd.DataFrame:
    erp_dict = build_lookup_dict(df_erp)
    mst_dict = build_lookup_dict(df_master)
    all_keys = set(erp_dict.keys()) | set(mst_dict.keys())
    results = []
    for gk in all_keys:
        parts = gk.split(" | ")
        dimension = parts[0] if parts else ""
        a_data = erp_dict.get(gk, {})
        b_data = mst_dict.get(gk, {})
        name_a = a_data.get("Name", a_data.get("RefName", ""))
        name_b = b_data.get("Name", b_data.get("RefName", ""))

        if mode == 1:
            results.extend(compare_mode_1(dimension, name_a, name_b, a_data, b_data))
        elif mode == 2:
            results.extend(compare_mode_2(dimension, name_a, name_b, a_data, b_data))
        else:
            results.extend(compare_mode_3(dimension, name_a, name_b, a_data, b_data))

    df_diff = pd.DataFrame(results)
    if not df_diff.empty:
        df_diff["Key"] = (df_diff["Dimension"].str.strip() + " | " +
                          df_diff["Name"].str.strip() + " | " +
                          df_diff["Attribute"].str.strip() + " | " +
                          df_diff["Value"].str.strip())
    return df_diff

def compare_mode_1(dimension, name_a, name_b, a_data, b_data):
    results = []
    all_attrs = set(a_data.keys()) | set(b_data.keys())
    for attr in all_attrs:
        va = a_data.get(attr, "")
        vb = b_data.get(attr, "")
        if va != vb:
            if va and not vb:
                results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
            elif vb and not va:
                results.append({"Dimension": dimension, "Name": name_b, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
            else:
                results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
                results.append({"Dimension": dimension, "Name": name_b, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
    return results

def compare_mode_2(dimension, name_a, name_b, a_data, b_data):
    results = []
    if name_a and name_b and (name_a == name_b):
        all_attrs = (set(a_data.keys()) | set(b_data.keys())) - {"Name"}
        for attr in all_attrs:
            va = a_data.get(attr, "")
            vb = b_data.get(attr, "")
            if va != vb:
                if va and not vb:
                    results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
                elif vb and not va:
                    results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
                else:
                    results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
                    results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
    else:
        if name_a and not name_b:
            results.append({"Dimension": dimension, "Name": name_a, "Attribute": "Name", "Value": name_a, "Missing In": "MASTER"})
        elif name_b and not name_a:
            results.append({"Dimension": dimension, "Name": name_b, "Attribute": "Name", "Value": name_b, "Missing In": "ERP"})
    return results

def compare_mode_3(dimension, name_a, name_b, a_data, b_data):
    results = []
    all_attrs = set(a_data.keys()) | set(b_data.keys())
    for attr in all_attrs:
        va = a_data.get(attr, "")
        vb = b_data.get(attr, "")
        if va == vb:
            results.append({"Dimension": dimension, "Name": name_a if name_a else name_b, "Attribute": attr, "Value": va, "Missing In": ""})
        else:
            if va and not vb:
                results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
            elif vb and not va:
                results.append({"Dimension": dimension, "Name": name_b, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
            else:
                results.append({"Dimension": dimension, "Name": name_a, "Attribute": attr, "Value": va, "Missing In": "MASTER"})
                results.append({"Dimension": dimension, "Name": name_a if name_a else name_b, "Attribute": attr, "Value": vb, "Missing In": "ERP"})
    return results

# -------------- EXCEPTIONS --------------
def read_exception_table(exc_path: Path) -> pd.DataFrame:
    if exc_path.is_file():
        try:
            return pd.read_excel(exc_path, sheet_name=0)
        except Exception as e:
            logging.error(f"Error reading exception table: {e}")
    return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame) -> pd.DataFrame:
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep = [c for c in df_exc.columns if c in ["Key","Comments_1","Comments_2","hide exception"]]
    if not keep:
        return df
    exc = df_exc[keep].copy()
    exc["Key"] = exc["Key"].astype(str).str.strip()

    merged = df.merge(exc, on="Key", how="left", suffixes=("", "_exc"))
    merged["hide exception"] = merged.get("hide exception", "").fillna("").str.lower()
    final = merged[merged["hide exception"]!="yes"].copy()

    if "Comments_1_exc" in final.columns:
        final["Comments_1"] = final["Comments_1_exc"].where(final["Comments_1_exc"].notna(), final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"] = final["Comments_2_exc"].where(final["Comments_2_exc"].notna(), final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)
    return final

# -------------- WRITE RESULTS --------------
def write_results(df: pd.DataFrame, out_path: Path, mode: int):
    if df.empty:
        logging.info("No differences to write.")
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)
    final_cols = ["Key","Dimension","Name","Attribute","Value","Comments_1","Comments_2","Action Item","Missing In"]
    for c in final_cols:
        if c not in df.columns:
            df[c] = ""
    df = df[final_cols]

    wb = Workbook()
    max_rows_per_sheet = 30000 if mode == 3 else 999999
    sheet_count = 1
    start = 0
    while start < len(df):
        end = min(start + max_rows_per_sheet, len(df))
        chunk = df.iloc[start:end]
        if sheet_count == 1:
            ws = wb.active
            ws.title = f"Results{sheet_count}"
        else:
            ws = wb.create_sheet(title=f"Results{sheet_count}")
        ws.append(final_cols)
        for row in chunk.itertuples(index=False):
            ws.append(row)
        # minimal styling
        header_font = Font(bold=True)
        fill = PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
        for cell in ws[1]:
            cell.font = header_font
            cell.fill = fill
            cell.alignment = Alignment(horizontal="center")
        for col in ws.columns:
            max_len = 0
            col_letter = col[0].column_letter
            for cell in col:
                val = str(cell.value) if cell.value is not None else ""
                max_len = max(max_len, len(val))
            ws.column_dimensions[col_letter].width = max_len + 2
        ws.freeze_panes = "A2"

        sheet_count += 1
        start = end
    wb.save(out_path)
    logging.info(f"Results saved to {out_path}")

# -------------- DASHBOARD --------------
class Dashboard(ctk.CTkFrame):
    def __init__(self, parent):
        super().__init__(parent)
        self.df_current = pd.DataFrame()
        self.df_history = pd.DataFrame()

        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        self.frame_heatmap = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.frame_heatmap, text="Discrepancy Heatmap")

        self.frame_status = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.frame_status, text="Status Distribution")

        self.frame_dimension = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.frame_dimension, text="Dimension Analysis")

        self.frame_attribute = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.frame_attribute, text="Attribute Comparison")

        self.frame_linechart = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.frame_linechart, text="Trend Over Days")

    def update_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current = df_current.copy()
        self.df_history = df_history.copy()
        self.plot_heatmap()
        self.plot_status_distribution()
        self.plot_dimension_analysis()
        self.plot_attribute_comparison()
        self.plot_run_trend()

    def plot_heatmap(self):
        for w in self.frame_heatmap.winfo_children():
            w.destroy()
        if self.df_current.empty:
            return

        mismatch_df = self.df_current[self.df_current["Missing In"] != ""]
        pivot_df = mismatch_df.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
        if pivot_df.empty:
            return

        fig, ax = plt.subplots(figsize=(6,5))
        cax = ax.imshow(pivot_df, cmap="Reds", aspect="auto")
        ax.set_xticks(range(len(pivot_df.columns)))
        ax.set_yticks(range(len(pivot_df.index)))
        ax.set_xticklabels(pivot_df.columns, rotation=90)
        ax.set_yticklabels(pivot_df.index)
        ax.set_title("Discrepancy Heatmap (# mismatches)")
        fig.colorbar(cax, ax=ax)

        canvas = FigureCanvasTkAgg(fig, master=self.frame_heatmap)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_status_distribution(self):
        for w in self.frame_status.winfo_children():
            w.destroy()
        if self.df_current.empty:
            return

        dist_counts = self.df_current["Missing In"].fillna("").value_counts()
        data_labels = dist_counts.index.tolist()
        data_values = dist_counts.values.tolist()

        fig, ax = plt.subplots(figsize=(5,5))
        ax.pie(data_values, labels=data_labels, autopct="%.1f%%", startangle=140)
        ax.set_title("Status Distribution (Current Run)")

        canvas = FigureCanvasTkAgg(fig, master=self.frame_status)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_dimension_analysis(self):
        for w in self.frame_dimension.winfo_children():
            w.destroy()
        if self.df_current.empty:
            return

        dim_counts = self.df_current.groupby("Dimension")["Key"].count().sort_values(ascending=False)

        fig, ax = plt.subplots(figsize=(6,4))
        dim_counts.plot(kind="bar", ax=ax, color="blue")
        ax.set_ylabel("Count of Rows")
        ax.set_title("Records per Dimension (Current Run)")

        canvas = FigureCanvasTkAgg(fig, master=self.frame_dimension)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_attribute_comparison(self):
        for w in self.frame_attribute.winfo_children():
            w.destroy()
        if self.df_current.empty:
            return
        attr_counts = self.df_current.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)

        fig, ax = plt.subplots(figsize=(6,4))
        attr_counts.plot(kind="bar", ax=ax, color="red")
        ax.set_ylabel("# of Mismatches")
        ax.set_title("Top 10 Mismatched Attributes (Current Run)")

        canvas = FigureCanvasTkAgg(fig, master=self.frame_attribute)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def plot_run_trend(self):
        for w in self.frame_linechart.winfo_children():
            w.destroy()
        if self.df_history.empty or "RunDate" not in self.df_history.columns:
            return

        mismatch_df = self.df_history[self.df_history["Missing In"] != ""]
        if mismatch_df.empty:
            return
        date_counts = mismatch_df.groupby("RunDate")["Key"].count().reset_index()

        fig, ax = plt.subplots(figsize=(6,4))
        ax.plot(date_counts["RunDate"], date_counts["Key"], marker="o", color="green")
        ax.set_xlabel("Date")
        ax.set_ylabel("Mismatch Count")
        ax.set_title("Mismatch Trend Over Days")
        plt.xticks(rotation=45)

        for i, row in date_counts.iterrows():
            ax.text(row["RunDate"], row["Key"], str(row["Key"]), ha="center", va="bottom")

        canvas = FigureCanvasTkAgg(fig, master=self.frame_linechart)
        canvas.draw()
        canvas.get_tk_widget().pack(fill="both", expand=True)

# -------------- MAIN APP --------------
class MainApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation (Multi-Delimiter + Multi-Encoding + Old Pandas Fallback)")
        self.geometry("1600x900")

        self.config_dict = load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.history_df = pd.DataFrame()

        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        # 1) Paths
        self.tab_paths = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_paths, text="Paths")
        self.build_paths_tab(self.tab_paths)

        # 2) ERP
        self.tab_erp = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_erp, text="ERP Config")
        self.erp_grid = ExcelGrid(self.tab_erp, self.config_dict.get("erp_grid", {}), "ERP")
        self.erp_grid.pack(fill="both", expand=True)

        # 3) Master
        self.tab_master = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_master, text="Master Config")
        self.master_grid = ExcelGrid(self.tab_master, self.config_dict.get("master_grid", {}), "Master")
        self.master_grid.pack(fill="both", expand=True)

        # 4) Dimension Renames
        self.tab_dim = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_dim, text="Dimension Renames")
        self.build_dimension_tab(self.tab_dim)

        # 5) Compare & Exceptions
        self.tab_compare = ctk.CTkFrame(self.notebook)
        self.notebook.add(self.tab_compare, text="Compare & Exceptions")
        self.build_compare_tab(self.tab_compare)

        # 6) Dashboard
        self.tab_dashboard = Dashboard(self.notebook)
        self.notebook.add(self.tab_dashboard, text="Dashboard")

        # Logging box
        self.log_box = ctk.CTkTextbox(self, height=100)
        self.log_box.pack(fill="both", expand=False)
        self.log_box.configure(state="disabled")
        handler = TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        # Load initial data
        self.refresh_erp_data()
        self.refresh_master_data()

    def build_paths_tab(self, parent):
        frm = ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        self.erp_path_var = tk.StringVar(value=self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.master_path_var = tk.StringVar(value=self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_path_var = tk.StringVar(value=self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_path_var = tk.StringVar(value=self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))
        self.cfg_path_var = tk.StringVar(value=self.config_dict["paths"].get("CONFIG_PATH", DEFAULT_PATHS["CONFIG_PATH"]))

        def mkrow(lbl, var):
            rowf = ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=200).pack(side="left", padx=5)
            ent = ctk.CTkEntry(rowf, textvariable=var, width=800)
            ent.pack(side="left", padx=5)
            def br():
                path = filedialog.askopenfilename()
                if path:
                    var.set(path)
            ctk.CTkButton(rowf, text="Browse", command=br).pack(side="left", padx=5)

        mkrow("ERP Excel Path:", self.erp_path_var)
        mkrow("Master ZIP Path:", self.master_path_var)
        mkrow("Exception Path:", self.exc_path_var)
        mkrow("Output Excel Path:", self.out_path_var)
        mkrow("JSON Config Path:", self.cfg_path_var)

    def refresh_erp_data(self):
        df = safe_read_erp_excel(Path(self.erp_path_var.get()))
        self.erp_grid.set_data(df)

    def refresh_master_data(self):
        df = read_master_data(Path(self.master_path_var.get()))
        self.master_grid.set_data(df)

    def build_dimension_tab(self, parent):
        frm = ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        # We'll use the property below (it has a setter now!)
        ctk.CTkLabel(frm, text="Old Dimension -> New Dimension").pack(pady=5)
        self.rename_rows = []
        self.list_frame = ctk.CTkScrollableFrame(frm, width=600, height=300)
        self.list_frame.pack(fill="both", expand=True)

        for old_dim, new_dim in self.dim_rename_map.items():
            self.add_dim_rename_row(old_dim, new_dim)

        ctk.CTkButton(frm, text="Add New Mapping", command=lambda: self.add_dim_rename_row("", "")).pack(pady=5)
        ctk.CTkButton(frm, text="Save Dimension Renames", command=self.save_dim_renames).pack(pady=5)

    def add_dim_rename_row(self, old_val: str, new_val: str):
        row = ctk.CTkFrame(self.list_frame)
        row.pack(fill="x", pady=2)
        tk_old = tk.StringVar(value=old_val)
        tk_new = tk.StringVar(value=new_val)
        ctk.CTkLabel(row, text="Old:").pack(side="left", padx=5)
        ctk.CTkEntry(row, textvariable=tk_old, width=200).pack(side="left", padx=5)
        ctk.CTkLabel(row, text=" -> ").pack(side="left")
        ctk.CTkEntry(row, textvariable=tk_new, width=200).pack(side="left", padx=5)
        self.rename_rows.append((tk_old, tk_new))

    def save_dim_renames(self):
        new_map = {}
        for (tk_old, tk_new) in self.rename_rows:
            oldv = tk_old.get().strip()
            newv = tk_new.get().strip()
            if oldv and newv and oldv != newv:
                new_map[oldv] = newv
        # Use our property setter
        self.dim_rename_map = new_map
        messagebox.showinfo("Saved", "Dimension renames saved in memory.")

    def build_compare_tab(self, parent):
        frm = ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        self.mode_var = tk.IntVar(value=self.config_dict.get("comparison_option", 1))
        for i, label in enumerate([
            "Option 1 - Show everything missing in ERP or Master",
            "Option 2 - If Name missing, do not show attributes, else show missing attributes",
            "Option 3 - Show missing + matching (can be large)"
        ], start=1):
            ctk.CTkRadioButton(frm, text=label, variable=self.mode_var, value=i).pack(anchor="w", padx=5, pady=2)

        btnf = ctk.CTkFrame(frm)
        btnf.pack(fill="x", pady=10)
        ctk.CTkButton(btnf, text="Run Comparison", command=self.run_reconciliation).pack(side="left", padx=5)
        ctk.CTkButton(btnf, text="Save Config", command=self.save_all_config).pack(side="left", padx=5)

    def run_reconciliation(self):
        self.config_dict["paths"]["ERP_EXCEL_PATH"] = self.erp_path_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"] = self.master_path_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"] = self.exc_path_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"] = self.out_path_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"] = self.cfg_path_var.get().strip()

        self.config_dict["comparison_option"] = self.mode_var.get()

        df_erp = self.erp_grid.get_filtered_df()
        df_master = self.master_grid.get_filtered_df()

        # meltdown
        erp_melt = meltdown_erp(df_erp, self.dim_rename_map)
        erp_ready = build_keys(erp_melt)

        master_melt = meltdown_master(df_master, self.dim_rename_map)
        master_ready = build_keys(master_melt)

        mode = self.config_dict["comparison_option"]
        df_diff = compare_data(erp_ready, master_ready, mode)

        exc_path = Path(self.exc_path_var.get().strip())
        df_exc = read_exception_table(exc_path)
        final = merge_exceptions(df_diff, df_exc)

        out_path = Path(self.out_path_var.get().strip())
        write_results(final, out_path, mode)

        # tag with date
        run_date = datetime.now().strftime("%Y-%m-%d")
        final["RunDate"] = run_date

        self.history_df = pd.concat([self.history_df, final], ignore_index=True)

        self.notebook.select(self.tab_dashboard)
        self.tab_dashboard.update_data(final, self.history_df)

        messagebox.showinfo("Done", f"Comparison for {run_date} complete! Results in {out_path}")

    @property
    def dim_rename_map(self) -> Dict[str,str]:
        """Return the dictionary of dimension renames."""
        return self.config_dict.get("dimension_renames", {})

    @dim_rename_map.setter
    def dim_rename_map(self, new_map: Dict[str,str]):
        """Allow setting the dimension rename map from code."""
        self.config_dict["dimension_renames"] = new_map

    def save_all_config(self):
        self.config_dict["erp_grid"] = self.erp_grid.get_config_block()
        self.config_dict["master_grid"] = self.master_grid.get_config_block()

        # Use our property (getter) for consistency
        self.config_dict["dimension_renames"] = self.dim_rename_map
        self.config_dict["paths"]["ERP_EXCEL_PATH"] = self.erp_path_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"] = self.master_path_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"] = self.exc_path_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"] = self.out_path_var.get().strip()
        self.config_dict["paths"]["CONFIG_PATH"] = self.cfg_path_var.get().strip()

        self.config_dict["comparison_option"] = self.mode_var.get()
        save_config(self.config_dict, Path(self.cfg_path_var.get().strip()))
        messagebox.showinfo("Saved", "All config saved successfully.")

def main():
    app = MainApp()
    app.mainloop()

if __name__ == "__main__":
    main()
