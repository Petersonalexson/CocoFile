# =============================================================================
# ULTRA-MEGA Data Reconciliation Tool
# =============================================================================
"""
Advanced Data Reconciliation Tool with Modern UI
----------------------------------------------
Features:
  • Sleek modern GUI using customtkinter
  • Comprehensive data comparison and analysis
  • Interactive visualizations
  • Extensive configuration options
  • Robust error handling and logging

Author: Al Pacino Dan
Last Updated: February 2025
"""

import customtkinter as ctk
import tkinter as tk
from tkinter import ttk, filedialog, simpledialog, messagebox
import logging
import os
import zipfile
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

import pandas as pd
import numpy as np

# Use TkAgg for better interactive chart support
import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import mplcursors

from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment, Border, Side

# =============================================================================
# CONFIGURATION SETTINGS
# =============================================================================

# File Paths
DEFAULT_PATHS = {
    "ALFA_PATH": "data/AlfaData.xlsx",
    "GAMMA_PATH": "data/GammaData.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/Missing_Items.xlsx",
    "CONFIG_PATH": "config/reconciliation_config.json",
    "LOG_PATH": "logs/reconciliation.log"
}

# Dimension Configurations
DIMENSION_CONFIG = {
    # ALFA Bad Dimensions (will be excluded)
    "ALFA_BAD_DIMS": [
        "Internal_Dim",
        "Test_Dim",
        "Legacy_Dim",
        "Temporary_Dim"
    ],
    
    # GAMMA Bad Dimensions
    "GAMMA_BAD_DIMS": [
        "Archive_Dim",
        "Backup_Dim",
        "Debug_Dim"
    ],
    
    # ALFA Dimension Renames (old_name: new_name)
    "ALFA_DIM_RENAMES": {
        "CustomerOld": "Customer",
        "ProductLegacy": "Product",
        "VendorArchive": "Vendor"
    },
    
    # GAMMA Dimension Renames
    "GAMMA_DIM_RENAMES": {
        "Cust_Old": "Customer",
        "Prod_Legacy": "Product",
        "Vend_Archive": "Vendor"
    }
}

# Attribute Configurations
ATTRIBUTE_CONFIG = {
    # ALFA Bad Attributes
    "ALFA_BAD_ATTRS": [
        "Internal_ID",
        "Created_By",
        "Modified_Date"
    ],
    
    # GAMMA Bad Attributes
    "GAMMA_BAD_ATTRS": [
        "System_ID",
        "Last_Modified",
        "Creator_ID"
    ],
    
    # ALFA Attribute Renames
    "ALFA_ATTR_RENAMES": {
        "Description_Old": "Description",
        "Status_Legacy": "Status",
        "Type_Archive": "Type"
    },
    
    # GAMMA Attribute Renames
    "GAMMA_ATTR_RENAMES": {
        "Desc_Old": "Description",
        "Stat_Legacy": "Status",
        "Type_Old": "Type"
    }
}

# Keep/DoNotKeep Rules
FILTER_RULES = {
    # ALFA Keep Rules (AND logic - must match all)
    "ALFA_KEEP_RULES": [
        ("Status", "Active,Pending"),
        ("Type", "Standard,Special"),
        ("Category", "A,B,C")
    ],
    
    # ALFA DoNotKeep Rules (OR logic - exclude if matches any)
    "ALFA_DONOTKEEP_RULES": [
        ("Status", "Deleted,Archived"),
        ("Type", "Test,Debug"),
        ("Category", "Internal")
    ],
    
    # GAMMA Keep Rules (OR logic - keep if matches any)
    "GAMMA_KEEP_RULES": [
        ("Status", "1,2,3"),
        ("Type", "STD,SPC"),
        ("Category", "PROD")
    ],
    
    # GAMMA DoNotKeep Rules (OR logic - exclude if matches any)
    "GAMMA_DONOTKEEP_RULES": [
        ("Status", "0,-1"),
        ("Type", "TST,DBG"),
        ("Category", "INT")
    ]
}

# UI Configuration
UI_CONFIG = {
    "WINDOW_SIZE": "1400x1000",
    "FONT_FAMILY": "Arial",
    "FONT_SIZES": {
        "HEADER": 16,
        "NORMAL": 14,
        "SMALL": 12
    },
    "PADDING": {
        "LARGE": 20,
        "MEDIUM": 10,
        "SMALL": 5
    },
    "COLORS": {
        "PRIMARY": "#2E86C1",
        "SECONDARY": "#85C1E9",
        "SUCCESS": "#58D68D",
        "WARNING": "#F4D03F",
        "ERROR": "#E74C3C"
    }
}

# =============================================================================
# Logging Configuration
# =============================================================================

def setup_logging(log_widget: Optional[ctk.CTkTextbox] = None) -> None:
    """
    Configure logging with both file and GUI output.
    
    Args:
        log_widget: Optional text widget for GUI logging
    """
    # Create logs directory if it doesn't exist
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # Create log file with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"reconciliation_{timestamp}.log"
    
    # Configure root logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    
    # Clear existing handlers
    logger.handlers.clear()
    
    # File handler
    file_handler = logging.FileHandler(log_file, mode="w", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        "%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter("%(levelname)s: %(message)s")
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # GUI handler if widget provided
    if log_widget:
        gui_handler = TextHandler(log_widget)
        gui_handler.setLevel(logging.INFO)
        gui_handler.setFormatter(console_formatter)
        logger.addHandler(gui_handler)
    
    logging.info("Logging system initialized")

class TextHandler(logging.Handler):
    """Custom logging handler for GUI text widget."""
    
    def __init__(self, text_widget: ctk.CTkTextbox):
        super().__init__()
        self.text_widget = text_widget
    
    def emit(self, record: logging.LogRecord):
        msg = self.format(record) + "\n"
        
        # Use after() to make thread-safe
        self.text_widget.after(0, self._write_to_widget, msg, record.levelname)
    
    def _write_to_widget(self, msg: str, level: str):
        self.text_widget.configure(state="normal")
        
        # Color-code based on log level
        color = {
            "DEBUG": "gray",
            "INFO": "black",
            "WARNING": "orange",
            "ERROR": "red",
            "CRITICAL": "red"
        }.get(level, "black")
        
        # Insert message with color
        self.text_widget.insert("end", msg, color)
        self.text_widget.see("end")
        self.text_widget.configure(state="disabled")

# =============================================================================
# Utility Functions
# =============================================================================

def load_config(config_path: Path) -> Dict[str, Any]:
    """
    Load configuration from JSON file.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Dictionary containing configuration settings
    """
    try:
        if config_path.exists():
            with open(config_path, 'r') as f:
                return json.load(f)
        logging.warning(f"Config file not found at {config_path}")
        return {}
    except Exception as e:
        logging.error(f"Error loading config: {e}")
        return {}

def save_config(config: Dict[str, Any], config_path: Path) -> None:
    """
    Save configuration to JSON file.
    
    Args:
        config: Configuration dictionary
        config_path: Path to save configuration
    """
    try:
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=4)
        logging.info(f"Configuration saved to {config_path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# =============================================================================
# Data Processing Functions
# =============================================================================

def filter_alfa_pre_merge(df: pd.DataFrame,
                         keep_rules: List[Tuple[str, str]],
                         disallow_rules: List[Tuple[str, str]]) -> pd.DataFrame:
    """
    Apply filtering to ALFA data before merge operations.
    
    Args:
        df: Input DataFrame
        keep_rules: List of (column, values) tuples for keeping rows
        disallow_rules: List of (column, values) tuples for excluding rows
        
    Returns:
        Filtered DataFrame
    """
    df = df.copy(deep=True)
    initial_rows = len(df)
    logging.info(f"Starting ALFA pre-merge filtering with {initial_rows} rows")
    
    # Apply KEEP rules (AND logic)
    if keep_rules:
        for column, values in keep_rules:
            if column not in df.columns:
                logging.warning(f"[ALFA Keep] Column '{column}' not found")
                continue
            
            allowed_values = [v.strip() for v in values.split(',') if v.strip()]
            df = df[df[column].isin(allowed_values)]
            
            rows_after = len(df)
            rows_filtered = initial_rows - rows_after
            logging.info(f"[ALFA Keep] Filter {column}: removed {rows_filtered} rows "
                        f"({rows_filtered/initial_rows:.1%})")
    
    # Apply DISALLOW rules (OR logic)
    if disallow_rules:
        for column, values in disallow_rules:
            if column not in df.columns:
                logging.warning(f"[ALFA Disallow] Column '{column}' not found")
                continue
            
            disallowed_values = [v.strip() for v in values.split(',') if v.strip()]
            df = df[~df[column].isin(disallowed_values)]
            
            rows_after = len(df)
            rows_filtered = initial_rows - rows_after
            logging.info(f"[ALFA Disallow] Filter {column}: removed {rows_filtered} rows "
                        f"({rows_filtered/initial_rows:.1%})")
    
    final_rows = len(df)
    total_filtered = initial_rows - final_rows
    logging.info(f"ALFA pre-merge filtering complete: {total_filtered} rows removed "
                f"({total_filtered/initial_rows:.1%})")
    
    return df

    # =============================================================================
# Data Processing Functions (continued)
# =============================================================================

def process_alfa_data(file_path: Path,
                     keep_rules: List[Tuple[str, str]],
                     disallow_rules: List[Tuple[str, str]],
                     bad_dimensions: List[str],
                     bad_attributes: List[str],
                     dimension_renames: Dict[str, str],
                     attribute_renames: Dict[str, str]) -> pd.DataFrame:
    """
    Process ALFA Excel data with comprehensive filtering and transformations.
    
    Args:
        file_path: Path to ALFA Excel file
        keep_rules: Rules for keeping data (AND logic)
        disallow_rules: Rules for excluding data
        bad_dimensions: Dimensions to exclude
        bad_attributes: Attributes to exclude
        dimension_renames: Mapping for dimension renaming
        attribute_renames: Mapping for attribute renaming
        
    Returns:
        Processed DataFrame
    """
    logging.info(f"[ALFA] Starting data processing from {file_path}")
    
    try:
        # Read Excel file
        df = pd.read_excel(file_path, engine="openpyxl")
        logging.info(f"[ALFA] Loaded {len(df)} initial rows")
        
        # Apply pre-merge filtering
        df = filter_alfa_pre_merge(df, keep_rules, disallow_rules)
        
        # Handle dimension column
        if "Dimension_Name" in df.columns:
            df.rename(columns={"Dimension_Name": "Dimension"}, inplace=True)
        elif "Dimension" not in df.columns:
            df.rename(columns={df.columns[2]: "Dimension"}, inplace=True)
        
        # Handle name column
        if "Name" not in df.columns:
            df.rename(columns={df.columns[3]: "Name"}, inplace=True)
        
        # Add record ID
        df["RecordID"] = df.index.astype(str)
        
        # Melt the DataFrame
        id_vars = ["Dimension", "RecordID"]
        value_vars = [c for c in df.columns if c not in id_vars]
        
        df_melted = df.melt(id_vars=id_vars,
                           value_vars=value_vars,
                           var_name="Attribute",
                           value_name="Value")
        
        # Apply renames
        if dimension_renames:
            df_melted["Dimension"] = df_melted["Dimension"].replace(dimension_renames)
        if attribute_renames:
            df_melted["Attribute"] = df_melted["Attribute"].replace(attribute_renames)
        
        # Apply exclusions
        if bad_dimensions:
            before_len = len(df_melted)
            df_melted = df_melted[~df_melted["Dimension"].isin(bad_dimensions)]
            logging.info(f"[ALFA] Removed {before_len - len(df_melted)} rows by bad dimensions")
        
        if bad_attributes:
            before_len = len(df_melted)
            df_melted = df_melted[~df_melted["Attribute"].isin(bad_attributes)]
            logging.info(f"[ALFA] Removed {before_len - len(df_melted)} rows by bad attributes")
        
        # Create reference name mapping
        ref_names = df_melted[df_melted["Attribute"] == "Name"][["RecordID", "Value"]]
        ref_names = ref_names.drop_duplicates("RecordID")
        ref_names.columns = ["RecordID", "RefName"]
        
        # Merge reference names back
        df_final = df_melted.merge(ref_names, on="RecordID", how="left")
        
        # Fill nulls and create keys
        for col in ["Dimension", "Attribute", "Value", "RefName"]:
            df_final[col] = df_final[col].fillna("").astype(str)
        
        df_final["GroupKey"] = (df_final["Dimension"].str.strip() + " | " + 
                               df_final["RefName"].str.strip())
        
        df_final["Key"] = (df_final["Dimension"].str.strip() + " | " +
                          df_final["RefName"].str.strip() + " | " +
                          df_final["Attribute"].str.strip() + " | " +
                          df_final["Value"].str.strip())
        
        # Remove duplicates
        df_final = df_final.drop_duplicates()
        
        logging.info(f"[ALFA] Processing complete: {len(df_final)} final rows")
        return df_final
        
    except Exception as e:
        logging.exception(f"[ALFA] Error processing data: {e}")
        raise

def process_gamma_data(zip_path: Path,
                      keep_rules: List[Tuple[str, str]],
                      disallow_rules: List[Tuple[str, str]],
                      bad_dimensions: List[str],
                      bad_attributes: List[str],
                      dimension_renames: Dict[str, str],
                      attribute_renames: Dict[str, str]) -> pd.DataFrame:
    """
    Process GAMMA ZIP data containing multiple text files.
    
    Args:
        zip_path: Path to GAMMA ZIP file
        keep_rules: Rules for keeping data (OR logic)
        disallow_rules: Rules for excluding data
        bad_dimensions: Dimensions to exclude
        bad_attributes: Attributes to exclude
        dimension_renames: Mapping for dimension renaming
        attribute_renames: Mapping for attribute renaming
        
    Returns:
        Processed DataFrame
    """
    logging.info(f"[GAMMA] Starting data processing from {zip_path}")
    
    try:
        all_data = []
        
        with zipfile.ZipFile(zip_path, 'r') as z:
            # Get list of text files
            txt_files = [f for f in z.namelist() if f.lower().endswith('.txt')]
            
            if not txt_files:
                logging.warning("[GAMMA] No .txt files found in ZIP")
                return pd.DataFrame()
            
            # Process each text file
            for txt_file in txt_files:
                try:
                    # Extract dimension from filename
                    dimension = Path(txt_file).stem.replace('_', ' ').strip()
                    
                    # Read and process file
                    with z.open(txt_file) as f:
                        df = pd.read_csv(f, encoding='utf-8')
                        
                        if df.empty:
                            logging.warning(f"[GAMMA] Empty file: {txt_file}")
                            continue
                        
                        # Add dimension column
                        df["Dimension"] = dimension
                        
                        # Handle name column
                        first_col = df.columns[0]
                        df.rename(columns={first_col: "Name"}, inplace=True)
                        df["Name"] = df["Name"].fillna("Unknown")
                        
                        # Apply keep/disallow rules
                        df = filter_gamma_rules(df, keep_rules, disallow_rules)
                        
                        # Add record ID and melt
                        df["RecordID"] = df.index.astype(str)
                        id_vars = ["Dimension", "RecordID"]
                        value_vars = [c for c in df.columns if c not in id_vars]
                        
                        df_melted = df.melt(id_vars=id_vars,
                                          value_vars=value_vars,
                                          var_name="Attribute",
                                          value_name="Value")
                        
                        all_data.append(df_melted)
                        logging.info(f"[GAMMA] Processed {txt_file}: {len(df_melted)} rows")
                        
                except Exception as e:
                    logging.error(f"[GAMMA] Error processing {txt_file}: {e}")
                    continue
        
        if not all_data:
            logging.warning("[GAMMA] No valid data processed")
            return pd.DataFrame()
        
        # Combine all data
        df_combined = pd.concat(all_data, ignore_index=True)
        
        # Apply renames
        if dimension_renames:
            df_combined["Dimension"] = df_combined["Dimension"].replace(dimension_renames)
        if attribute_renames:
            df_combined["Attribute"] = df_combined["Attribute"].replace(attribute_renames)
        
        # Apply exclusions
        if bad_dimensions:
            before_len = len(df_combined)
            df_combined = df_combined[~df_combined["Dimension"].isin(bad_dimensions)]
            logging.info(f"[GAMMA] Removed {before_len - len(df_combined)} rows by bad dimensions")
        
        if bad_attributes:
            before_len = len(df_combined)
            df_combined = df_combined[~df_combined["Attribute"].isin(bad_attributes)]
            logging.info(f"[GAMMA] Removed {before_len - len(df_combined)} rows by bad attributes")
        
        # Create reference names
        ref_names = df_combined[df_combined["Attribute"] == "Name"][["RecordID", "Value"]]
        ref_names = ref_names.drop_duplicates("RecordID")
        ref_names.columns = ["RecordID", "RefName"]
        
        # Merge reference names
        df_final = df_combined.merge(ref_names, on="RecordID", how="left")
        
        # Fill nulls and create keys
        for col in ["Dimension", "Attribute", "Value", "RefName"]:
            df_final[col] = df_final[col].fillna("").astype(str)
        
        df_final["GroupKey"] = (df_final["Dimension"].str.strip() + " | " + 
                               df_final["RefName"].str.strip())
        
        df_final["Key"] = (df_final["Dimension"].str.strip() + " | " +
                          df_final["RefName"].str.strip() + " | " +
                          df_final["Attribute"].str.strip() + " | " +
                          df_final["Value"].str.strip())
        
        # Remove duplicates
        df_final = df_final.drop_duplicates()
        
        logging.info(f"[GAMMA] Processing complete: {len(df_final)} final rows")
        return df_final
        
    except Exception as e:
        logging.exception(f"[GAMMA] Error processing ZIP file: {e}")
        raise

def filter_gamma_rules(df: pd.DataFrame,
                      keep_rules: List[Tuple[str, str]],
                      disallow_rules: List[Tuple[str, str]]) -> pd.DataFrame:
    """
    Apply GAMMA-specific filtering rules.
    
    Args:
        df: Input DataFrame
        keep_rules: Rules for keeping data (OR logic)
        disallow_rules: Rules for excluding data
        
    Returns:
        Filtered DataFrame
    """
    df = df.copy(deep=True)
    initial_rows = len(df)
    
    # Apply KEEP rules (OR logic)
    if keep_rules:
        keep_mask = pd.Series(False, index=df.index)
        
        for column, values in keep_rules:
            if column not in df.columns:
                logging.warning(f"[GAMMA Keep] Column '{column}' not found")
                continue
            
            allowed_values = [v.strip() for v in values.split(',') if v.strip()]
            keep_mask |= df[column].isin(allowed_values)
        
        df = df[keep_mask]
        rows_after = len(df)
        rows_filtered = initial_rows - rows_after
        logging.info(f"[GAMMA Keep] Filtered {rows_filtered} rows ({rows_filtered/initial_rows:.1%})")
    
    # Apply DISALLOW rules (OR logic)
    if disallow_rules:
        for column, values in disallow_rules:
            if column not in df.columns:
                logging.warning(f"[GAMMA Disallow] Column '{column}' not found")
                continue
            
            disallowed_values = [v.strip() for v in values.split(',') if v.strip()]
            df = df[~df[column].isin(disallowed_values)]
            
            rows_after = len(df)
            rows_filtered = initial_rows - rows_after
            logging.info(f"[GAMMA Disallow] Filtered {rows_filtered} rows ({rows_filtered/initial_rows:.1%})")
    
    return df

# =============================================================================
# Reconciliation Logic
# =============================================================================

def compare_data(df_alfa: pd.DataFrame,
                df_gamma: pd.DataFrame,
                comparison_mode: int = 2) -> pd.DataFrame:
    """
    Compare ALFA and GAMMA data using the specified comparison mode.
    
    Args:
        df_alfa: Processed ALFA DataFrame
        df_gamma: Processed GAMMA DataFrame
        comparison_mode: 
            1 = All Missing
            2 = Missing - Name Special
            3 = Full Comparison
            
    Returns:
        DataFrame containing differences
    """
    logging.info(f"Starting data comparison (Mode {comparison_mode})")
    
    differences = []
    
    # Build lookup dictionaries
    alfa_dict = build_lookup_dict(df_alfa)
    gamma_dict = build_lookup_dict(df_gamma)
    
    # Get all unique keys
    all_keys = set(alfa_dict.keys()) | set(gamma_dict.keys())
    
    for group_key in all_keys:
        dimension = group_key.split(" | ")[0]
        alfa_data = alfa_dict.get(group_key, {})
        gamma_data = gamma_dict.get(group_key, {})
        
        if comparison_mode == 1:  # All Missing
            differences.extend(compare_mode_1(dimension, group_key, alfa_data, gamma_data))
        elif comparison_mode == 2:  # Missing - Name Special
            differences.extend(compare_mode_2(dimension, group_key, alfa_data, gamma_data))
        elif comparison_mode == 3:  # Full Comparison
            differences.extend(compare_mode_3(dimension, group_key, alfa_data, gamma_data))
    
    if not differences:
        logging.info("No differences found")
        return pd.DataFrame()
    
    df_differences = pd.DataFrame(differences)
    logging.info(f"Found {len(df_differences)} differences")
    return df_differences

def build_lookup_dict(df: pd.DataFrame) -> Dict[str, Dict[str, str]]:
    """Build a nested dictionary for efficient lookups."""
    lookup = {}
    for group_key, group_df in df.groupby("GroupKey"):
        lookup[group_key] = dict(zip(group_df["Attribute"], group_df["Value"]))
    return lookup

def compare_mode_1(dimension: str, group_key: str, 
                  alfa_data: Dict[str, str], 
                  gamma_data: Dict[str, str]) -> List[Dict[str, str]]:
    """Implement comparison mode 1 (All Missing)."""
    differences = []
    
    # Get all attributes
    all_attrs = set(alfa_data.keys()) | set(gamma_data.keys())
    
    for attr in all_attrs:
        alfa_val = alfa_data.get(attr, "").strip()
        gamma_val = gamma_data.get(attr, "").strip()
        
        if alfa_val != gamma_val:
            # Handle missing in ALFA
            if not alfa_val and gamma_val:
                differences.append({
                    "Dimension": dimension,
                    "GroupKey": group_key,
                    "Attribute": attr,
                    "Value": gamma_val,
                    "Missing_In": "ALFA"
                })
            # Handle missing in GAMMA
            elif alfa_val and not gamma_val:
                differences.append({
                    "Dimension": dimension,
                    "GroupKey": group_key,
                    "Attribute": attr,
                    "Value": alfa_val,
                    "Missing_In": "GAMMA"
                })
            # Handle different values
            else:
                differences.extend([
                    {
                        "Dimension": dimension,
                        "GroupKey": group_key,
                        "Attribute": attr,
                        "Value": alfa_val,
                        "Missing_In": "GAMMA"
                    },
                    {
                        "Dimension": dimension,
                        "GroupKey": group_key,
                        "Attribute": attr,
                        "Value": gamma_val,
                        "Missing_In": "ALFA"
                    }
                ])
    
    return differences

def compare_mode_2(dimension: str, group_key: str,
                  alfa_data: Dict[str, str],
                  gamma_data: Dict[str, str]) -> List[Dict[str, str]]:
    """Implement comparison mode 2 (Missing - Name Special)."""
    differences = []
    
    alfa_name = alfa_data.get("Name", "").strip()
    gamma_name = gamma_data.get("Name", "").strip()
    
    # Case 1: Both have names but they differ
    if alfa_name and gamma_name and alfa_name != gamma_name:
        differences.extend([
            {
                "Dimension": dimension,
                "GroupKey": group_key,
                "Attribute": "Name",
                "Value": alfa_name,
                "Missing_In": "GAMMA"
            },
            {
                "Dimension": dimension,
                "GroupKey": group_key,
                "Attribute": "Name",
                "Value": gamma_name,
                "Missing_In": "ALFA"
            }
        ])
    
    # Case 2: Names match, compare other attributes
    elif alfa_name and gamma_name and alfa_name == gamma_name:
        all_attrs = set(alfa_data.keys()) | set(gamma_data.keys()) - {"Name"}
        for attr in all_attrs:
            alfa_val = alfa_data.get(attr, "").strip()
            gamma_val = gamma_data.get(attr, "").strip()
            
            if alfa_val != gamma_val:
                if not alfa_val and gamma_val:
                    differences.append({
                        "Dimension": dimension,
                        "GroupKey": group_key,
                        "Attribute": attr,
                        "Value": gamma_val,
                        "Missing_In": "ALFA"
                    })
                elif alfa_val and not gamma_val:
                    differences.append({
                        "Dimension": dimension,
                        "GroupKey": group_key,
                        "Attribute": attr,
                        "Value": alfa_val,
                        "Missing_In": "GAMMA"
                    })
    
    # Case 3: One side missing name
    elif not alfa_name and gamma_name:
        differences.append({
            "Dimension": dimension,
            "GroupKey": group_key,
            "Attribute": "Name",
            "Value": gamma_name,
            "Missing_In": "ALFA"
        })
    elif alfa_name and not gamma_name:
        differences.append({
            "Dimension": dimension,
            "GroupKey": group_key,
            "Attribute": "Name",
            "Value": alfa_name,
            "Missing_In": "GAMMA"
        })
    
    return differences

def compare_mode_3(dimension: str, group_key: str,
                  alfa_data: Dict[str, str],
                  gamma_data: Dict[str, str]) -> List[Dict[str, str]]:
    """Implement comparison mode 3 (Full Comparison)."""
    differences = []
    matches = []
    
    all_attrs = set(alfa_data.keys()) | set(gamma_data.keys())
    
    for attr in all_attrs:
        alfa_val = alfa_data.get(attr, "").strip()
        gamma_val = gamma_data.get(attr, "").strip()
        
        if alfa_val == gamma_val:
            matches.append({
                "Dimension": dimension,
                "GroupKey": group_key,
                "Attribute": attr,
                "Value": alfa_val,
                "Missing_In": ""
            })
        else:
            if not alfa_val and gamma_val:
                differences.append({
                    "Dimension": dimension,
                    "GroupKey": group_key,
                    "Attribute": attr,
                    "Value": gamma_val,
                    "Missing_In": "ALFA"
                })
            elif alfa_val and not gamma_val:
                differences.append({
                    "Dimension": dimension,
                    "GroupKey": group_key,
                    "Attribute": attr,
                    "Value": alfa_val,
                    "Missing_In": "GAMMA"
                })
            else:
                differences.extend([
                    {
                        "Dimension": dimension,
                        "GroupKey": group_key,
                        "Attribute": attr,
                        "Value": alfa_val,
                        "Missing_In": "GAMMA"
                    },
                    {
                        "Dimension": dimension,
                        "GroupKey": group_key,
                        "Attribute": attr,
                        "Value": gamma_val,
                        "Missing_In": "ALFA"
                    }
                ])
    
    return differences

# =============================================================================
# GUI Components
# =============================================================================

class ModernScrollableFrame(ctk.CTkFrame):
    """Enhanced scrollable frame with both vertical and horizontal scrollbars."""
    
    def __init__(self, container, *args, **kwargs):
        super().__init__(container, *args, **kwargs)
        
        # Create canvas
        self.canvas = ctk.CTkCanvas(self)
        self.scrollable_frame = ctk.CTkFrame(self.canvas)
        
        # Add scrollbars
        self.vsb = ctk.CTkScrollbar(self, orientation="vertical",
                                   command=self.canvas.yview)
        self.hsb = ctk.CTkScrollbar(self, orientation="horizontal",
                                   command=self.canvas.xview)
        
        # Configure canvas
        self.canvas.configure(yscrollcommand=self.vsb.set,
                            xscrollcommand=self.hsb.set)
        
        # Grid layout
        self.canvas.grid(row=0, column=0, sticky="nsew")
        self.vsb.grid(row=0, column=1, sticky="ns")
        self.hsb.grid(row=1, column=0, sticky="ew")
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame,
                                anchor="nw")
        
        # Configure grid weights
        self.grid_rowconfigure(0, weight=1)
        self.grid_columnconfigure(0, weight=1)
        
        # Bind events
        self.scrollable_frame.bind("<Configure>",
            lambda e: self.canvas.configure(
                scrollregion=self.canvas.bbox("all")
            )
        )
        
        # Bind mouse wheel
        self.canvas.bind_all("<MouseWheel>", self._on_mousewheel)
        
    def _on_mousewheel(self, event):
        self.canvas.yview_scroll(int(-1*(event.delta/120)), "units")

class ReconciliationGUI(ctk.CTk):
    """Main GUI application for data reconciliation."""
    
    def __init__(self):
        super().__init__()
        
        # Configure window
        self.title("Data Reconciliation Tool")
        self.geometry(UI_CONFIG["WINDOW_SIZE"])
        
        # Initialize variables
        self.comparison_mode = tk.IntVar(value=2)
        self.current_config = {}
        self.df_differences = None
        
        # Create tabs
        self.setup_tabs()
        
        # Initialize logging
        self.setup_logging()
        
        # Load configuration
        self.load_saved_config()
    
    def setup_tabs(self):
        """Create and configure all tabs."""
        self.tab_view = ctk.CTkTabview(self)
        self.tab_view.pack(fill="both", expand=True, padx=10, pady=10)
        
        # Add tabs
        self.tab_view.add("Settings")
        self.tab_view.add("Rules")
        self.tab_view.add("Process")
        self.tab_view.add("Results")
        
        # Build each tab
        self.build_settings_tab()
        self.build_rules_tab()
        self.build_process_tab()
        self.build_results_tab()
    
    def build_settings_tab(self):
        """Build the settings tab with file paths and configuration."""
        tab = self.tab_view.tab("Settings")
        
        # Create frames
        paths_frame = ctk.CTkFrame(tab)
        paths_frame.pack(fill="x", padx=10, pady=5)
        
        # File path settings
        self.path_entries = {}
        for key, default in DEFAULT_PATHS.items():
            if key not in ["CONFIG_PATH", "LOG_PATH"]:
                frame = ctk.CTkFrame(paths_frame)
                frame.pack(fill="x", pady=5)
                
                label = ctk.CTkLabel(frame, text=f"{key.replace('_', ' ')}:",
                                   font=(UI_CONFIG["FONT_FAMILY"], 
                                        UI_CONFIG["FONT_SIZES"]["NORMAL"]))
                label.pack(side="left", padx=5)
                
                entry = ctk.CTkEntry(frame, width=400,
                                   font=(UI_CONFIG["FONT_FAMILY"],
                                        UI_CONFIG["FONT_SIZES"]["NORMAL"]))
                entry.insert(0, default)
                entry.pack(side="left", padx=5)
                
                btn = ctk.CTkButton(frame, text="Browse",
                                  command=lambda k=key: self.browse_file(k))
                btn.pack(side="left", padx=5)
                
                self.path_entries[key] = entry
    
    def build_rules_tab(self):
        """Build the rules tab with all filtering and transformation rules."""
        tab = self.tab_view.tab("Rules")
        
        # Create scrollable frame
        scroll_frame = ModernScrollableFrame(tab)
        scroll_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        # Create sections for different rule types
        self.create_dimension_rules(scroll_frame.scrollable_frame)
        self.create_attribute_rules(scroll_frame.scrollable_frame)
        self.create_filter_rules(scroll_frame.scrollable_frame)
    
    def build_process_tab(self):
        """Build the process tab with comparison options and execution controls."""
        tab = self.tab_view.tab("Process")
        
        # Comparison mode options
        mode_frame = ctk.CTkFrame(tab)
        mode_frame.pack(fill="x", padx=10, pady=5)
        
        ctk.CTkLabel(mode_frame, text="Comparison Mode:",
                    font=(UI_CONFIG["FONT_FAMILY"],
                          UI_CONFIG["FONT_SIZES"]["HEADER"])).pack(pady=5)
        
        modes = [
            ("All Missing", 1),
            ("Missing - Name Special", 2),
            ("Full Comparison", 3)
        ]
        
        for text, value in modes:
            ctk.CTkRadioButton(mode_frame, text=text,
                              variable=self.comparison_mode,
                              value=value,
                              font=(UI_CONFIG["FONT_FAMILY"],
                                    UI_CONFIG["FONT_SIZES"]["NORMAL"])).pack(pady=2)
# Progress indicators 
        progress_frame = ctk.CTkFrame(tab)
        progress_frame.pack(fill="x", padx=10, pady=10)
        
        self.progress_bar = ctk.CTkProgressBar(progress_frame)
        self.progress_bar.pack(fill="x", padx=20, pady=5)
        self.progress_bar.set(0)
        
        self.progress_label = ctk.CTkLabel(progress_frame, 
                                         text="Ready to process",
                                         font=(UI_CONFIG["FONT_FAMILY"],
                                               UI_CONFIG["FONT_SIZES"]["NORMAL"]))
        self.progress_label.pack(pady=5)
        
        # Control buttons
        button_frame = ctk.CTkFrame(tab)
        button_frame.pack(fill="x", padx=10, pady=5)
        
        ctk.CTkButton(button_frame,
                     text="Run Reconciliation",
                     command=self.run_reconciliation,
                     font=(UI_CONFIG["FONT_FAMILY"],
                           UI_CONFIG["FONT_SIZES"]["NORMAL"])).pack(side="left", padx=5)
        
        ctk.CTkButton(button_frame,
                     text="Save Results",
                     command=self.save_results,
                     font=(UI_CONFIG["FONT_FAMILY"],
                           UI_CONFIG["FONT_SIZES"]["NORMAL"])).pack(side="left", padx=5)
    
    def build_results_tab(self):
        """Build the results tab with visualizations and analysis."""
        tab = self.tab_view.tab("Results")
        
        # Create scrollable frame for charts
        self.chart_frame = ModernScrollableFrame(tab)
        self.chart_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        # Placeholder for charts
        self.chart_containers = {
            'dimension': ctk.CTkFrame(self.chart_frame.scrollable_frame),
            'attribute': ctk.CTkFrame(self.chart_frame.scrollable_frame),
            'missing': ctk.CTkFrame(self.chart_frame.scrollable_frame)
        }
        
        for container in self.chart_containers.values():
            container.pack(fill="x", pady=10)
    
    def run_reconciliation(self):
        """Execute the reconciliation process."""
        try:
            # Update progress
            self.progress_bar.set(0.1)
            self.progress_label.configure(text="Loading ALFA data...")
            self.update()
            
            # Process ALFA data
            df_alfa = process_alfa_data(
                Path(self.path_entries["ALFA_PATH"].get()),
                self.get_rules("ALFA_KEEP"),
                self.get_rules("ALFA_DONOTKEEP"),
                self.get_dimensions("ALFA_BAD"),
                self.get_attributes("ALFA_BAD"),
                self.get_renames("ALFA_DIM"),
                self.get_renames("ALFA_ATTR")
            )
            
            # Update progress
            self.progress_bar.set(0.3)
            self.progress_label.configure(text="Loading GAMMA data...")
            self.update()
            
            # Process GAMMA data
            df_gamma = process_gamma_data(
                Path(self.path_entries["GAMMA_PATH"].get()),
                self.get_rules("GAMMA_KEEP"),
                self.get_rules("GAMMA_DONOTKEEP"),
                self.get_dimensions("GAMMA_BAD"),
                self.get_attributes("GAMMA_BAD"),
                self.get_renames("GAMMA_DIM"),
                self.get_renames("GAMMA_ATTR")
            )
            
            # Update progress
            self.progress_bar.set(0.6)
            self.progress_label.configure(text="Comparing data...")
            self.update()
            
            # Compare data
            self.df_differences = compare_data(
                df_alfa,
                df_gamma,
                self.comparison_mode.get()
            )
            
            # Update progress
            self.progress_bar.set(0.8)
            self.progress_label.configure(text="Generating visualizations...")
            self.update()
            
            # Create visualizations
            self.create_visualizations()
            
            # Final update
            self.progress_bar.set(1.0)
            self.progress_label.configure(text="Processing complete!")
            self.update()
            
            # Switch to results tab
            self.tab_view.set("Results")
            
        except Exception as e:
            logging.exception("Error during reconciliation")
            messagebox.showerror("Error", f"An error occurred: {str(e)}")
            self.progress_label.configure(text="Error occurred during processing")
    
    def create_visualizations(self):
        """Create and update all visualizations."""
        if self.df_differences is None or self.df_differences.empty:
            logging.warning("No differences to visualize")
            return
        
        # Clear existing charts
        for container in self.chart_containers.values():
            for widget in container.winfo_children():
                widget.destroy()
        
        # Create dimension chart
        dim_counts = self.df_differences["Dimension"].value_counts()
        self.create_bar_chart(
            self.chart_containers['dimension'],
            dim_counts,
            "Differences by Dimension",
            "Dimension",
            "Count"
        )
        
        # Create attribute chart
        attr_counts = self.df_differences["Attribute"].value_counts()
        self.create_bar_chart(
            self.chart_containers['attribute'],
            attr_counts,
            "Differences by Attribute",
            "Attribute",
            "Count"
        )
        
        # Create missing chart
        missing_counts = self.df_differences["Missing_In"].value_counts()
        self.create_bar_chart(
            self.chart_containers['missing'],
            missing_counts,
            "Differences by Source",
            "Source",
            "Count"
        )
    
    def create_bar_chart(self, container, data, title, xlabel, ylabel):
        """Create an interactive bar chart."""
        # Create figure
        fig = Figure(figsize=(12, 6))
        ax = fig.add_subplot(111)
        
        # Create bars
        bars = ax.bar(range(len(data)), data.values,
                     color=UI_CONFIG["COLORS"]["PRIMARY"])
        
        # Customize appearance
        ax.set_xticks(range(len(data)))
        ax.set_xticklabels(data.index, rotation=45, ha='right')
        
        ax.set_title(title, pad=20)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height):,}',
                   ha='center', va='bottom')
        
        # Create canvas
        canvas = FigureCanvasTkAgg(fig, master=container)
        canvas.draw()
        
        # Add tooltips
        mplcursors.cursor(bars, hover=True)
        
        # Pack canvas
        canvas.get_tk_widget().pack(fill="both", expand=True)
    
    def save_results(self):
        """Save the reconciliation results to Excel."""
        if self.df_differences is None or self.df_differences.empty:
            messagebox.showwarning("Warning", "No results to save")
            return
        
        try:
            output_path = Path(self.path_entries["OUTPUT_PATH"].get())
            
            # Ensure output directory exists
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Save to Excel
            self.df_differences.to_excel(output_path, index=False)
            
            # Apply Excel formatting
            self.format_excel_output(output_path)
            
            messagebox.showinfo("Success", f"Results saved to {output_path}")
            
        except Exception as e:
            logging.exception("Error saving results")
            messagebox.showerror("Error", f"Error saving results: {str(e)}")
    
    def format_excel_output(self, path: Path):
        """Apply formatting to the Excel output."""
        try:
            wb = load_workbook(path)
            ws = wb.active
            
            # Define styles
            header_font = Font(name=UI_CONFIG["FONT_FAMILY"],
                             size=UI_CONFIG["FONT_SIZES"]["NORMAL"],
                             bold=True)
            
            data_font = Font(name=UI_CONFIG["FONT_FAMILY"],
                           size=UI_CONFIG["FONT_SIZES"]["NORMAL"])
            
            header_fill = PatternFill(start_color="E0E0E0",
                                    end_color="E0E0E0",
                                    fill_type="solid")
            
            # Apply header styles
            for cell in ws[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center")
            
            # Apply data styles
            for row in ws.iter_rows(min_row=2):
                for cell in row:
                    cell.font = data_font
                    cell.alignment = Alignment(horizontal="left")
            
            # Adjust column widths
            for column in ws.columns:
                max_length = 0
                column = list(column)
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = (max_length + 2)
                ws.column_dimensions[column[0].column_letter].width = adjusted_width
            
            # Freeze top row
            ws.freeze_panes = "A2"
            
            # Save formatted workbook
            wb.save(path)
            
        except Exception as e:
            logging.exception("Error formatting Excel output")
            raise

def main():
    """Main entry point for the application."""
    try:
        app = ReconciliationGUI()
        app.mainloop()
    except Exception as e:
        logging.exception("Critical error in main application")
        messagebox.showerror("Critical Error",
                           f"Application encountered a critical error: {str(e)}")

if __name__ == "__main__":
    main()
