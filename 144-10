#!/usr/bin/env python3
"""
ULTRA-MEGA Data Reconciliation Tool
----------------------------------------------
Advanced Data Reconciliation Tool with Modern UI

Features:
  • Sleek modern GUI using customtkinter.
  • Comprehensive data comparison and analysis.
  • Interactive visualizations with Matplotlib (mplcursors enabled).
  • Extensive configuration options via JSON and in–UI rules.
  • Robust error handling and logging.
  • Excel output includes columns: Key, Dimension, Name, Attribute, Value,
    Comments_1, Comments_2, Action Item, Missing In.
  • Exception table merging: if provided, rows with "hide exception"=="yes"
    are omitted and Comments_1/Comments_2 are merged.
  
Author: Al Pacino Dan
Last Updated: February 2025
"""

import customtkinter as ctk
import tkinter as tk
from tkinter import ttk, filedialog, simpledialog, messagebox
import logging
import os
import zipfile
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

import pandas as pd
import numpy as np

# Use TkAgg backend for interactive charts
import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import mplcursors

from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment

# =============================================================================
# CONFIGURATION SETTINGS
# =============================================================================

DEFAULT_PATHS = {
    "ALFA_PATH": "data/AlfaData.xlsx",
    "GAMMA_PATH": "data/GammaData.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",  # Optional exception table
    "OUTPUT_PATH": "output/Missing_Items.xlsx",
    "CONFIG_PATH": "config/reconciliation_config.json",
    "LOG_PATH": "logs/reconciliation.log"
}

DIMENSION_CONFIG = {
    "ALFA_BAD_DIMS": ["Internal_Dim", "Test_Dim", "Legacy_Dim", "Temporary_Dim"],
    "GAMMA_BAD_DIMS": ["Archive_Dim", "Backup_Dim", "Debug_Dim"],
    "ALFA_DIM_RENAMES": {"CustomerOld": "Customer", "ProductLegacy": "Product", "VendorArchive": "Vendor"},
    "GAMMA_DIM_RENAMES": {"Cust_Old": "Customer", "Prod_Legacy": "Product", "Vend_Archive": "Vendor"}
}

ATTRIBUTE_CONFIG = {
    "ALFA_BAD_ATTRS": ["Internal_ID", "Created_By", "Modified_Date"],
    "GAMMA_BAD_ATTRS": ["System_ID", "Last_Modified", "Creator_ID"],
    "ALFA_ATTR_RENAMES": {"Description_Old": "Description", "Status_Legacy": "Status", "Type_Archive": "Type"},
    "GAMMA_ATTR_RENAMES": {"Desc_Old": "Description", "Stat_Legacy": "Status", "Type_Old": "Type"}
}

FILTER_RULES = {
    "ALFA_KEEP_RULES": [("Status", "Active,Pending"), ("Type", "Standard,Special"), ("Category", "A,B,C")],
    "ALFA_DONOTKEEP_RULES": [("Status", "Deleted,Archived"), ("Type", "Test,Debug"), ("Category", "Internal")],
    "GAMMA_KEEP_RULES": [("Status", "1,2,3"), ("Type", "STD,SPC"), ("Category", "PROD")],
    "GAMMA_DONOTKEEP_RULES": [("Status", "0,-1"), ("Type", "TST,DBG"), ("Category", "INT")]
}

UI_CONFIG = {
    "WINDOW_SIZE": "1400x1000",
    "FONT_FAMILY": "Arial",
    "FONT_SIZES": {"HEADER": 16, "NORMAL": 14, "SMALL": 12},
    "PADDING": {"LARGE": 20, "MEDIUM": 10, "SMALL": 5},
    "COLORS": {"PRIMARY": "#2E86C1", "SECONDARY": "#85C1E9", "SUCCESS": "#58D68D", "WARNING": "#F4D03F", "ERROR": "#E74C3C"}
}

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

def setup_logging(log_widget: Optional[ctk.CTkTextbox] = None) -> None:
    # Ensure logs directory exists
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"reconciliation_{timestamp}.log"
    
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()
    
    fh = logging.FileHandler(log_file, mode="w", encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"))
    logger.addHandler(fh)
    
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
    logger.addHandler(ch)
    
    if log_widget:
        gui_handler = TextHandler(log_widget)
        gui_handler.setLevel(logging.INFO)
        gui_handler.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
        logger.addHandler(gui_handler)
    
    logging.info("Logging system initialized")

class TextHandler(logging.Handler):
    """Custom logging handler for outputting logs into a CTkTextbox."""
    def __init__(self, text_widget: ctk.CTkTextbox):
        super().__init__()
        self.text_widget = text_widget
    def emit(self, record: logging.LogRecord):
        msg = self.format(record) + "\n"
        self.text_widget.after(0, self._append, msg)
    def _append(self, msg: str):
        self.text_widget.configure(state="normal")
        self.text_widget.insert("end", msg)
        self.text_widget.see("end")
        self.text_widget.configure(state="disabled")

# =============================================================================
# UTILITY FUNCTIONS (CONFIG FILES)
# =============================================================================

def load_config(config_path: Path) -> Dict[str, Any]:
    try:
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
                logging.info(f"Loaded configuration from {config_path}")
                return config
        logging.warning(f"Config file not found at {config_path}")
        return {}
    except Exception as e:
        logging.error(f"Error loading config: {e}")
        return {}

def save_config(config: Dict[str, Any], config_path: Path) -> None:
    try:
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=4)
        logging.info(f"Configuration saved to {config_path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# =============================================================================
# DATA PROCESSING FUNCTIONS
# =============================================================================

def filter_alfa_pre_merge(df: pd.DataFrame,
                          keep_rules: List[Tuple[str, str]],
                          disallow_rules: List[Tuple[str, str]]) -> pd.DataFrame:
    df = df.copy(deep=True)
    initial_rows = len(df)
    # Apply KEEP rules (AND logic)
    if keep_rules:
        for col, val_str in keep_rules:
            if col not in df.columns:
                logging.warning(f"[ALFA KEEP] Column '{col}' not found.")
                continue
            allowed = {v.strip() for v in val_str.split(",") if v.strip()}
            df = df[df[col].isin(allowed)]
    # Apply DISALLOW rules (OR logic)
    if disallow_rules:
        for col, val_str in disallow_rules:
            if col not in df.columns:
                logging.warning(f"[ALFA DISALLOW] Column '{col}' not found.")
                continue
            not_allowed = {v.strip() for v in val_str.split(",") if v.strip()}
            df = df[~df[col].isin(not_allowed)]
    logging.info(f"[ALFA] Pre-merge filtering complete: {initial_rows - len(df)} rows removed")
    return df

def process_alfa_data(file_path: Path,
                      keep_rules: List[Tuple[str, str]],
                      disallow_rules: List[Tuple[str, str]],
                      bad_dimensions: List[str],
                      bad_attributes: List[str],
                      dimension_renames: Dict[str, str],
                      attribute_renames: Dict[str, str]) -> pd.DataFrame:
    logging.info(f"[ALFA] Processing data from {file_path}")
    try:
        df = pd.read_excel(file_path, engine="openpyxl")
        logging.info(f"[ALFA] Loaded {len(df)} rows")
        df = filter_alfa_pre_merge(df, keep_rules, disallow_rules)
        # Rename columns (use "Dimension_Name" if available, else assume third column is Dimension)
        if "Dimension_Name" in df.columns:
            df.rename(columns={"Dimension_Name": "Dimension"}, inplace=True)
        elif "Dimension" not in df.columns and len(df.columns) >= 3:
            df.rename(columns={df.columns[2]: "Dimension"}, inplace=True)
        if "Name" not in df.columns and len(df.columns) >= 4:
            df.rename(columns={df.columns[3]: "Name"}, inplace=True)
        df["RecordID"] = df.index.astype(str)
        # Melt the data
        id_vars = ["Dimension", "RecordID"]
        value_vars = [col for col in df.columns if col not in id_vars]
        df_melted = df.melt(id_vars=id_vars, value_vars=value_vars,
                            var_name="Attribute", value_name="Value")
        if dimension_renames:
            df_melted["Dimension"] = df_melted["Dimension"].replace(dimension_renames)
        if attribute_renames:
            df_melted["Attribute"] = df_melted["Attribute"].replace(attribute_renames)
        if bad_dimensions:
            before = len(df_melted)
            df_melted = df_melted[~df_melted["Dimension"].isin(bad_dimensions)]
            logging.info(f"[ALFA] Excluded {before - len(df_melted)} rows by dimensions")
        if bad_attributes:
            before = len(df_melted)
            df_melted = df_melted[~df_melted["Attribute"].isin(bad_attributes)]
            logging.info(f"[ALFA] Excluded {before - len(df_melted)} rows by attributes")
        # Create reference names from rows where Attribute=="Name"
        ref_df = df_melted[df_melted["Attribute"] == "Name"][["RecordID", "Value"]].drop_duplicates("RecordID")
        ref_df.rename(columns={"Value": "RefName"}, inplace=True)
        df_final = df_melted.merge(ref_df, on="RecordID", how="left")
        for col in ["Dimension", "Attribute", "Value", "RefName"]:
            df_final[col] = df_final[col].fillna("").astype(str)
        df_final["GroupKey"] = df_final["Dimension"].str.strip() + " | " + df_final["RefName"].str.strip()
        df_final["Name"] = df_final["RefName"]  # use RefName as Name
        df_final["Key"] = (df_final["Dimension"].str.strip() + " | " +
                           df_final["Name"].str.strip() + " | " +
                           df_final["Attribute"].str.strip() + " | " +
                           df_final["Value"].str.strip())
        df_final.drop_duplicates(inplace=True)
        logging.info(f"[ALFA] Processing complete: {len(df_final)} final rows")
        return df_final
    except Exception as e:
        logging.exception(f"[ALFA] Error processing data: {e}")
        raise

def filter_gamma_rules(df: pd.DataFrame,
                       keep_rules: List[Tuple[str, str]],
                       disallow_rules: List[Tuple[str, str]]) -> pd.DataFrame:
    df = df.copy(deep=True)
    initial_rows = len(df)
    if keep_rules:
        mask = pd.Series(False, index=df.index)
        for col, val_str in keep_rules:
            if col not in df.columns:
                logging.warning(f"[GAMMA KEEP] Column '{col}' not found.")
                continue
            allowed = {v.strip() for v in val_str.split(",") if v.strip()}
            mask |= df[col].isin(allowed)
        df = df[mask]
        logging.info(f"[GAMMA] KEEP rules filtered to {len(df)} rows")
    if disallow_rules:
        for col, val_str in disallow_rules:
            if col not in df.columns:
                logging.warning(f"[GAMMA DISALLOW] Column '{col}' not found.")
                continue
            not_allowed = {v.strip() for v in val_str.split(",") if v.strip()}
            df = df[~df[col].isin(not_allowed)]
        logging.info(f"[GAMMA] DISALLOW rules applied; {len(df)} rows remain")
    return df

def process_gamma_data(zip_path: Path,
                       keep_rules: List[Tuple[str, str]],
                       disallow_rules: List[Tuple[str, str]],
                       bad_dimensions: List[str],
                       bad_attributes: List[str],
                       dimension_renames: Dict[str, str],
                       attribute_renames: Dict[str, str],
                       delimiter: str = ",",
                       remove_substring: str = "_ceaster.txt",
                       encoding: str = "utf-8") -> pd.DataFrame:
    logging.info(f"[GAMMA] Processing data from {zip_path}")
    all_dfs = []
    try:
        with zipfile.ZipFile(zip_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
            if not txt_files:
                logging.warning("[GAMMA] No .txt files found in the ZIP.")
                return pd.DataFrame()
            for txt_file in txt_files:
                try:
                    base_name = os.path.basename(txt_file)
                    if remove_substring in base_name:
                        base_name = base_name.replace(remove_substring, "")
                    else:
                        base_name, _ = os.path.splitext(base_name)
                    dimension = base_name.replace("_", " ").strip()
                    with z.open(txt_file) as fo:
                        df = pd.read_csv(fo, delimiter=delimiter, encoding=encoding).copy(deep=True)
                    if df.empty:
                        logging.warning(f"[GAMMA] '{txt_file}' is empty; skipping.")
                        continue
                    first_col = df.columns[0]
                    df.rename(columns={first_col: "Name"}, inplace=True)
                    df["Name"] = df["Name"].fillna("Unknown").astype(str)
                    df = filter_gamma_rules(df, keep_rules, disallow_rules)
                    df["Dimension"] = dimension
                    df["RecordID"] = df.index.astype(str)
                    id_vars = ["Dimension", "RecordID"]
                    value_vars = [col for col in df.columns if col not in id_vars]
                    df_melted = df.melt(id_vars=id_vars, value_vars=value_vars,
                                        var_name="Attribute", value_name="Value")
                    if dimension_renames:
                        df_melted["Dimension"] = df_melted["Dimension"].replace(dimension_renames)
                    if attribute_renames:
                        df_melted["Attribute"] = df_melted["Attribute"].replace(attribute_renames)
                    if bad_dimensions:
                        before = len(df_melted)
                        df_melted = df_melted[~df_melted["Dimension"].isin(bad_dimensions)]
                        logging.info(f"[GAMMA] Excluded {before - len(df_melted)} rows by dimensions")
                    if bad_attributes:
                        before = len(df_melted)
                        df_melted = df_melted[~df_melted["Attribute"].isin(bad_attributes)]
                        logging.info(f"[GAMMA] Excluded {before - len(df_melted)} rows by attributes")
                    ref_df = df_melted[df_melted["Attribute"] == "Name"][["RecordID", "Value"]].drop_duplicates("RecordID")
                    ref_df.rename(columns={"Value": "RefName"}, inplace=True)
                    df_final = df_melted.merge(ref_df, on="RecordID", how="left")
                    for col in ["Dimension", "Attribute", "Value", "RefName"]:
                        df_final[col] = df_final[col].fillna("").astype(str)
                    df_final["GroupKey"] = df_final["Dimension"].str.strip() + " | " + df_final["RefName"].str.strip()
                    df_final["Name"] = df_final["RefName"]
                    df_final["Key"] = (df_final["Dimension"].str.strip() + " | " +
                                       df_final["Name"].str.strip() + " | " +
                                       df_final["Attribute"].str.strip() + " | " +
                                       df_final["Value"].str.strip())
                    df_final.drop_duplicates(inplace=True)
                    logging.info(f"[GAMMA] Processed '{txt_file}' with {len(df_final)} final rows.")
                    all_dfs.append(df_final.copy(deep=True))
                except Exception as e2:
                    logging.error(f"[GAMMA] Error processing '{txt_file}': {e2}")
                    continue
            if all_dfs:
                df_combined = pd.concat(all_dfs, ignore_index=True)
                logging.info(f"[GAMMA] Combined data has {len(df_combined)} rows.")
                return df_combined
            else:
                logging.warning("[GAMMA] No valid data found; returning empty DataFrame.")
                return pd.DataFrame()
    except Exception as e:
        logging.exception(f"[GAMMA] Error reading ZIP file: {e}")
        return pd.DataFrame()

# =============================================================================
# RECONCILIATION LOGIC
# =============================================================================

def build_lookup_dict(df: pd.DataFrame) -> Dict[str, Dict[str, str]]:
    lookup = {}
    for group_key, group_df in df.groupby("GroupKey"):
        lookup[group_key] = dict(zip(group_df["Attribute"], group_df["Value"]))
    return lookup

def compare_mode_1(dimension: str, group_key: str, 
                   alfa_data: Dict[str, str], 
                   gamma_data: Dict[str, str]) -> List[Dict[str, str]]:
    differences = []
    all_attrs = set(alfa_data.keys()) | set(gamma_data.keys())
    for attr in all_attrs:
        a_val = alfa_data.get(attr, "").strip()
        g_val = gamma_data.get(attr, "").strip()
        if a_val != g_val:
            if not a_val and g_val:
                differences.append({
                    "Dimension": dimension,
                    "Name": gamma_data.get("Name", "").strip(),
                    "Attribute": attr,
                    "Value": g_val,
                    "Missing In": "ALFA"
                })
            elif a_val and not g_val:
                differences.append({
                    "Dimension": dimension,
                    "Name": alfa_data.get("Name", "").strip(),
                    "Attribute": attr,
                    "Value": a_val,
                    "Missing In": "GAMMA"
                })
            else:
                differences.extend([
                    {"Dimension": dimension, "Name": alfa_data.get("Name", "").strip(), "Attribute": attr, "Value": a_val, "Missing In": "GAMMA"},
                    {"Dimension": dimension, "Name": alfa_data.get("Name", "").strip(), "Attribute": attr, "Value": g_val, "Missing In": "ALFA"}
                ])
    return differences

def compare_mode_2(dimension: str, group_key: str,
                   alfa_data: Dict[str, str],
                   gamma_data: Dict[str, str]) -> List[Dict[str, str]]:
    differences = []
    a_name = alfa_data.get("Name", "").strip()
    g_name = gamma_data.get("Name", "").strip()
    if a_name and g_name:
        if a_name != g_name:
            differences.extend([
                {"Dimension": dimension, "Name": a_name, "Attribute": "Name", "Value": a_name, "Missing In": "GAMMA"},
                {"Dimension": dimension, "Name": g_name, "Attribute": "Name", "Value": g_name, "Missing In": "ALFA"}
            ])
        else:
            for attr in set(alfa_data.keys()) | set(gamma_data.keys()):
                if attr == "Name":
                    continue
                a_val = alfa_data.get(attr, "").strip()
                g_val = gamma_data.get(attr, "").strip()
                if a_val != g_val:
                    if not a_val and g_val:
                        differences.append({"Dimension": dimension, "Name": a_name, "Attribute": attr, "Value": g_val, "Missing In": "ALFA"})
                    elif a_val and not g_val:
                        differences.append({"Dimension": dimension, "Name": a_name, "Attribute": attr, "Value": a_val, "Missing In": "GAMMA"})
                    else:
                        differences.extend([
                            {"Dimension": dimension, "Name": a_name, "Attribute": attr, "Value": a_val, "Missing In": "GAMMA"},
                            {"Dimension": dimension, "Name": a_name, "Attribute": attr, "Value": g_val, "Missing In": "ALFA"}
                        ])
    else:
        if not a_name and g_name:
            differences.append({"Dimension": dimension, "Name": g_name, "Attribute": "Name", "Value": g_name, "Missing In": "ALFA"})
        elif a_name and not g_name:
            differences.append({"Dimension": dimension, "Name": a_name, "Attribute": "Name", "Value": a_name, "Missing In": "GAMMA"})
        else:
            differences.append({"Dimension": dimension, "Name": "", "Attribute": "Name", "Value": "", "Missing In": "Both"})
    return differences

def compare_mode_3(dimension: str, group_key: str,
                   alfa_data: Dict[str, str],
                   gamma_data: Dict[str, str]) -> List[Dict[str, str]]:
    differences = []
    all_attrs = set(alfa_data.keys()) | set(gamma_data.keys())
    for attr in all_attrs:
        a_val = alfa_data.get(attr, "").strip()
        g_val = gamma_data.get(attr, "").strip()
        if a_val == g_val:
            continue
        if not a_val and g_val:
            differences.append({"Dimension": dimension, "Name": alfa_data.get("Name", "").strip(), "Attribute": attr, "Value": g_val, "Missing In": "ALFA"})
        elif a_val and not g_val:
            differences.append({"Dimension": dimension, "Name": alfa_data.get("Name", "").strip(), "Attribute": attr, "Value": a_val, "Missing In": "GAMMA"})
        else:
            differences.extend([
                {"Dimension": dimension, "Name": alfa_data.get("Name", "").strip(), "Attribute": attr, "Value": a_val, "Missing In": "GAMMA"},
                {"Dimension": dimension, "Name": alfa_data.get("Name", "").strip(), "Attribute": attr, "Value": g_val, "Missing In": "ALFA"}
            ])
    return differences

def compare_data(df_alfa: pd.DataFrame,
                 df_gamma: pd.DataFrame,
                 comparison_mode: int = 2) -> pd.DataFrame:
    logging.info(f"Starting data comparison (Mode {comparison_mode})")
    alfa_lookup = build_lookup_dict(df_alfa)
    gamma_lookup = build_lookup_dict(df_gamma)
    all_keys = set(alfa_lookup.keys()) | set(gamma_lookup.keys())
    
    differences = []
    for key in all_keys:
        parts = key.split(" | ")
        dimension = parts[0] if parts else ""
        a_data = alfa_lookup.get(key, {})
        g_data = gamma_lookup.get(key, {})
        if comparison_mode == 1:
            differences.extend(compare_mode_1(dimension, key, a_data, g_data))
        elif comparison_mode == 2:
            differences.extend(compare_mode_2(dimension, key, a_data, g_data))
        elif comparison_mode == 3:
            differences.extend(compare_mode_3(dimension, key, a_data, g_data))
    df_diff = pd.DataFrame(differences)
    # Create composite Key column for merging with exception table
    if not df_diff.empty:
        df_diff["Key"] = (df_diff["Dimension"].str.strip() + " | " +
                          df_diff["Name"].str.strip() + " | " +
                          df_diff["Attribute"].str.strip() + " | " +
                          df_diff["Value"].str.strip())
    logging.info(f"Comparison complete: {len(df_diff)} differences found")
    return df_diff

# =============================================================================
# EXCEPTION TABLE FUNCTIONS
# =============================================================================

def read_exception_table(exc_path: Path) -> pd.DataFrame:
    if not exc_path or not exc_path.is_file():
        logging.warning(f"[Exception] Exception file not found: {exc_path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(exc_path, sheet_name="Sheet1")
        return df.copy(deep=True)
    except Exception as e:
        logging.exception(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame()

def merge_exceptions(df_diff: pd.DataFrame, df_exceptions: pd.DataFrame) -> pd.DataFrame:
    if df_exceptions.empty:
        return df_diff
    # Only consider the following columns from the exception table
    exc_cols = [col for col in df_exceptions.columns if col in {"Key", "Comments_1", "Comments_2", "hide exception"}]
    exc = df_exceptions[exc_cols].copy()
    exc["Key"] = exc["Key"].astype(str).str.strip()
    df_diff = df_diff.merge(exc, on="Key", how="left", suffixes=("", "_exc"))
    df_diff["hide exception"] = df_diff.get("hide exception", "no").fillna("no").str.lower()
    before_len = len(df_diff)
    df_diff = df_diff[df_diff["hide exception"] != "yes"]
    logging.debug(f"[Excel] Excluded {before_len - len(df_diff)} rows due to hidden exception.")
    return df_diff

# =============================================================================
# CUSTOM SCROLLABLE FRAME (Modern)
# =============================================================================

class ModernScrollableFrame(ctk.CTkFrame):
    def __init__(self, container, *args, **kwargs):
        super().__init__(container, *args, **kwargs)
        self.canvas = ctk.CTkCanvas(self)
        self.scrollable_frame = ctk.CTkFrame(self.canvas)
        self.vsb = ctk.CTkScrollbar(self, orientation="vertical", command=self.canvas.yview)
        self.hsb = ctk.CTkScrollbar(self, orientation="horizontal", command=self.canvas.xview)
        self.canvas.configure(yscrollcommand=self.vsb.set, xscrollcommand=self.hsb.set)
        self.canvas.grid(row=0, column=0, sticky="nsew")
        self.vsb.grid(row=0, column=1, sticky="ns")
        self.hsb.grid(row=1, column=0, sticky="ew")
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw")
        self.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.grid_rowconfigure(0, weight=1)
        self.grid_columnconfigure(0, weight=1)
        self.canvas.bind_all("<MouseWheel>", self._on_mousewheel)
    def _on_mousewheel(self, event):
        self.canvas.yview_scroll(int(-1*(event.delta/120)), "units")

# =============================================================================
# MAIN GUI APPLICATION
# =============================================================================

class ReconciliationGUI(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Data Reconciliation Tool")
        self.geometry(UI_CONFIG["WINDOW_SIZE"])
        self.comparison_mode = tk.IntVar(value=2)
        self.current_config = {}
        self.df_differences = None
        self.setup_tabs()
        self.setup_logging()
        self.load_saved_config()
        self.log_text = ctk.CTkTextbox(self, height=150, font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"]))
        self.log_text.configure(state="disabled")
        self.log_text.pack(fill="both", padx=10, pady=(0,10))
        setup_logging(log_widget=self.log_text)

    def setup_tabs(self):
        self.tab_view = ctk.CTkTabview(self)
        self.tab_view.pack(fill="both", expand=True, padx=10, pady=10)
        self.tab_view.add("Settings")
        self.tab_view.add("Rules")
        self.tab_view.add("Process")
        self.tab_view.add("Results")
        self.build_settings_tab()
        self.build_rules_tab()
        self.build_process_tab()
        self.build_results_tab()

    def build_settings_tab(self):
        tab = self.tab_view.tab("Settings")
        paths_frame = ctk.CTkFrame(tab)
        paths_frame.pack(fill="x", padx=10, pady=5)
        self.path_entries = {}
        for key in ["ALFA_PATH", "GAMMA_PATH", "EXCEPTION_PATH", "OUTPUT_PATH"]:
            frame = ctk.CTkFrame(paths_frame)
            frame.pack(fill="x", pady=5)
            label = ctk.CTkLabel(frame, text=f"{key.replace('_', ' ')}:", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"]))
            label.pack(side="left", padx=5)
            entry = ctk.CTkEntry(frame, width=400, font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"]))
            entry.insert(0, DEFAULT_PATHS.get(key, ""))
            entry.pack(side="left", padx=5)
            btn = ctk.CTkButton(frame, text="Browse", command=lambda k=key: self.browse_file(k))
            btn.pack(side="left", padx=5)
            self.path_entries[key] = entry

    def build_rules_tab(self):
        tab = self.tab_view.tab("Rules")
        scroll_frame = ModernScrollableFrame(tab)
        scroll_frame.pack(fill="both", expand=True, padx=10, pady=5)
        lbl_dim = ctk.CTkLabel(scroll_frame.scrollable_frame, text="Dimension Rules (to be implemented)", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"]))
        lbl_dim.pack(pady=5)
        lbl_attr = ctk.CTkLabel(scroll_frame.scrollable_frame, text="Attribute Rules (to be implemented)", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"]))
        lbl_attr.pack(pady=5)
        lbl_filter = ctk.CTkLabel(scroll_frame.scrollable_frame, text="Filter Rules (to be implemented)", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"]))
        lbl_filter.pack(pady=5)

    def build_process_tab(self):
        tab = self.tab_view.tab("Process")
        mode_frame = ctk.CTkFrame(tab)
        mode_frame.pack(fill="x", padx=10, pady=5)
        ctk.CTkLabel(mode_frame, text="Comparison Mode:", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"])).pack(pady=5)
        modes = [("All Missing", 1), ("Missing - Name Special", 2), ("Full Comparison", 3)]
        for text, value in modes:
            ctk.CTkRadioButton(mode_frame, text=text, variable=self.comparison_mode, value=value,
                               font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"])).pack(pady=2)
        progress_frame = ctk.CTkFrame(tab)
        progress_frame.pack(fill="x", padx=10, pady=10)
        self.progress_bar = ctk.CTkProgressBar(progress_frame)
        self.progress_bar.pack(fill="x", padx=20, pady=5)
        self.progress_bar.set(0)
        self.progress_label = ctk.CTkLabel(progress_frame, text="Ready to process", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"]))
        self.progress_label.pack(pady=5)
        button_frame = ctk.CTkFrame(tab)
        button_frame.pack(fill="x", padx=10, pady=5)
        ctk.CTkButton(button_frame, text="Run Reconciliation", command=self.run_reconciliation,
                      font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"])).pack(side="left", padx=5)
        ctk.CTkButton(button_frame, text="Save Results", command=self.save_results,
                      font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["NORMAL"])).pack(side="left", padx=5)

    def build_results_tab(self):
        tab = self.tab_view.tab("Results")
        self.chart_frame = ModernScrollableFrame(tab)
        self.chart_frame.pack(fill="both", expand=True, padx=10, pady=5)
        self.chart_containers = {
            'dimension': ctk.CTkFrame(self.chart_frame.scrollable_frame),
            'attribute': ctk.CTkFrame(self.chart_frame.scrollable_frame),
            'missing': ctk.CTkFrame(self.chart_frame.scrollable_frame)
        }
        for container in self.chart_containers.values():
            container.pack(fill="x", pady=10)

    def run_reconciliation(self):
        try:
            self.progress_bar.set(0.1)
            self.progress_label.configure(text="Loading ALFA data...")
            self.update()
            df_alfa = process_alfa_data(
                Path(self.path_entries["ALFA_PATH"].get()),
                FILTER_RULES.get("ALFA_KEEP_RULES", []),
                FILTER_RULES.get("ALFA_DONOTKEEP_RULES", []),
                DIMENSION_CONFIG.get("ALFA_BAD_DIMS", []),
                ATTRIBUTE_CONFIG.get("ALFA_BAD_ATTRS", []),
                DIMENSION_CONFIG.get("ALFA_DIM_RENAMES", {}),
                ATTRIBUTE_CONFIG.get("ALFA_ATTR_RENAMES", {})
            )
            self.progress_bar.set(0.3)
            self.progress_label.configure(text="Loading GAMMA data...")
            self.update()
            df_gamma = process_gamma_data(
                Path(self.path_entries["GAMMA_PATH"].get()),
                FILTER_RULES.get("GAMMA_KEEP_RULES", []),
                FILTER_RULES.get("GAMMA_DONOTKEEP_RULES", []),
                DIMENSION_CONFIG.get("GAMMA_BAD_DIMS", []),
                ATTRIBUTE_CONFIG.get("GAMMA_BAD_ATTRS", []),
                DIMENSION_CONFIG.get("GAMMA_DIM_RENAMES", {}),
                ATTRIBUTE_CONFIG.get("GAMMA_ATTR_RENAMES", {})
            )
            self.progress_bar.set(0.6)
            self.progress_label.configure(text="Comparing data...")
            self.update()
            df_diff = compare_data(df_alfa, df_gamma, self.comparison_mode.get())
            # Ensure the composite Key column exists
            if not df_diff.empty:
                df_diff["Key"] = (df_diff["Dimension"].str.strip() + " | " +
                                  df_diff["Name"].str.strip() + " | " +
                                  df_diff["Attribute"].str.strip() + " | " +
                                  df_diff["Value"].str.strip())
            # Load and merge exceptions (if provided)
            exc_path = Path(self.path_entries["EXCEPTION_PATH"].get())
            df_exceptions = read_exception_table(exc_path) if exc_path.exists() else pd.DataFrame()
            df_diff = merge_exceptions(df_diff, df_exceptions)
            # Ensure Action Item column exists
            if "Action Item" not in df_diff.columns:
                df_diff["Action Item"] = ""
            self.df_differences = df_diff
            self.progress_bar.set(0.8)
            self.progress_label.configure(text="Generating visualizations...")
            self.update()
            self.create_visualizations()
            self.progress_bar.set(1.0)
            self.progress_label.configure(text="Processing complete!")
            self.update()
            self.tab_view.set("Results")
        except Exception as e:
            logging.exception("Error during reconciliation")
            messagebox.showerror("Error", f"An error occurred: {str(e)}")
            self.progress_label.configure(text="Error occurred during processing")

    def create_visualizations(self):
        if self.df_differences is None or self.df_differences.empty:
            logging.warning("No differences to visualize")
            return
        for container in self.chart_containers.values():
            for widget in container.winfo_children():
                widget.destroy()
        # Differences by Dimension
        dim_counts = self.df_differences["Dimension"].value_counts()
        self.create_bar_chart(self.chart_containers['dimension'], dim_counts, "Differences by Dimension", "Dimension", "Count")
        # Differences by Attribute
        attr_counts = self.df_differences["Attribute"].value_counts()
        self.create_bar_chart(self.chart_containers['attribute'], attr_counts, "Differences by Attribute", "Attribute", "Count")
        # Differences by Source (Missing In)
        missing_counts = self.df_differences["Missing In"].value_counts()
        self.create_bar_chart(self.chart_containers['missing'], missing_counts, "Differences by Source", "Source", "Count")

    def create_bar_chart(self, container, data, title, xlabel, ylabel):
        fig = Figure(figsize=(12, 6))
        ax = fig.add_subplot(111)
        bars = ax.bar(range(len(data)), data.values, color=UI_CONFIG["COLORS"]["PRIMARY"])
        ax.set_xticks(range(len(data)))
        ax.set_xticklabels(data.index, rotation=45, ha='right')
        ax.set_title(title, pad=20)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height, f'{int(height):,}', ha='center', va='bottom')
        canvas = FigureCanvasTkAgg(fig, master=container)
        canvas.draw()
        mplcursors.cursor(bars, hover=True)
        canvas.get_tk_widget().pack(fill="both", expand=True)

    def save_results(self):
        if self.df_differences is None or self.df_differences.empty:
            messagebox.showwarning("Warning", "No results to save")
            return
        try:
            output_path = Path(self.path_entries["OUTPUT_PATH"].get())
            output_path.parent.mkdir(parents=True, exist_ok=True)
            # Ensure final column order
            final_cols = ["Key", "Dimension", "Name", "Attribute", "Value", "Comments_1", "Comments_2", "Action Item", "Missing In"]
            df_out = self.df_differences.copy()
            for col in final_cols:
                if col not in df_out.columns:
                    df_out[col] = ""
            df_out = df_out[final_cols]
            df_out.to_excel(output_path, index=False)
            self.format_excel_output(output_path)
            messagebox.showinfo("Success", f"Results saved to {output_path}")
        except Exception as e:
            logging.exception("Error saving results")
            messagebox.showerror("Error", f"Error saving results: {str(e)}")

    def format_excel_output(self, path: Path):
        try:
            wb = load_workbook(path)
            ws = wb.active
            header_font = Font(name=UI_CONFIG["FONT_FAMILY"], size=UI_CONFIG["FONT_SIZES"]["NORMAL"], bold=True)
            data_font = Font(name=UI_CONFIG["FONT_FAMILY"], size=UI_CONFIG["FONT_SIZES"]["NORMAL"])
            header_fill = PatternFill(start_color="E0E0E0", end_color="E0E0E0", fill_type="solid")
            for cell in ws[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center")
            for row in ws.iter_rows(min_row=2):
                for cell in row:
                    cell.font = data_font
                    cell.alignment = Alignment(horizontal="left")
            for column in ws.columns:
                max_length = 0
                column = list(column)
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except Exception:
                        pass
                ws.column_dimensions[column[0].column_letter].width = max_length + 2
            ws.freeze_panes = "A2"
            wb.save(path)
        except Exception as e:
            logging.exception("Error formatting Excel output")
            raise

    def browse_file(self, key: str):
        initial_dir = os.path.dirname(self.path_entries[key].get())
        file_path = filedialog.askopenfilename(initialdir=initial_dir)
        if file_path:
            self.path_entries[key].delete(0, tk.END)
            self.path_entries[key].insert(0, file_path)

    def load_saved_config(self):
        config_path = Path(DEFAULT_PATHS["CONFIG_PATH"])
        config = load_config(config_path)
        if config:
            self.current_config = config
            logging.info("Configuration loaded from file")
        else:
            logging.info("No configuration file found; using defaults")

    def setup_logging(self):
        setup_logging()

    # Stub methods for future rule UI implementations
    def create_dimension_rules(self, container):
        lbl = ctk.CTkLabel(container, text="Dimension Rules (to be implemented)", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"]))
        lbl.pack(pady=5)

    def create_attribute_rules(self, container):
        lbl = ctk.CTkLabel(container, text="Attribute Rules (to be implemented)", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"]))
        lbl.pack(pady=5)

    def create_filter_rules(self, container):
        lbl = ctk.CTkLabel(container, text="Filter Rules (to be implemented)", font=(UI_CONFIG["FONT_FAMILY"], UI_CONFIG["FONT_SIZES"]["HEADER"]))
        lbl.pack(pady=5)

def main():
    try:
        app = ReconciliationGUI()
        app.mainloop()
    except Exception as e:
        logging.exception("Critical error in main application")
        messagebox.showerror("Critical Error", f"Application encountered a critical error: {str(e)}")

if __name__ == "__main__":
    main()
