# mamaliga bo 4
"""
Ultra-Mega Reconciliation (Updated Specs)
 - Auto-load all history JSON on startup
 - 6 user paths in the UI, others remain in code
 - Master & ERP previews:
     - No header-based filtering on Start/End Date
     - We automatically keep only End Date blank or > today
     - Dates forced to YYYY-MM-DD format if possible
     - NaN displayed as blank
 - Compare => missing_items.xlsx with 2 sheets:
     1) "Mismatch"
     2) "Case_Differences"
   Each row has:
       Key, Dimension, Name, Attribute, Master, ERP, Comments_1, Comments_2, Gap In
   'Gap In' = 'ERP', 'MASTER', 'BOTH' or 'CASE'
 - Case differences go to second sheet, real mismatches to the first
 - Dashboard uses only the first sheet (Mismatch). 'Missing In' renamed to 'Gap In'
 - 8-chart dashboard with Bollinger band from entire mismatch history
 - Bollinger data saved on close to a user-chosen JSON
 - Master .txt reading attempts multiple encodings (utf-8-sig, utf-16-le, utf-16-be, cp1252, latin-1, ascii)
"""

import os
import sys
import json
import math
import logging
import zipfile
import shutil
import io
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Set, List

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import customtkinter as ctk

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
from matplotlib.backends.backend_pdf import PdfPages

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------------------------------------------------------
# DEFAULT CONFIG & SAVE/LOAD
# ----------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/missing_items.xlsx",
    "PDF_EXPORT_PATH": "output/dashboard_report.pdf",
    "PARAMETER_PATH": "data/parameters.xlsx",

    # The following are used in code but NOT shown in the UI
    "CONFIG_PATH": "config/ui_config.json",
    "MASTER_CSV_OUTPUT": "temp_master_csv",
    "LOGO_PATH": "images/company_logo.png",
    "HISTORY_PATH": "history_runs",
    "BAND_CHART_JSON_PATH": "data/bollinger_data.json"
}

def default_config() -> Dict:
    return {
        "paths": DEFAULT_PATHS.copy(),
        "erp_grid": {"filters": {}},
        "master_grid": {"filters": {}},
        "dashboard": {
            "selected_dims": [],
            "selected_attrs": [],
            "top_n": 10
        }
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config => {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        # Convert sets->lists in erp_grid
        if "erp_grid" in cfg and "filters" in cfg["erp_grid"]:
            newf = {}
            for col, svals in cfg["erp_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["erp_grid"]["filters"] = newf

        # Convert sets->lists in master_grid
        if "master_grid" in cfg and "filters" in cfg["master_grid"]:
            newf = {}
            for col, svals in cfg["master_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["master_grid"]["filters"] = newf

        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config => {e}")


# ----------------------------------------------------------------------------
# TEXT LOGGER HANDLER
# ----------------------------------------------------------------------------
class TextHandler(logging.Handler):
    """Redirect logs to a ctk.CTkTextbox widget."""
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget
    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")


# ----------------------------------------------------------------------------
# READ PARAM
# ----------------------------------------------------------------------------
def read_param_file(path: Path) -> Dict[str, object]:
    """Reads the 'parameters.xlsx' which define dimension/attribute keep & map."""
    param = {
        "dim_erp_keep": set(),
        "dim_erp_map": {},
        "dim_master_map": {},
        "attr_erp_map": {},
        "attr_master_map": {}
    }
    if not path.is_file():
        logging.warning(f"Param file not found => {path}")
        return param
    try:
        dim_df = pd.read_excel(path, sheet_name="Dimension Parameters")
        dim_df.columns = dim_df.columns.astype(str).str.strip()

        def s(x): return str(x).strip() if pd.notna(x) else ""

        for _, row in dim_df.iterrows():
            fn  = s(row.get("FileName", ""))
            vsc = s(row.get("V S C", ""))
            dim = s(row.get("Dimension", ""))
            ev  = s(row.get("ERP Values", ""))
            if ev.lower() == "x" and vsc and dim:
                param["dim_erp_keep"].add(vsc)
            if vsc and dim:
                param["dim_erp_map"][vsc] = dim
            if fn and dim and ev.lower() == "x":
                param["dim_master_map"][fn] = dim

        attr_df = pd.read_excel(path, sheet_name="Attribute Parameters")
        attr_df.columns = attr_df.columns.astype(str).str.strip()
        for _, row in attr_df.iterrows():
            e_orig = s(row.get("ERP Original Attributes", ""))
            m_orig = s(row.get("Master Original Attributes", ""))
            final_ = s(row.get("Attribute", ""))
            onoff  = s(row.get("On/Off", ""))
            if onoff.lower() == "x" and final_:
                if e_orig:
                    param["attr_erp_map"][e_orig] = final_
                if m_orig:
                    param["attr_master_map"][m_orig] = final_
        return param
    except Exception as e:
        logging.error(f"Error reading param file => {e}")
        return param


# ----------------------------------------------------------------------------
# ERP
# ----------------------------------------------------------------------------
def read_erp_excel(path: Path) -> pd.DataFrame:
    """Reads the ERP Excel (skips top rows?)."""
    if not path.is_file():
        logging.warning(f"ERP Excel not found => {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path, skiprows=3)
        df.columns = df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df = df[df["Enabled_Flag"] == "Enabled"]
        return df
    except Exception as e:
        logging.error(f"Error reading ERP => {e}")
        return pd.DataFrame()


# ----------------------------------------------------------------------------
# MASTER
# ----------------------------------------------------------------------------
def try_read_csv_bytes(raw: bytes) -> pd.DataFrame:
    """
    Attempt multiple encodings for the raw .txt data. Return a DataFrame or empty on fail.
    We remove fully-empty rows/columns. We rename first col to 'Name' if no 'Name' present.
    """
    encodings = [
       "utf-8-sig",
       "utf-16-le",
       "utf-16-be",
       "cp1252",
       "latin-1",
       "ascii"
    ]
    errors_accum = []
    for enc in encodings:
        try:
            buf = io.BytesIO(raw)
            df = pd.read_csv(buf, encoding=enc, on_bad_lines="skip", engine="python")
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            df.columns = df.columns.astype(str).str.strip()

            # If there's no 'Name' column, rename the first column to 'Name'.
            if "Name" not in df.columns and len(df.columns) > 0:
                first_col = df.columns[0]
                df.rename(columns={first_col: "Name"}, inplace=True)

            return df
        except Exception as e:
            errors_accum.append(f"({enc} => {e})")

    # If we get here, all encodings failed
    logging.error(f"Failed all encodings: {errors_accum}")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path):
    """Unzips .txt files, tries multiple encodings, and writes .csv for each successfully read .txt."""
    if not zip_path.is_file():
        logging.warning(f"[Master] ZIP not found => {zip_path}")
        return []
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)

    csvs = []
    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name = os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw = fo.read()
                df = try_read_csv_bytes(raw)
                if df.empty:
                    continue
                df["RawFileName"] = base_name
                out_csv = out_dir / (base_name.replace(".txt", ".csv"))
                df.to_csv(out_csv, index=False, encoding="utf-8")
                csvs.append(out_csv)
            except Exception as e:
                logging.error(f"[Master] error reading {txt_file} => {e}")
    return csvs

def unify_master_csvs(csvs: List[Path]) -> pd.DataFrame:
    frames = []
    for cp in csvs:
        if not cp.is_file():
            continue
        try:
            df = pd.read_csv(cp, encoding="utf-8", on_bad_lines="skip")
            df.columns = df.columns.str.strip()
            frames.append(df)
        except Exception as e:
            logging.error(f"[unify_master_csvs] reading {cp} => {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()


# ----------------------------------------------------------------------------
# HELPER: Clean up date columns
# ----------------------------------------------------------------------------
def to_yyyy_mm_dd(val) -> str:
    """Try to parse a date and return 'YYYY-MM-DD'. Otherwise return blank."""
    if pd.isna(val):
        return ""
    if isinstance(val, str):
        val = val.strip()
        if not val:
            return ""
    # Let pandas parse it
    try:
        dt = pd.to_datetime(val, errors="coerce")
        if pd.isna(dt):
            return ""
        return dt.strftime("%Y-%m-%d")
    except:
        return ""

def filter_end_date(df: pd.DataFrame) -> pd.DataFrame:
    """
    We keep rows whose 'End Date' is blank OR strictly after today.
    If there's no 'End Date' column, do nothing.
    """
    if "End Date" not in df.columns:
        return df
    today_str = datetime.now().strftime("%Y-%m-%d")
    today_dt = datetime.strptime(today_str, "%Y-%m-%d")

    def keep_func(x):
        if not x:  # blank
            return True
        try:
            dt = datetime.strptime(x, "%Y-%m-%d")
            return dt > today_dt
        except:
            # If it's some garbage => keep it, or skip? We'll skip it if we can't parse?
            return False

    return df[df["End Date"].apply(keep_func)]


# ----------------------------------------------------------------------------
# MELTDOWN
# ----------------------------------------------------------------------------
def meltdown_erp_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    """
    Filter rows by param['dim_erp_keep'], meltdown on attribute, rename dimension and attributes.
    Convert any 'Start Date','End Date' to 'YYYY-MM-DD'.  Then auto-filter out End Date <= today.
    """
    if "V_S_C" not in df.columns:
        return pd.DataFrame()
    keep = param.get("dim_erp_keep", set())
    dmap = param.get("dim_erp_map", {})
    amap = param.get("attr_erp_map", {})

    # Keep only relevant V_S_C
    df2 = df[df["V_S_C"].isin(keep)].copy()
    if df2.empty:
        return pd.DataFrame()

    skip_cols = {"V_S_C", "Enabled_Flag"}
    id_vars = []
    if "Value" in df2.columns:
        id_vars.append("Value")
        skip_cols.add("Value")
    df2["DimRaw"] = df2["V_S_C"]
    skip_cols.add("DimRaw")
    id_vars.insert(0, "DimRaw")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    # Map dimension
    def rename_dim(v):
        return dmap.get(v, v)
    melted["Dimension"] = melted["DimRaw"].apply(rename_dim)

    if "Value" in id_vars:
        melted.rename(columns={"Value": "Name"}, inplace=True)
    else:
        melted["Name"] = ""

    # Keep only relevant attributes
    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)

    # Convert known date attributes to "YYYY-MM-DD"
    def fix_dates(a, v):
        if a in ["Start Date", "End Date"]:
            return to_yyyy_mm_dd(v)
        else:
            return v if pd.notna(v) else ""
    melted["Value"] = [
        fix_dates(a, vx)
        for a, vx in zip(melted["Attribute"], melted["ValX"])
    ]

    # Pivot back
    dfp = melted[["Dimension","Name","Attribute","Value"]]
    # Filter out End Date <= today
    # We'll pivot to wide and do the filter_end_date
    if dfp.empty:
        return dfp

    # pivot for preview
    dfp = dfp.drop_duplicates(subset=["Dimension","Name","Attribute"])
    try:
        dfp_wide = dfp.pivot(
            index=["Dimension","Name"],
            columns="Attribute",
            values="Value"
        ).reset_index()
    except Exception as e:
        logging.error(f"Pivot error => {e}")
        return pd.DataFrame()

    # Now filter on End Date => blank or > today
    dfp_wide = filter_end_date(dfp_wide)

    return dfp_wide


def meltdown_master_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    """
    Filter rows by param['dim_master_map'] (matching RawFileName), meltdown on attribute,
    rename dimension and attributes, fix date columns, filter out EndDate <= today. 
    """
    if df.empty or "RawFileName" not in df.columns:
        return pd.DataFrame()
    keep_map = param.get("dim_master_map", {})
    amap = param.get("attr_master_map", {})

    df2 = df[df["RawFileName"].isin(keep_map.keys())].copy()
    if df2.empty:
        return pd.DataFrame()

    df2["DimRaw"] = df2["RawFileName"]
    skip_cols = {"RawFileName", "DimRaw"}
    id_vars = ["DimRaw"]
    if "Name" in df2.columns:
        id_vars.append("Name")
        skip_cols.add("Name")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    # rename dimension
    def rename_dim(fn):
        return keep_map.get(fn, fn)
    melted["Dimension"] = melted["DimRaw"].apply(rename_dim)

    if "Name" in id_vars:
        melted.rename(columns={"Name": "Name"}, inplace=True)
    else:
        melted["Name"] = ""

    # keep only relevant attributes
    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)

    # convert known dates
    def fix_dates(a, v):
        if a in ["Start Date","End Date"]:
            return to_yyyy_mm_dd(v)
        else:
            return v if pd.notna(v) else ""
    melted["Value"] = [
        fix_dates(a, vx)
        for a, vx in zip(melted["Attribute"], melted["ValX"])
    ]

    dfp = melted[["Dimension","Name","Attribute","Value"]]
    if dfp.empty:
        return dfp

    # pivot
    dfp = dfp.drop_duplicates(subset=["Dimension","Name","Attribute"])
    try:
        dfp_wide = dfp.pivot(
            index=["Dimension","Name"],
            columns="Attribute",
            values="Value"
        ).reset_index()
    except Exception as e:
        logging.error(f"Pivot error => {e}")
        return pd.DataFrame()

    # Filter out EndDate <= today
    dfp_wide = filter_end_date(dfp_wide)

    return dfp_wide


# ----------------------------------------------------------------------------
# 2-SHEET COMPARISON (Mismatch + Case Differences)
# ----------------------------------------------------------------------------

def read_exception_table(path: Path)-> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"Exception table not found => {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path)
        df.columns = df.columns.astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"Error reading exception => {e}")
        return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame)-> pd.DataFrame:
    """
    Merge with exception table on 'Key'. If 'hide exception'=='yes', drop that row.
    Overwrite Comments_1, Comments_2 if found in exception. 
    """
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep = [c for c in ["Key","Comments_1","Comments_2","hide exception"] if c in df_exc.columns]
    if not keep:
        return df
    exc= df_exc[keep].copy()
    exc["Key"]= exc["Key"].astype(str).str.strip()
    merged= df.merge(exc, on="Key", how="left", suffixes=("","_exc"))
    # hide?
    merged["hide exception"] = merged.get("hide exception","").fillna("").str.lower()
    final= merged[merged["hide exception"]!="yes"].copy()

    # overwrite comments if we have them
    if "Comments_1_exc" in final.columns:
        final["Comments_1"]= np.where(final["Comments_1_exc"].notna(),
                                      final["Comments_1_exc"],
                                      final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"]= np.where(final["Comments_2_exc"].notna(),
                                      final["Comments_2_exc"],
                                      final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)

    return final

def meltdown_to_long(df_wide: pd.DataFrame) -> pd.DataFrame:
    """Given a pivoted preview DataFrame (Dimension, Name, possibly date columns),
       meltdown so each row is (Dimension, Name, Attribute, Value).
    """
    if df_wide.empty or {"Dimension","Name"}.difference(df_wide.columns):
        return pd.DataFrame()
    meltdown_cols = [c for c in df_wide.columns if c not in ("Dimension","Name")]
    melted = df_wide.melt(
        id_vars=["Dimension","Name"],
        value_vars=meltdown_cols,
        var_name="Attribute",
        value_name="Value"
    )
    # Replace real NaN with blank
    melted["Value"] = melted["Value"].fillna("")
    return melted

def compare_2sheet(erp_long: pd.DataFrame, mast_long: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):
    """
    Return two DataFrames: mismatch_df, casediff_df
    Each with columns:
        Key, Dimension, Name, Attribute, Master, ERP, Comments_1, Comments_2, Gap In
    Where 'Gap In' can be MASTER / ERP / BOTH for the mismatch sheet,
          or 'CASE' for the case differences sheet.
    """
    # Convert to dict of dict for easy compare
    # outer key = (dim,name), inner key = attribute => value
    def to_dict(d):
        out={}
        for (dim,name), grp in d.groupby(["Dimension","Name"]):
            a_map = {}
            for _, row in grp.iterrows():
                attr = row["Attribute"].strip()
                val  = str(row["Value"]).strip()
                a_map[attr] = val
            out[(dim,name)] = a_map
        return out

    e_dict = to_dict(erp_long)
    m_dict = to_dict(mast_long)

    all_dimname = set(e_dict.keys()) | set(m_dict.keys())

    mismatch_records = []
    case_records = []

    for dm in all_dimname:
        dim, nam = dm
        e_attrs = e_dict.get(dm, {})
        m_attrs = m_dict.get(dm, {})

        all_attrs = set(e_attrs.keys()) | set(m_attrs.keys())
        for at in all_attrs:
            e_val = e_attrs.get(at, "")
            m_val = m_attrs.get(at, "")

            # Are they truly the same?
            if m_val == e_val:
                # Perfect match => no row
                continue

            # Check if ignoring case they match
            if m_val.lower() == e_val.lower() and m_val and e_val:
                # This is a case difference
                # Gap In = CASE
                key = f"{dim}|{nam}|{at}|{m_val}|{e_val}"
                key = key.upper()
                rec = {
                    "Key": key,
                    "Dimension": dim,
                    "Name": nam,
                    "Attribute": at,
                    "Master": m_val,
                    "ERP": e_val,
                    "Comments_1": "",
                    "Comments_2": "",
                    "Gap In": "CASE"
                }
                case_records.append(rec)
                continue

            # Otherwise it's a mismatch
            if not m_val and e_val:
                # Missing in Master
                gap_in = "MASTER"
                master_str = ""
                erp_str = e_val
            elif m_val and not e_val:
                # Missing in ERP
                gap_in = "ERP"
                master_str = m_val
                erp_str = ""
            else:
                # Both exist but differ
                gap_in = "BOTH"
                master_str = m_val
                erp_str = e_val

            key = f"{dim}|{nam}|{at}|{master_str}|{erp_str}"
            key = key.upper()
            rec = {
                "Key": key,
                "Dimension": dim,
                "Name": nam,
                "Attribute": at,
                "Master": master_str,
                "ERP": erp_str,
                "Comments_1": "",
                "Comments_2": "",
                "Gap In": gap_in
            }
            mismatch_records.append(rec)

    mismatch_df = pd.DataFrame(mismatch_records)
    case_df     = pd.DataFrame(case_records)
    return mismatch_df, case_df

def write_2sheet_excel(mismatch_df: pd.DataFrame,
                       case_df: pd.DataFrame,
                       out_path: Path):
    """
    Writes mismatch_df to first sheet (Mismatch),
    case_df to second sheet (Case_Differences).
    Both share columns: Key, Dimension, Name, Attribute, Master, ERP, Comments_1, Comments_2, Gap In
    Auto-resize columns & freeze header row.
    """
    if mismatch_df.empty and case_df.empty:
        logging.info("No mismatches or case diffs => skip writing excel.")
        return

    out_path.parent.mkdir(parents=True, exist_ok=True)

    columns = ["Key","Dimension","Name","Attribute","Master","ERP",
               "Comments_1","Comments_2","Gap In"]

    wb = Workbook()
    # 1) Mismatch
    ws_m = wb.active
    ws_m.title = "Mismatch"
    ws_m.append(columns)
    for rowvals in mismatch_df[columns].itertuples(index=False):
        ws_m.append(rowvals)

    # 2) Case Differences
    ws_c = wb.create_sheet("Case_Differences")
    ws_c.append(columns)
    for rowvals in case_df[columns].itertuples(index=False):
        ws_c.append(rowvals)

    # style the headers
    header_font = Font(bold=True)
    fill = PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")

    for sheet in [ws_m, ws_c]:
        for cell in sheet[1]:
            cell.font = header_font
            cell.fill = fill
            cell.alignment = Alignment(horizontal="center")

        # auto column width
        for col in sheet.columns:
            max_len= 0
            letter= col[0].column_letter
            for cell in col:
                val= str(cell.value) if cell.value else ""
                max_len= max(max_len, len(val))
            sheet.column_dimensions[letter].width= max_len+2

        sheet.freeze_panes= "A2"

    wb.save(out_path)
    logging.info(f"Missing items => {out_path}")


# ----------------------------------------------------------------------------
# SIMPLE PREVIEW (with no date filtering on headers; we just show data)
# ----------------------------------------------------------------------------
class SimplePreview(ctk.CTkFrame):
    """
    A simple table preview: no user filter popups on Start/End Date.
    We'll just show the data. We do allow a popup with the unique values
    but do not actually filter. (Or we can do nothing on heading click.)
    """
    def __init__(self, parent, name: str, filters_dict=None):
        super().__init__(parent)
        self.name= name
        self.df= pd.DataFrame()
        self.filters = {}  # Not used for date filtering in the new spec, but we keep around

        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        bar= ctk.CTkFrame(self, corner_radius=10, fg_color="#f0f0f0")
        bar.pack(fill="x", padx=5, pady=5)
        ctk.CTkLabel(
            bar, text=f"{self.name} Preview",
            fg_color="#800020", corner_radius=8,
            text_color="white",
            font=ctk.CTkFont(size=14, weight="bold")
        ).pack(side="left", padx=5)

    def create_table(self):
        container= ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree= ttk.Treeview(container, show="headings")
        vsb= ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb= ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")
        container.rowconfigure(0,weight=1)
        container.columnconfigure(0,weight=1)

    def create_statusbar(self):
        self.status_label= ctk.CTkLabel(self, text="0 rows", fg_color="#f0f0f0", text_color="black")
        self.status_label.pack(fill="x")

    def set_data(self, df: pd.DataFrame):
        """Set the DataFrame to preview (already date-filtered in meltdown)."""
        self.df= df.copy()
        # Replace actual np.nan with empty string so it shows up blank
        self.df = self.df.fillna("")
        self.refresh_table()

    def refresh_table(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if self.df.empty:
            self.tree["columns"]=[]
            self.status_label.configure(text="0 rows")
            return

        cols= list(self.df.columns)
        self.tree["columns"]= cols
        for c in cols:
            self.tree.heading(c, text=c, anchor="w")
            self.tree.column(c, anchor="w", width=150)

        for _, row in self.df.iterrows():
            rowvals= [row[c] for c in cols]
            self.tree.insert("", "end", values=rowvals)
        self.status_label.configure(text=f"{len(self.df)} rows")

    def get_filtered_df(self)-> pd.DataFrame:
        """
        The old code returned the subset that passed user filters, but
        now we do no header-based filtering on Start/End Date. 
        We'll just return the DF as is.
        """
        return self.df.copy()


# ----------------------------------------------------------------------------
# PDF REPORT
# ----------------------------------------------------------------------------
class EnhancedPDFReport:
    def __init__(self, df_current: pd.DataFrame, df_history: pd.DataFrame, config: Dict):
        """
        df_current = mismatch for the latest run
        df_history = all mismatch over time
        """
        self.df_current = df_current
        self.df_history = df_history
        self.config = config
        self.page_count = 0
        self.colors = {
            'primary': '#800020',
            'text': '#2C1810',
            'background': '#FFFFFF'
        }
        self.logo_path = self.config["paths"].get("LOGO_PATH","images/company_logo.png")

        self.PAGE_WIDTH = 8.5
        self.PAGE_HEIGHT= 11
        self.CHART_SCALE = 0.7

    def generate(self) -> Path:
        pdf_path= self._get_output_path()
        with PdfPages(pdf_path) as pdf:
            self._cover_page(pdf)
            self._summary_page(pdf)
            self._topdimsattrs_page(pdf)
            self._all_charts(pdf)
        logging.info(f"PDF => {pdf_path}")
        return pdf_path

    def _get_output_path(self)-> Path:
        stamp= datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_dir= Path("Reconciliation_pdf")
        pdf_dir.mkdir(parents=True, exist_ok=True)
        pdf_name= f"Reconciliationpdf_{stamp}.pdf"
        pdf_path= pdf_dir / pdf_name
        return pdf_path

    def _new_page(self)-> plt.Figure:
        self.page_count += 1
        fig= plt.figure(figsize=(self.PAGE_WIDTH,self.PAGE_HEIGHT))
        fig.patch.set_facecolor(self.colors['background'])
        plt.axis('off')
        if self.logo_path and os.path.exists(self.logo_path):
            try:
                import matplotlib.image as mpimg
                img= mpimg.imread(self.logo_path)
                ax_img= fig.add_axes([0.65,0.65, 0.25, 0.25], anchor='NE', zorder=10)
                ax_img.imshow(img, alpha=0.2)
                ax_img.axis('off')
            except Exception as e:
                logging.error(f"Logo => {e}")

        fig.text(0.5, 0.97, "Reconciliation Report", ha='center', fontsize=10, color='gray')
        fig.text(0.9, 0.03, f"Page {self.page_count}", ha='right', fontsize=8, color='gray')
        fig.text(0.5, 0.02, "© Ultra-Mega Reconciliation", ha='center', fontsize=8, color='gray')
        return fig

    def _cover_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.7, "Reconciliation Analysis Report",
                 ha='center', fontsize=24, fontweight='bold', color=self.colors['primary'],
                 transform=fig.transFigure)
        plt.text(0.5, 0.6, f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                 ha='center', fontsize=12, color=self.colors['text'], transform=fig.transFigure)
        plt.text(0.5, 0.1, "CONFIDENTIAL", ha='center', fontsize=9, color=self.colors['text'],
                 transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _summary_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.92, "Reconciliation Summary", ha='center', fontsize=18, fontweight='bold',
                 color=self.colors['primary'], transform=fig.transFigure)
        y= 0.75
        if self.df_current.empty:
            plt.text(0.5, y, "No mismatches found this run.",
                     ha='center', fontsize=14, color=self.colors['text'], transform=fig.transFigure)
        else:
            total= len(self.df_current)
            c_master = (self.df_current["Gap In"]=="MASTER").sum()
            c_erp    = (self.df_current["Gap In"]=="ERP").sum()
            c_both   = (self.df_current["Gap In"]=="BOTH").sum()
            summary= (
                f"Total Mismatches: {total}\n"
                f"Gap In MASTER: {c_master}\n"
                f"Gap In ERP: {c_erp}\n"
                f"Gap In BOTH: {c_both}"
            )
            plt.text(0.5, y, summary, ha='center', fontsize=14, color=self.colors['text'],
                     transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _topdimsattrs_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.92, "Top Dimensions & Attributes", ha='center', fontsize=18, fontweight='bold',
                 color=self.colors['primary'], transform=fig.transFigure)
        if self.df_current.empty:
            plt.text(0.5, 0.7, "No data available.", ha='center', fontsize=12, color=self.colors['text'],
                     transform=fig.transFigure)
        else:
            if "Dimension" in self.df_current.columns:
                dims= self.df_current["Dimension"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in dims.items()]
                dim_txt= "Top Dimensions:\n" + "\n".join(lines)
                plt.text(0.2, 0.7, dim_txt, fontsize=12, color=self.colors['text'],
                         transform=fig.transFigure)
            if "Attribute" in self.df_current.columns:
                attrs= self.df_current["Attribute"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in attrs.items()]
                attr_txt= "Top Attributes:\n" + "\n".join(lines)
                plt.text(0.6, 0.7, attr_txt, fontsize=12, color=self.colors['text'],
                         transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _chart_page(self, pdf: PdfPages, title: str, plot_func, **kwargs):
        fig= self._new_page()
        fig.suptitle(title, fontsize=14, fontweight='bold', color=self.colors['primary'], y=0.93)

        w_inch= self.PAGE_WIDTH * self.CHART_SCALE
        # Just place the axes in the center
        if "Heatmap" in title:
            h_inch= w_inch * 1.4
            left_margin= (self.PAGE_WIDTH - w_inch)*0.5 + 0.4
        else:
            h_inch= w_inch * (9.0/16.0)
            left_margin= (self.PAGE_WIDTH - w_inch)*0.5 + 0.2

        bottom_margin= (self.PAGE_HEIGHT - h_inch)*0.5
        left_rel= left_margin / self.PAGE_WIDTH
        bottom_rel= bottom_margin / self.PAGE_HEIGHT
        width_rel= w_inch / self.PAGE_WIDTH
        height_rel= h_inch / self.PAGE_HEIGHT

        ax_rect= [left_rel, bottom_rel, width_rel, height_rel]
        ax= fig.add_axes(ax_rect)

        try:
            plot_func(ax, **kwargs)
            pdf.savefig(fig)
        except Exception as e:
            logging.error(f"{title} => {e}")
        finally:
            plt.close(fig)

    def _all_charts(self, pdf: PdfPages):
        dfc= self.df_current.copy()
        if dfc.empty:
            return

        df_m= dfc[dfc["Gap In"]!=""]  # basically all mismatch rows

        # Heatmap
        if not df_m.empty and {"Dimension","Attribute"}.issubset(df_m.columns):
            pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
            if not pivot.empty:
                self._chart_page(pdf, "Heatmap", self._plot_heatmap, pivot=pivot)

        # Lollipop
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim.empty:
            self._chart_page(pdf, "Lollipop", self._plot_lollipop, cdim=cdim)

        # Circular
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr.empty:
            self._chart_page(pdf, "Circular", self._plot_circular, cattr=cattr)

        # Scatter
        cdim_sc= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim_sc.sort_values("Count", ascending=False, inplace=True)
        cdim_sc= cdim_sc.head(10)
        if not cdim_sc.empty:
            self._chart_page(pdf, "Scatter", self._plot_scatter, cdim=cdim_sc)

        # Radar
        cdim_ra= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim_ra.empty and len(cdim_ra)>1:
            self._chart_page(pdf, "Radar", self._plot_radar, cdim=cdim_ra)

        # Pie
        dist= df_m["Gap In"].value_counts()
        if not dist.empty:
            self._chart_page(pdf, "Pie: Gap In distribution", self._plot_pie, dist=dist)

        # Bar
        cattr_b= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr_b.empty:
            self._chart_page(pdf, "Bar: Mismatch attributes", self._plot_bar, cattr=cattr_b)

        # Bollinger
        if not self.df_history.empty and "RunDate" in self.df_history.columns:
            date_ct= self.df_history.groupby("RunDate")["Key"].count().reset_index(name="Count")
            date_ct.sort_values("RunDate", inplace=True)
            if not date_ct.empty:
                self._chart_page(pdf, "Bollinger Band Over Time", self._plot_bollinger, date_ct=date_ct)

    # Plot helpers
    def _plot_heatmap(self, ax, pivot):
        im= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=45, ha="right")
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        plt.colorbar(im, ax=ax)

    def _plot_lollipop(self, ax, cdim):
        ax.hlines(y=cdim.index, xmin=0, xmax=cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_xlabel("Count")

    def _plot_circular(self, ax, cattr):
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)

    def _plot_scatter(self, ax, cdim):
        xvals= np.arange(len(cdim))
        yvals= cdim["Count"].values
        ax.scatter(xvals, yvals, color="green")
        for i, row in cdim.iterrows():
            ax.text(xvals[i], yvals[i], row["Dimension"], ha="center", va="bottom", rotation=60, fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("Mismatch Count")

    def _plot_radar(self, ax, cdim):
        cat= cdim.index.tolist()
        val= cdim.values.tolist()
        angles= np.linspace(0, 2*np.pi, len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)

    def _plot_pie(self, ax, dist):
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)

    def _plot_bar(self, ax, cattr):
        bars= ax.bar(range(len(cattr)), cattr.values, color="blue")
        ax.set_xticks(range(len(cattr)))
        ax.set_xticklabels(cattr.index, rotation=45, ha="right", fontsize=8)
        ax.set_ylabel("Count")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, f"{int(h)}", ha="center", va="bottom")

    def _plot_bollinger(self, ax, date_ct):
        date_ct["RunDate_dt"] = pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)

        date_ct["rolling_mean"] = date_ct["Count"].rolling(window=3, min_periods=1).mean()
        date_ct["rolling_std"]  = date_ct["Count"].rolling(window=3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]   = date_ct["rolling_mean"] + 2*date_ct["rolling_std"]
        date_ct["lower_band"]   = date_ct["rolling_mean"] - 2*date_ct["rolling_std"]

        xvals = np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"],
                        color="blue", alpha=0.2, label="±2σ Band")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Actual Count")

        ax.set_xticks(xvals)
        xlabels = [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_title("Bollinger Band Over Time")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Mismatch Count")
        ax.legend()


# ----------------------------------------------------------------------------
# ADVANCED DASHBOARD
# ----------------------------------------------------------------------------
class AdvancedDashboard(ctk.CTkFrame):
    """
    The mismatch DataFrame must contain columns:
        Dimension, Attribute, Gap In, Key, etc.
    The existing 8 charts group by dimension/attribute to count mismatches.
    """
    def __init__(self, parent, config: Dict):
        super().__init__(parent)
        self.config= config
        dash_cfg= config.get("dashboard", {})
        self.selected_dims: Set[str] = set(dash_cfg.get("selected_dims", []))
        self.selected_attrs: Set[str] = set(dash_cfg.get("selected_attrs", []))
        self.top_n= dash_cfg.get("top_n", 10)

        self.df_current= pd.DataFrame()
        self.df_history= pd.DataFrame()

        self.topbar= ctk.CTkScrollableFrame(self, orientation="horizontal", height=60)
        self.topbar.pack(fill="x", pady=5)
        self.metric_label= ctk.CTkLabel(self.topbar, text="Metrics: 0 mismatch, 0 dimension", width=300)
        self.metric_label.pack(side="left", padx=5)

        ctk.CTkButton(
            self.topbar, text="Filter Dimension", command=self.show_dimension_filter,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

        ctk.CTkButton(
            self.topbar, text="Filter Attribute", command=self.show_attribute_filter,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

        ctk.CTkButton(
            self.topbar, text="Toggle Top 10 / All", command=self.toggle_top_n,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)
        self.frames= {}
        chart_names= [
            "Heatmap","Lollipop","Circular","Scatter","Radar",
            "Normal Pie","Normal Bar","Bollinger Chart"
        ]
        for lbl in chart_names:
            fr= ctk.CTkFrame(self.notebook)
            fr.pack(fill="both", expand=True)
            self.notebook.add(fr, text=lbl)
            self.frames[lbl]= fr

    def toggle_top_n(self):
        if self.top_n==10:
            self.top_n= None
        else:
            self.top_n= 10
        self.update_data_filters()

    def show_dimension_filter(self):
        self.show_filter_popup("Dimension")

    def show_attribute_filter(self):
        self.show_filter_popup("Attribute")

    def show_filter_popup(self, col: str):
        base_df= self.df_history if not self.df_history.empty else self.df_current
        if base_df.empty or col not in base_df.columns:
            return
        popup= tk.Toplevel(self)
        popup.title(f"Filter: {col}")
        popup.geometry("300x400")
        frame= ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)

        unique_vals= base_df[col].dropna().unique()
        display_map={}
        for v in unique_vals:
            dsp= v if v else "(blank)"
            display_map[v]= dsp
        sorted_vals= sorted(display_map.keys(), key=lambda x: display_map[x].lower())

        if col=="Dimension":
            curr= self.selected_dims
        else:
            curr= self.selected_attrs
        if not curr:
            # if user hasn't set anything => treat as "all"
            curr= set(unique_vals)

        all_vals= set(sorted_vals)
        selall_var= tk.BooleanVar(value=(curr==all_vals or not curr))

        def toggle_all():
            check= selall_var.get()
            for vb in var_dict.values():
                vb.set(check)

        ctk.CTkCheckBox(
            frame, text="Select All", variable=selall_var, command=toggle_all,
            fg_color="#800020", hover_color="#a52a2a", text_color="black"
        ).pack(anchor="w", pady=5)

        scroll= ctk.CTkScrollableFrame(frame, width=250, height=250)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)
        var_dict={}
        for rv in sorted_vals:
            in_filter= (rv in curr) or (not curr)
            bvar= tk.BooleanVar(value=in_filter)
            var_dict[rv]= bvar
            ctk.CTkCheckBox(
                scroll, text=display_map[rv], variable=bvar,
                fg_color="#800020", hover_color="#a52a2a", text_color="black"
            ).pack(anchor="w")

        def apply_():
            sel= {rv for rv,bv in var_dict.items() if bv.get()}
            if col=="Dimension":
                self.selected_dims= sel
            else:
                self.selected_attrs= sel
            popup.destroy()
            self.update_data_filters()

        bf= ctk.CTkFrame(frame)
        bf.pack(fill="x", pady=5)
        ctk.CTkButton(
            bf, text="Apply", command=apply_,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)
        ctk.CTkButton(
            bf, text="Cancel", command=popup.destroy,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

    def update_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current= df_current.copy()
        self.df_history= df_history.copy()
        self.update_data_filters()

    def update_data_filters(self):
        dfc= self.df_current.copy()
        if not dfc.empty:
            if self.selected_dims:
                dfc= dfc[dfc["Dimension"].isin(self.selected_dims)]
            if self.selected_attrs:
                dfc= dfc[dfc["Attribute"].isin(self.selected_attrs)]

        mism= len(dfc)
        dims= dfc["Dimension"].nunique() if not dfc.empty and "Dimension" in dfc.columns else 0
        self.metric_label.configure(text=f"Mismatches: {mism}, Dims: {dims}")

        self.plotHeatmap(dfc)
        self.plotLollipop(dfc)
        self.plotCircular(dfc)
        self.plotScatter(dfc)
        self.plotRadar(dfc)
        self.plotNormalPie(dfc)
        self.plotNormalBar(dfc)
        self.plotBollinger()

    def plot_chart(self, frame, fig):
        for w in frame.winfo_children():
            w.destroy()

        canvas= FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()

        toolbar_frame= ctk.CTkFrame(frame)
        toolbar_frame.pack(side="top", fill="x")
        toolbar= NavigationToolbar2Tk(canvas, toolbar_frame)
        toolbar.update()

        canvas.get_tk_widget().pack(fill="both", expand=True)
        plt.close(fig)

    def _limit_if_needed(self, series: pd.Series)-> pd.Series:
        if self.top_n==10:
            return series.head(10)
        else:
            return series

    def plotHeatmap(self, dfc: pd.DataFrame):
        fr= self.frames["Heatmap"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty or "Gap In" not in dfc.columns:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty or not {"Dimension","Attribute"}.issubset(df_m.columns):
            return
        pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
        if pivot.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        cax= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=90)
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        fig.colorbar(cax, ax=ax)
        ax.set_title("Heatmap: Mismatch Count")
        self.plot_chart(fr, fig)

    def plotLollipop(self, dfc: pd.DataFrame):
        fr= self.frames["Lollipop"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        cdim= self._limit_if_needed(cdim)
        if cdim.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        ax.hlines(y= cdim.index, xmin=0, xmax=cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_title("Lollipop: Missing by Dimension")
        ax.set_xlabel("Count")
        self.plot_chart(fr, fig)

    def plotCircular(self, dfc: pd.DataFrame):
        fr= self.frames["Circular"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False)
        cattr= self._limit_if_needed(cattr)
        if cattr.empty:
            return
        fig= plt.figure(figsize=(6,6))
        ax= fig.add_subplot(111, polar=True)
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular: Missing Attributes")
        self.plot_chart(fr, fig)

    def plotScatter(self, dfc: pd.DataFrame):
        fr= self.frames["Scatter"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cdim= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim.sort_values("Count", ascending=False, inplace=True)
        if self.top_n==10:
            cdim= cdim.head(10)
        if cdim.empty:
            return
        fig, ax= plt.subplots(figsize=(6,5))
        xvals= np.arange(len(cdim))
        yvals= cdim["Count"].values
        labels= cdim["Dimension"].values
        ax.scatter(xvals,yvals,color="green")
        for i, txt in enumerate(labels):
            ax.text(xvals[i], yvals[i], txt, ha="center", va="bottom", rotation=60)
        ax.set_xticks([])
        ax.set_ylabel("Mismatch Count")
        ax.set_title("Scatter: Mismatch by Dimension")
        self.plot_chart(fr, fig)

    def plotRadar(self, dfc: pd.DataFrame):
        fr= self.frames["Radar"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        if self.top_n==10:
            cdim= cdim.head(10)
        if cdim.empty or len(cdim)<2:
            return
        cat= cdim.index.tolist()
        val= cdim.values.tolist()
        fig= plt.figure(figsize=(6,6))
        angles= np.linspace(0,2*np.pi,len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax= fig.add_subplot(111, polar=True)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)
        ax.set_title("Radar: Mismatch Dims", y=1.08)
        self.plot_chart(fr, fig)

    def plotNormalPie(self, dfc: pd.DataFrame):
        fr= self.frames["Normal Pie"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        dist= df_m["Gap In"].value_counts()
        fig, ax= plt.subplots(figsize=(5,5))
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Pie: Gap In distribution")
        self.plot_chart(fr, fig)

    def plotNormalBar(self, dfc: pd.DataFrame):
        fr= self.frames["Normal Bar"]
        for w in fr.winfo_children():
            w.destroy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]
        if df_m.empty:
            return
        cattr= df_m["Attribute"].value_counts().sort_values(ascending=False)
        if self.top_n==10:
            cattr= cattr.head(10)
        if cattr.empty:
            return
        fig, ax= plt.subplots(figsize=(6,4))
        bars= ax.bar(range(len(cattr)), cattr.values, color="blue")
        ax.set_xticks(range(len(cattr)))
        ax.set_xticklabels(cattr.index, rotation=45, ha="right")
        ax.set_ylabel("Count")
        ax.set_title("Bar: Mismatch attributes")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., h, str(int(h)), ha="center", va="bottom")
        plt.tight_layout()
        self.plot_chart(fr, fig)

    def plotBollinger(self):
        fr= self.frames["Bollinger Chart"]
        for w in fr.winfo_children():
            w.destroy()

        df_hist= self.df_history.copy()
        if df_hist.empty or "RunDate" not in df_hist.columns:
            return

        date_ct= df_hist.groupby("RunDate")["Key"].count().reset_index(name="Count")
        date_ct.sort_values("RunDate", inplace=True)
        if date_ct.empty:
            return

        fig, ax= plt.subplots(figsize=(12,4))  # wide for scrolling
        date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)

        date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
        date_ct["rolling_std"] = date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]  = date_ct["rolling_mean"] + 2*date_ct["rolling_std"]
        date_ct["lower_band"]  = date_ct["rolling_mean"] - 2*date_ct["rolling_std"]

        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"],
                        color="blue", alpha=0.2, label="±2σ Band")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Actual Count")

        ax.set_xticks(xvals)
        xlabels = [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Mismatch Count")
        ax.set_title("Bollinger Band Over Time (GUI)")
        ax.legend()

        self.plot_chart(fr, fig)


# ----------------------------------------------------------------------------
# HISTORY TAB
# ----------------------------------------------------------------------------
class HistoryTab(ctk.CTkFrame):
    def __init__(self, parent, hist_dir: Path):
        super().__init__(parent)
        self.history_dir= hist_dir
        self.tree= None
        self.build_ui()

    def build_ui(self):
        lbl= ctk.CTkLabel(self, text="Reconciliation Runs History", font=("Arial",16))
        lbl.pack(pady=5)

        self.tree= ttk.Treeview(self, columns=("Filename",), show="headings", height=15)
        self.tree.heading("Filename", text="History File")
        self.tree.pack(fill="both", expand=True, padx=10, pady=10)

        self.tree.bind("<Double-1>", self.on_double_click)

        refresh_btn= ctk.CTkButton(self, text="Refresh", command=self.refresh_history,
                                   fg_color="#800020", hover_color="#a52a2a", text_color="white")
        refresh_btn.pack(pady=5)

        self.refresh_history()

    def refresh_history(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if not self.history_dir.is_dir():
            self.history_dir.mkdir(parents=True, exist_ok=True)
        files= sorted(self.history_dir.glob("run_*.json"), reverse=True)
        for f in files:
            self.tree.insert("", "end", values=(f.name,))

    def on_double_click(self, event):
        item_id= self.tree.focus()
        if not item_id:
            return
        filename= self.tree.item(item_id,"values")[0]
        path= self.history_dir / filename
        if not path.is_file():
            return
        try:
            with open(path, "r", encoding="utf-8") as f:
                content= f.read()
            popup= tk.Toplevel(self)
            popup.title(f"Viewing {filename}")
            txt= ctk.CTkTextbox(popup, width=800, height=600)
            txt.pack(fill="both", expand=True)
            txt.insert("end", content)
            txt.configure(state="disabled")
        except Exception as e:
            logging.error(f"Error opening {path} => {e}")


# ----------------------------------------------------------------------------
# MAIN APP
# ----------------------------------------------------------------------------
class MainApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation (Updated Specs)")
        self.geometry("1600x900")
        ctk.set_appearance_mode("light")

        # On close => write config & bollinger data
        self.protocol("WM_DELETE_WINDOW", self.on_close)

        self.config_dict= load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.param_dict= read_param_file(Path(self.config_dict["paths"].get("PARAMETER_PATH",DEFAULT_PATHS["PARAMETER_PATH"])))
        self.history_df= pd.DataFrame()  # accumulates mismatch data across runs

        self.tabs= ttk.Notebook(self)
        self.tabs.pack(fill="both", expand=True)

        # 1) Paths
        self.tab_paths= ctk.CTkFrame(self.tabs)
        self.build_paths_tab(self.tab_paths)
        self.tabs.add(self.tab_paths, text="Paths")

        # 2) ERP preview
        self.tab_erp= ctk.CTkFrame(self.tabs)
        erp_filters= self.config_dict.get("erp_grid",{}).get("filters",{})
        self.erp_preview= SimplePreview(self.tab_erp,"ERP",filters_dict=erp_filters)
        self.erp_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_erp, text="ERP Preview")

        # 3) Master preview
        self.tab_master= ctk.CTkFrame(self.tabs)
        mast_filters= self.config_dict.get("master_grid",{}).get("filters",{})
        self.master_preview= SimplePreview(self.tab_master,"Master",filters_dict=mast_filters)
        self.master_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_master, text="Master Preview")

        # 4) Compare
        self.tab_compare= ctk.CTkFrame(self.tabs)
        self.build_compare_tab(self.tab_compare)
        self.tabs.add(self.tab_compare, text="Compare")

        # 5) Dashboard
        self.dashboard_tab= AdvancedDashboard(self.tabs, self.config_dict)
        self.tabs.add(self.dashboard_tab, text="Dashboard")

        # 6) History
        hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))
        self.history_tab= HistoryTab(self.tabs, hist_path)
        self.tabs.add(self.history_tab, text="History")

        # Log box
        self.log_box= ctk.CTkTextbox(self, height=120)
        self.log_box.pack(fill="both", side="bottom")
        self.log_box.configure(state="disabled")
        handler= TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        # Make sure the Master CSV folder exists (but hidden from user)
        self.temp_csv_dir= Path(self.config_dict["paths"].get("MASTER_CSV_OUTPUT","temp_master_csv"))
        self.temp_csv_dir.mkdir(parents=True, exist_ok=True)

        # Load existing runs
        self.load_all_runs()

        # Initial meltdown => preview
        self.refresh_erp()
        self.refresh_master()

        # Update the dashboard with the entire loaded mismatch history
        self.dashboard_tab.update_data(pd.DataFrame(), self.history_df)

        ctk.CTkButton(self, text="Close", command=self.on_close,
                      fg_color="#800020", hover_color="#a52a2a", text_color="white").pack(pady=5)

    def build_paths_tab(self, parent):
        """
        Now we only expose 6 user-selectable paths:
          - ERP Excel
          - Master ZIP
          - Exception Path
          - Missing Items Output
          - Parameter File
          - PDF Export Path
        The rest remain in code.
        """
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        # We'll keep references in code/config but NOT show them in the UI:
        #  CONFIG_PATH, MASTER_CSV_OUTPUT, LOGO_PATH, HISTORY_PATH, BAND_CHART_JSON_PATH

        self.erp_var= tk.StringVar(value=self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mast_var= tk.StringVar(value=self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_var= tk.StringVar(value=self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_var= tk.StringVar(value=self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))
        self.par_var= tk.StringVar(value=self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"]))
        self.pdf_var= tk.StringVar(value=self.config_dict["paths"].get("PDF_EXPORT_PATH", DEFAULT_PATHS["PDF_EXPORT_PATH"]))

        def mkrow(lbl, var, is_dir=False):
            rowf= ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=220).pack(side="left", padx=5)
            e= ctk.CTkEntry(rowf, textvariable=var, width=600)
            e.pack(side="left", padx=5)
            def br():
                if is_dir:
                    p= filedialog.askdirectory()
                else:
                    p= filedialog.askopenfilename()
                if p:
                    var.set(p)
            ctk.CTkButton(rowf, text="Browse", command=br,
                          fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)

        mkrow("ERP Excel:", self.erp_var)
        mkrow("Master ZIP:", self.mast_var)
        mkrow("Exception Path:", self.exc_var)
        mkrow("Missing Items Output (Excel):", self.out_var)
        mkrow("Parameter File:", self.par_var)
        mkrow("PDF Export Path:", self.pdf_var)

        bf= ctk.CTkFrame(frm)
        bf.pack(fill="x", pady=10)
        ctk.CTkButton(bf, text="Save Config", command=self.save_all_config,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Refresh ERP", command=self.refresh_erp,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Refresh Master", command=self.refresh_master,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)

    def build_compare_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Generate Missing Items", font=("Arial",16)).pack(pady=5)
        ctk.CTkButton(frm, text="Run Reconciliation", command=self.run_comparison,
                      fg_color="#800020", hover_color="#a52a2a").pack(pady=10)
        ctk.CTkButton(frm, text="Export PDF Report",
                      command=self.export_pdf,
                      fg_color="#800020", hover_color="#a52a2a").pack(pady=10)

    def load_all_runs(self):
        """Merge all run_*.json in HISTORY_PATH into self.history_df so we see older mismatch data on startup."""
        hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))
        if not hist_path.is_dir():
            return
        frames=[]
        for jf in hist_path.glob("run_*.json"):
            try:
                jdata= pd.read_json(jf, orient="records")
                frames.append(jdata)
            except Exception as e:
                logging.error(f"Error reading {jf} => {e}")
        if frames:
            big= pd.concat(frames, ignore_index=True)
            big.drop_duplicates(inplace=True)
            # We only keep rows where "Gap In" != "CASE" because the dashboard is for mismatches
            # But if you prefer to store them in history anyway, you can. 
            # Usually we skip them so Bollinger counts only real mismatch.
            big = big[ big["Gap In"]!="CASE" ]
            if self.history_df.empty:
                self.history_df= big
            else:
                self.history_df= pd.concat([self.history_df, big], ignore_index=True)
                self.history_df.drop_duplicates(inplace=True)
            logging.info(f"Loaded all runs => total {len(self.history_df)} mismatch records from {hist_path}")

    def refresh_erp(self):
        erp_path= Path(self.erp_var.get().strip())
        raw_erp= read_erp_excel(erp_path)
        if raw_erp.empty:
            self.erp_preview.set_data(pd.DataFrame())
            return
        dfp_wide= meltdown_erp_for_preview(raw_erp, self.param_dict)
        self.erp_preview.set_data(dfp_wide)

    def refresh_master(self):
        zip_path= Path(self.mast_var.get().strip())
        out_dir= self.temp_csv_dir
        csvs= convert_master_txt_to_csv(zip_path, out_dir)
        raw_mast= unify_master_csvs(csvs)
        if raw_mast.empty:
            self.master_preview.set_data(pd.DataFrame())
            return
        dfp_wide= meltdown_master_for_preview(raw_mast, self.param_dict)
        self.master_preview.set_data(dfp_wide)

    def run_comparison(self):
        """
        1) Take the wide data from ERP & Master previews
        2) meltdown to long
        3) produce mismatch_df & casediff_df
        4) merge with exceptions
        5) write to 2-sheet Excel
        6) store mismatch_df in self.history_df + JSON
        7) update dashboard
        """
        df_erp_wide= self.erp_preview.get_filtered_df()
        df_mast_wide= self.master_preview.get_filtered_df()

        erp_long = meltdown_to_long(df_erp_wide)
        mast_long= meltdown_to_long(df_mast_wide)

        mismatch_df, case_df = compare_2sheet(erp_long, mast_long)

        exc_path= Path(self.exc_var.get().strip())
        df_exc= read_exception_table(exc_path)

        mismatch_df = merge_exceptions(mismatch_df, df_exc)
        case_df     = merge_exceptions(case_df, df_exc)

        # Write 2-sheet xlsx
        out_path= Path(self.out_var.get().strip())
        write_2sheet_excel(mismatch_df, case_df, out_path)

        # For "history" + Dashboard + Bollinger, we only track real mismatch (exclude "CASE")
        # We'll add a "RunDate" column
        run_timestamp= datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        mismatch_df["RunDate"]= run_timestamp
        # Because the user wants Bollinger only for mismatch
        dash_df = mismatch_df.copy()

        if self.history_df.empty:
            self.history_df = dash_df.copy()
        else:
            self.history_df = pd.concat([self.history_df, dash_df], ignore_index=True)

        # remove duplicates
        self.history_df.drop_duplicates(inplace=True)

        # Write JSON (for mismatch only)
        hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))
        hist_path.mkdir(parents=True, exist_ok=True)
        run_file= hist_path / f"run_{run_timestamp.replace(':','-').replace(' ','_')}.json"
        try:
            dash_df.to_json(run_file, orient="records", indent=2)
            logging.info(f"Saved run => {run_file}")
        except Exception as e:
            logging.error(f"Error writing JSON => {e}")

        # update dashboard with new data
        self.dashboard_tab.update_data(dash_df, self.history_df)
        # refresh history UI
        self.history_tab.refresh_history()

        self.tabs.select(self.dashboard_tab)
        messagebox.showinfo("Done", f"Missing items => {out_path}")

    def export_pdf(self):
        if self.history_df.empty:
            messagebox.showinfo("PDF Export", "No mismatch data => history is empty.")
            return
        if "RunDate" in self.history_df.columns:
            last_run= self.history_df["RunDate"].max()
            df_current= self.history_df[self.history_df["RunDate"]== last_run].copy()
        else:
            df_current= self.history_df.copy()

        df_history= self.history_df.copy()  # entire mismatch history
        rep= EnhancedPDFReport(df_current, df_history, self.config_dict)
        pdf_path= rep.generate()
        messagebox.showinfo("PDF Export", f"PDF exported => {pdf_path}")

    def save_all_config(self):
        # Update the main config dict with the 6 user paths
        self.config_dict["paths"]["ERP_EXCEL_PATH"]= self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mast_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        self.config_dict["paths"]["PARAMETER_PATH"]= self.par_var.get().strip()
        self.config_dict["paths"]["PDF_EXPORT_PATH"]= self.pdf_var.get().strip()

        # The others remain in code/config but not shown in UI. Keep them as-is in self.config_dict["paths"].

        # Save grid filters (even though we no longer filter Start/End Date by heading).
        self.config_dict.setdefault("erp_grid", {})
        self.config_dict["erp_grid"]["filters"]= self.erp_preview.filters

        self.config_dict.setdefault("master_grid", {})
        self.config_dict["master_grid"]["filters"]= self.master_preview.filters

        # Dashboard config
        dash_cfg= self.config_dict.setdefault("dashboard", {})
        dash_cfg["selected_dims"]= list(self.dashboard_tab.selected_dims)
        dash_cfg["selected_attrs"]= list(self.dashboard_tab.selected_attrs)
        dash_cfg["top_n"]= self.dashboard_tab.top_n

        # Finally write the JSON
        cfg_path= Path(self.config_dict["paths"]["CONFIG_PATH"])
        save_config(self.config_dict, cfg_path)

    def on_close(self):
        # 1) Save config
        self.save_all_config()

        # 2) Write final Bollinger JSON from entire mismatch history
        band_path= self.config_dict["paths"].get("BAND_CHART_JSON_PATH","")
        if band_path and not self.history_df.empty and "RunDate" in self.history_df.columns:
            try:
                outp= Path(band_path)
                date_ct= self.history_df.groupby("RunDate")["Key"].count().reset_index(name="Count")
                date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
                date_ct.sort_values("RunDate_dt", inplace=True)
                date_ct.reset_index(drop=True, inplace=True)

                # rolling calc
                date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
                date_ct["rolling_std"] = date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
                date_ct["upper_band"]  = date_ct["rolling_mean"] + 2*date_ct["rolling_std"]
                date_ct["lower_band"]  = date_ct["rolling_mean"] - 2*date_ct["rolling_std"]

                # Convert rundate_dt to string
                date_ct["RunDate"]= date_ct["RunDate_dt"].dt.strftime("%Y-%m-%d %H:%M:%S")
                date_ct.drop(columns=["RunDate_dt"], inplace=True)

                date_ct.to_json(outp, orient="records", indent=2)
                logging.info(f"Bollinger data saved on close => {outp}")
            except Exception as e:
                logging.error(f"Bollinger JSON on close => {e}")

        self.destroy()


def main():
    app= MainApp()
    app.mainloop()

if __name__=="__main__":
    main()
