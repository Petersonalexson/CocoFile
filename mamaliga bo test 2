#!/usr/bin/env python3
"""
Ultra-Mega Reconciliation with:
 - Automatic loading of all history JSON runs on startup
 - ERP & Master Previews (Start/End Date filterable) with possible "<<NaN>>" placeholders
 - Compare => missing_items.xlsx with 2 sheets (Mismatch + Case_Differences), but we only use the first sheet for the Dashboard
 - "Gap In" column instead of "Missing In"
 - 8-chart Dashboard with Bollinger band
 - Bollinger data saved on close (for entire history) to a user-chosen JSON
 - KEY = (Dimension|Name|Attribute|Master|ERP).upper() in Mismatch
 - Only check case differences for rows where Attribute=="Name"
 - Path tab is visible, but we hide only the ones indicated in comments
 - CSV reading tries multiple encodings (utf-8-sig, utf-16-le, utf-16-be, cp1252, latin-1, ascii)

Explanation of Key Points:

1) We produce 2 sheets in "missing_items.xlsx" => "Mismatch" + "Case_Differences",
   but for the Dashboard (GUI and PDF), we only read the first sheet ("Mismatch") as the main data.
2) We handle "<<NaN>>" placeholders if the data is truly NaN.
3) The Dashboard has a "Bollinger Chart" tab that reads from all historical JSON runs,
   aggregates by RunDate, and computes rolling stats (window=3) for upper/lower band.
4) On close, we save the entire historical Bollinger data to JSON.
5) We keep the Path tab in the UI, but hide certain paths if desired.
"""

import os
import sys
import json
import math
import logging
import zipfile
import shutil
import io
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Set, List, Tuple

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import customtkinter as ctk

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
from matplotlib.backends.backend_pdf import PdfPages

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------------------------------------------------------
# DEFAULT CONFIG & SAVE/LOAD
# ----------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/missing_items.xlsx",
    # We hide these next few paths in the UI (just do a comment so user won't see them)
    "CONFIG_PATH": "config/ui_config.json",          # hidden
    "PARAMETER_PATH": "data/parameters.xlsx",        # hidden
    "MASTER_CSV_OUTPUT": "temp_master_csv",          # hidden
    "PDF_EXPORT_PATH": "output/dashboard_report.pdf", # hidden
    "LOGO_PATH": "images/company_logo.png",           # hidden
    "HISTORY_PATH": "history_runs",                  # hidden
    "BAND_CHART_JSON_PATH": "data/bollinger_data.json" # hidden
}

def default_config() -> Dict:
    return {
        "paths": DEFAULT_PATHS.copy(),
        "erp_grid": {"filters": {}},
        "master_grid": {"filters": {}},
        "dashboard": {
            "selected_dims": [],
            "selected_attrs": [],
            "top_n": 10
        }
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config => {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        # Convert sets->lists in erp_grid
        if "erp_grid" in cfg and "filters" in cfg["erp_grid"]:
            newf = {}
            for col, svals in cfg["erp_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["erp_grid"]["filters"] = newf

        # Convert sets->lists in master_grid
        if "master_grid" in cfg and "filters" in cfg["master_grid"]:
            newf = {}
            for col, svals in cfg["master_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["master_grid"]["filters"] = newf

        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config => {e}")


# ----------------------------------------------------------------------------
# TEXT LOGGER HANDLER
# ----------------------------------------------------------------------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget
    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")


# ----------------------------------------------------------------------------
# READ PARAM
# ----------------------------------------------------------------------------
def read_param_file(path: Path) -> Dict[str, object]:
    param = {
        "dim_erp_keep": set(),
        "dim_erp_map": {},
        "dim_master_map": {},
        "attr_erp_map": {},
        "attr_master_map": {}
    }
    if not path.is_file():
        logging.warning(f"Param file not found => {path}")
        return param
    try:
        dim_df = pd.read_excel(path, sheet_name="Dimension Parameters")
        dim_df.columns = dim_df.columns.astype(str).str.strip()

        def s(x): return str(x).strip() if pd.notna(x) else ""

        for _, row in dim_df.iterrows():
            fn  = s(row.get("FileName", ""))
            vsc = s(row.get("V S C", ""))
            dim = s(row.get("Dimension", ""))
            ev  = s(row.get("ERP Values", ""))
            if ev.lower() == "x" and vsc and dim:
                param["dim_erp_keep"].add(vsc)
            if vsc and dim:
                param["dim_erp_map"][vsc] = dim
            if fn and dim and ev.lower() == "x":
                param["dim_master_map"][fn] = dim

        attr_df = pd.read_excel(path, sheet_name="Attribute Parameters")
        attr_df.columns = attr_df.columns.astype(str).str.strip()
        for _, row in attr_df.iterrows():
            e_orig = s(row.get("ERP Original Attributes", ""))
            m_orig = s(row.get("Master Original Attributes", ""))
            final_ = s(row.get("Attribute", ""))
            onoff  = s(row.get("On/Off", ""))
            if onoff.lower() == "x" and final_:
                if e_orig:
                    param["attr_erp_map"][e_orig] = final_
                if m_orig:
                    param["attr_master_map"][m_orig] = final_
        return param
    except Exception as e:
        logging.error(f"Error reading param file => {e}")
        return param


# ----------------------------------------------------------------------------
# Keep blank/future End Date
# ----------------------------------------------------------------------------
def keep_valid_end_date(attr: str, val) -> bool:
    if attr != "End Date":
        return True
    v = str(val).strip()
    if not v:
        return True
    try:
        dt = datetime.strptime(v, "%Y-%m-%d").date()
        return dt > datetime.now().date()
    except:
        return False

def strip_t(val) -> str:
    if pd.isna(val):
        return "<<NaN>>"  # explicitly put <<NaN>> placeholder
    s = str(val).strip()
    if "T" in s:
        s = s.split("T")[0]
    if not s:
        return "<<BLANK>>"
    return s


# ----------------------------------------------------------------------------
# ERP reading
# ----------------------------------------------------------------------------
def read_erp_excel(path: Path) -> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"ERP Excel not found => {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path, skiprows=3)
        df.columns = df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df = df[df["Enabled_Flag"] == "Enabled"]
        return df
    except Exception as e:
        logging.error(f"Error reading ERP => {e}")
        return pd.DataFrame()


# ----------------------------------------------------------------------------
# MASTER reading (all encodings)
# ----------------------------------------------------------------------------
def read_txt_2encodings(raw: bytes) -> pd.DataFrame:
    import io
    encodings_to_try = ["utf-8-sig","utf-16-le","utf-16-be","cp1252","latin-1","ascii"]
    for enc in encodings_to_try:
        try:
            buf = io.BytesIO(raw)
            df = pd.read_csv(buf, encoding=enc, on_bad_lines="skip", engine="python")
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            df.columns = df.columns.astype(str).str.strip()
            logging.info(f"[read_txt_2encodings] success with {enc}, shape={df.shape}")
            return df
        except Exception as e:
            logging.debug(f"[read_txt_2encodings] fail with {enc} => {e}")
    logging.error("[read_txt_2encodings] cannot parse => empty.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path):
    if not zip_path.is_file():
        logging.warning(f"[Master] ZIP not found => {zip_path}")
        return []
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    csvs = []
    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name = os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw = fo.read()
                df = read_txt_2encodings(raw)
                if df.empty:
                    continue
                df["RawFileName"] = base_name
                if "Name" not in df.columns and len(df.columns) > 0:
                    first_col = df.columns[0]
                    if first_col != "Name":
                        df.rename(columns={first_col: "Name"}, inplace=True)
                out_csv = out_dir / (base_name.replace(".txt", ".csv"))
                df.to_csv(out_csv, index=False, encoding="utf-8")
                csvs.append(out_csv)
            except Exception as e:
                logging.error(f"[Master] error reading {txt_file} => {e}")
    return csvs

def read_csv_2encodings(path: Path) -> pd.DataFrame:
    encodings_to_try = ["utf-8-sig","utf-16-le","utf-16-be","cp1252","latin-1","ascii"]
    for enc in encodings_to_try:
        try:
            df = pd.read_csv(path, encoding=enc, on_bad_lines="skip")
            df.columns = df.columns.astype(str).str.strip()
            return df
        except Exception as ex:
            logging.debug(f"Failed reading {path} with {enc}: {ex}")
    logging.error(f"Cannot read CSV => {path}")
    return pd.DataFrame()

def unify_master_csvs(csvs: List[Path]) -> pd.DataFrame:
    frames = []
    for cp in csvs:
        if not cp.is_file():
            continue
        try:
            df = read_csv_2encodings(cp)
            frames.append(df)
        except Exception as e:
            logging.error(f"[unify_master_csvs] reading {cp} => {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()


# ----------------------------------------------------------------------------
# MELTDOWN => keep valid/future End Date, keep placeholders for NaN
# ----------------------------------------------------------------------------
def meltdown_erp_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    if "V_S_C" not in df.columns:
        return pd.DataFrame()
    keep = param["dim_erp_keep"]
    dmap = param["dim_erp_map"]
    amap = param["attr_erp_map"]

    df2 = df[df["V_S_C"].isin(keep)].copy()
    if df2.empty:
        return pd.DataFrame()

    skip_cols = {"V_S_C", "Enabled_Flag"}
    id_vars = []
    if "Value" in df2.columns:
        id_vars.append("Value")
        skip_cols.add("Value")

    df2["Dimension"] = df2["V_S_C"].map(dmap).fillna(df2["V_S_C"])
    skip_cols.add("Dimension")
    if "Value" in id_vars:
        id_vars.insert(0, "Dimension")
    else:
        id_vars.append("Dimension")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    if "Value" in id_vars:
        melted.rename(columns={"Value": "Name"}, inplace=True)
    else:
        melted["Name"] = ""

    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)

    def handle_date(a, val):
        s = strip_t(val)
        if a in ["Start Date","End Date"] and s not in ["<<NaN>>","<<BLANK>>"]:
            try:
                dt = datetime.strptime(s, "%Y-%m-%d").date()
                s = dt.strftime("%Y-%m-%d")
            except:
                s = "<<BLANK>>"
        return s

    melted["Value"] = melted.apply(lambda r: handle_date(r["Attribute"], r["ValX"]), axis=1)

    keep_rows = []
    for _, row in melted.iterrows():
        if keep_valid_end_date(row["Attribute"], row["Value"]):
            keep_rows.append(row)

    out = pd.DataFrame(keep_rows)
    return out[["Dimension","Name","Attribute","Value"]]

def meltdown_master_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    if df.empty or "RawFileName" not in df.columns:
        return pd.DataFrame()
    dmap = param["dim_master_map"]
    amap = param["attr_master_map"]

    df2 = df[df["RawFileName"].isin(dmap.keys())].copy()
    if df2.empty:
        return pd.DataFrame()

    df2["Dimension"] = df2["RawFileName"].map(dmap).fillna(df2["RawFileName"])

    skip_cols = {"RawFileName", "Dimension"}
    id_vars = ["Dimension"]
    if "Name" in df2.columns:
        id_vars.append("Name")
        skip_cols.add("Name")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)

    def handle_date(a, val):
        s = strip_t(val)
        if a in ["Start Date","End Date"] and s not in ["<<NaN>>","<<BLANK>>"]:
            try:
                dt = datetime.strptime(s, "%Y-%m-%d").date()
                s = dt.strftime("%Y-%m-%d")
            except:
                s = "<<BLANK>>"
        return s

    melted["Value"] = melted.apply(lambda r: handle_date(r["Attribute"], r["ValX"]), axis=1)

    keep_rows = []
    for _, row in melted.iterrows():
        if keep_valid_end_date(row["Attribute"], row["Value"]):
            keep_rows.append(row)
    out = pd.DataFrame(keep_rows)
    if "Name" not in out.columns:
        out["Name"] = ""
    return out[["Dimension","Name","Attribute","Value"]]

def pivot_for_preview(df: pd.DataFrame) -> pd.DataFrame:
    if not df.empty and {"Dimension","Name","Attribute"}.issubset(df.columns):
        df = df.drop_duplicates(subset=["Dimension","Name","Attribute"])
        try:
            df = df.pivot(index=["Dimension","Name"], columns="Attribute", values="Value").reset_index()
        except Exception as e:
            logging.error(f"Pivot error => {e}")
    return df


# ----------------------------------------------------------------------------
# COMPARE => produce "Gap In" => Mismatch + Case_Differences => 2 sheets
# ----------------------------------------------------------------------------
def melt_back(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or {"Dimension","Name"}.difference(df.columns):
        return pd.DataFrame()
    meltdown_cols = [c for c in df.columns if c not in ("Dimension","Name")]
    melted = df.melt(id_vars=["Dimension","Name"], value_vars=meltdown_cols,
                     var_name="Attribute", value_name="Value")
    return melted[["Dimension","Name","Attribute","Value"]]

def build_keys(df: pd.DataFrame)-> pd.DataFrame:
    df = df.copy()
    for c in ["Dimension","Name","Attribute","Value"]:
        if c not in df.columns:
            df[c] = ""
        df[c] = df[c].fillna("").astype(str).str.strip()
    df["GroupKey"] = df["Dimension"] + " | " + df["Name"]
    return df

def compare_mode2(df_erp: pd.DataFrame, df_mst: pd.DataFrame)-> pd.DataFrame:
    def to_dict(d):
        out={}
        for gk, grp in d.groupby("GroupKey"):
            rec={}
            nm= grp["Name"].iloc[0] if not grp.empty else ""
            rec["Name"] = nm
            for _, row in grp.iterrows():
                rec[row["Attribute"]] = row["Value"]
            out[gk] = rec
        return out

    e_dict = to_dict(df_erp)
    m_dict = to_dict(df_mst)
    all_gk = set(e_dict.keys()) | set(m_dict.keys())

    results=[]
    for gk in all_gk:
        dim = gk.split(" | ")[0]
        a_data= e_dict.get(gk,{})
        b_data= m_dict.get(gk,{})
        name_a= a_data.get("Name","")
        name_b= b_data.get("Name","")
        if name_a and name_b and (name_a==name_b):
            union_attrs= (set(a_data.keys()) | set(b_data.keys())) - {"Name"}
            for at in union_attrs:
                va= a_data.get(at,"")
                vb= b_data.get(at,"")
                if va != vb:
                    if va and not vb:
                        results.append({"Dimension": dim,"Name": name_a,"Attribute": at,
                                        "Master": "", "ERP": va, "Gap In": "MASTER"})
                    elif vb and not va:
                        results.append({"Dimension": dim,"Name": name_a,"Attribute": at,
                                        "Master": vb, "ERP": "", "Gap In": "ERP"})
                    else:
                        results.append({"Dimension": dim,"Name": name_a,"Attribute": at,
                                        "Master": vb, "ERP": va, "Gap In": "MISMATCH"})
        else:
            if name_a and not name_b:
                results.append({"Dimension": dim, "Name": name_a, "Attribute": "Name",
                                "Master": "", "ERP": name_a, "Gap In": "MASTER"})
            elif name_b and not name_a:
                results.append({"Dimension": dim, "Name": name_b, "Attribute": "Name",
                                "Master": name_b, "ERP": "", "Gap In": "ERP"})
            else:
                if name_a or name_b:
                    results.append({"Dimension": dim, "Name": name_a or name_b, "Attribute": "Name",
                                    "Master": name_b, "ERP": name_a, "Gap In": "MISMATCH"})

    df_res= pd.DataFrame(results)
    if not df_res.empty:
        def build_key_original(r):
            return f"{r['Dimension']}|{r['Name']}|{r['Attribute']}|{r['Master']}|{r['ERP']}"
        df_res["Key_Original"] = df_res.apply(build_key_original, axis=1)
        df_res["Key"] = df_res["Key_Original"].str.upper()
    return df_res

def read_exception_table(path: Path)-> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"Exception table not found => {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path)
        df.columns = df.columns.astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"Error reading exception => {e}")
        return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame)-> pd.DataFrame:
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep_cols= [c for c in ["Key","Comments_1","Comments_2","hide exception"] if c in df_exc.columns]
    if not keep_cols:
        return df
    exc= df_exc[keep_cols].copy()
    exc["Key"]= exc["Key"].astype(str).str.strip()

    merged= df.merge(exc, on="Key", how="left", suffixes=("","_exc"))
    merged["hide exception"]= merged.get("hide exception","").fillna("").str.lower()
    final= merged[merged["hide exception"]!="yes"].copy()

    if "Comments_1_exc" in final.columns:
        final["Comments_1"]= np.where(final["Comments_1_exc"].notna(), final["Comments_1_exc"], final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"]= np.where(final["Comments_2_exc"].notna(), final["Comments_2_exc"], final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)

    return final

# ----------------------------------------------------------------------------
# CASE DIFF => second sheet, rest => mismatch in first sheet
# ----------------------------------------------------------------------------
def separate_case_diffs(df: pd.DataFrame) -> Tuple[pd.DataFrame,pd.DataFrame]:
    if df.empty:
        return df, pd.DataFrame()
    mask_attr = df["Attribute"]=="Name"
    same_lower = df["Master"].str.lower() == df["ERP"].str.lower()
    diff_actual = df["Master"] != df["ERP"]
    case_mask = mask_attr & same_lower & diff_actual

    case_df = df[case_mask].copy()
    mismatch_df= df[~case_mask].copy()
    if not case_df.empty:
        case_df.rename(columns={"Key":"Key_Upper","Key_Original":"Key"}, inplace=True)
    return mismatch_df, case_df

# ----------------------------------------------------------------------------
# WRITE 2 sheets => 'Mismatch' + 'Case_Differences'
# But the Dashboard only uses 'Mismatch' (first sheet)
# ----------------------------------------------------------------------------
def write_two_sheet_excel(mismatch: pd.DataFrame, case_only: pd.DataFrame, out_path: Path):
    out_path.parent.mkdir(parents=True, exist_ok=True)
    base_cols = ["Key","Dimension","Name","Attribute","Master","ERP","Comments_1","Comments_2","Gap In"]
    def extra_cols(df: pd.DataFrame) -> List[str]:
        e = []
        if "Key_Original" in df.columns:
            e.append("Key_Original")
        if "Key_Upper" in df.columns:
            e.append("Key_Upper")
        return e

    mismatch_extras= extra_cols(mismatch)
    case_extras= extra_cols(case_only)

    def finalize_cols(df: pd.DataFrame, extras:List[str]) -> pd.DataFrame:
        want = base_cols + extras
        final_df= df.copy()
        for c in want:
            if c not in final_df.columns:
                final_df[c] = ""
        return final_df[want]

    mismatch_f= finalize_cols(mismatch, mismatch_extras)
    case_f= finalize_cols(case_only, case_extras)

    wb= Workbook()
    ws_m= wb.active
    ws_m.title= "Mismatch"
    ws_m.append(mismatch_f.columns.tolist())
    for rowvals in mismatch_f.itertuples(index=False):
        ws_m.append(rowvals)

    header_font= Font(bold=True)
    fill= PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
    for cell in ws_m[1]:
        cell.font= header_font
        cell.fill= fill
        cell.alignment= Alignment(horizontal="center")
    for col in ws_m.columns:
        mx= 0
        let= col[0].column_letter
        for cell in col:
            val= str(cell.value) if cell.value else ""
            mx= max(mx, len(val))
        ws_m.column_dimensions[let].width= mx+2
    ws_m.freeze_panes= "A2"

    ws_c= wb.create_sheet("Case_Differences")
    ws_c.append(case_f.columns.tolist())
    for rowvals in case_f.itertuples(index=False):
        ws_c.append(rowvals)
    for cell in ws_c[1]:
        cell.font= header_font
        cell.fill= fill
        cell.alignment= Alignment(horizontal="center")
    for col in ws_c.columns:
        mx= 0
        let= col[0].column_letter
        for cell in col:
            val= str(cell.value) if cell.value else ""
            mx= max(mx, len(val))
        ws_c.column_dimensions[let].width= mx+2
    ws_c.freeze_panes= "A2"

    wb.save(out_path)
    logging.info(f"Missing items => {out_path}")


# ----------------------------------------------------------------------------
# SIMPLE PREVIEW => meltdown pivot
# ----------------------------------------------------------------------------
class SimplePreview(ctk.CTkFrame):
    FILTERABLE= {"Start Date","End Date"}

    def __init__(self, parent, name: str, filters_dict=None):
        super().__init__(parent)
        self.name= name
        self.df= pd.DataFrame()
        self.filters: Dict[str, Set[str]] = {}
        if filters_dict:
            for col, val_list in filters_dict.items():
                if isinstance(val_list, list):
                    self.filters[col] = set(val_list)
                else:
                    self.filters[col] = set()
        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        bar= ctk.CTkFrame(self, corner_radius=10, fg_color="#f0f0f0")
        bar.pack(fill="x", padx=5, pady=5)
        ctk.CTkLabel(
            bar, text=f"{self.name} Preview",
            fg_color="#800020", corner_radius=8,
            text_color="white",
            font=ctk.CTkFont(size=14, weight="bold")
        ).pack(side="left", padx=5)

        # Info + Clear Filters for date
        ctk.CTkButton(
            bar, text="ⓘ", width=30, command=self.show_info,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

        ctk.CTkButton(
            bar, text="Clear Date Filters", command=self.clear_filters,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

    def show_info(self):
        messagebox.showinfo(
            "Info",
            f"{self.name} meltdown data.\nOnly Start/End Date columns are filterable.\n\n"
            "We show <<NaN>> for missing data, and <<BLANK>> for empty strings."
        )

    def create_table(self):
        container= ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree= ttk.Treeview(container, show="headings")
        vsb= ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb= ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")
        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

    def create_statusbar(self):
        self.status_label= ctk.CTkLabel(self, text="0 rows", fg_color="#f0f0f0", text_color="black")
        self.status_label.pack(fill="x")

    def set_data(self, df: pd.DataFrame):
        self.df= df.copy()
        self.refresh_table()

    def refresh_table(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if self.df.empty:
            self.tree["columns"] = []
            self.status_label.configure(text="0 rows")
            return

        cols= list(self.df.columns)
        self.tree["columns"]= cols
        for c in cols:
            self.tree.heading(
                c, text=c, anchor="w",
                command=lambda col=c: self.on_heading_click(col)
            )
            self.tree.column(c, anchor="w", width=150)

        df_f= self.apply_filters()
        for _, row in df_f.iterrows():
            rowvals= [row[c] for c in cols]
            self.tree.insert("", "end", values=rowvals)
        self.status_label.configure(text=f"{len(df_f)} rows")

    def apply_filters(self)-> pd.DataFrame:
        df_f= self.df.copy()
        for col, allowed_strs in self.filters.items():
            if col not in df_f.columns:
                continue
            if not allowed_strs:
                df_f = df_f.iloc[0:0]
                continue
            def keeper(x):
                if x=="<<NaN>>":
                    return ("<<NaN>>" in allowed_strs)
                elif x=="<<BLANK>>":
                    return ("<<BLANK>>" in allowed_strs)
                return (str(x) in allowed_strs)
            df_f= df_f[df_f[col].apply(keeper)]
        return df_f

    def on_heading_click(self, col_name: str):
        if col_name in self.FILTERABLE:
            self.show_filter_popup(col_name)

    def show_filter_popup(self, col_name: str):
        if self.df.empty or col_name not in self.df.columns:
            return
        popup= tk.Toplevel(self)
        popup.title(f"Filter: {col_name}")
        popup.geometry("300x400")

        frame= ctk.CTkFrame(popup)
        frame.pack(fill="both", expand=True, padx=5, pady=5)

        unique_vals= self.df[col_name].unique()
        display_map={}
        reverse_map={}
        for v in unique_vals:
            dsp = str(v)
            sentinel = str(v)
            display_map[v]= dsp
            reverse_map[dsp]= sentinel
        sorted_displays= sorted(display_map.values(), key=lambda x: x.lower())

        curr_filter= self.filters.get(col_name, set())
        all_sentinels= set(reverse_map.values())
        selall_var= tk.BooleanVar(value=(curr_filter==all_sentinels or not curr_filter))

        def toggle_all():
            check= selall_var.get()
            for vb in var_dict.values():
                vb.set(check)

        ctk.CTkCheckBox(
            frame, text="Select All", variable=selall_var, command=toggle_all,
            fg_color="#800020", hover_color="#a52a2a", text_color="black"
        ).pack(anchor="w", pady=5)

        scroll= ctk.CTkScrollableFrame(frame, width=250, height=250)
        scroll.pack(fill="both", expand=True, padx=5, pady=5)
        var_dict={}
        for dsp in sorted_displays:
            sentinel= reverse_map[dsp]
            in_filter= (sentinel in curr_filter) or (not curr_filter)
            bvar= tk.BooleanVar(value=in_filter)
            var_dict[dsp]= bvar
            ctk.CTkCheckBox(
                scroll, text=dsp, variable=bvar,
                fg_color="#800020", hover_color="#a52a2a", text_color="black"
            ).pack(anchor="w")

        def apply_():
            sel= {reverse_map[d] for d,bv in var_dict.items() if bv.get()}
            if sel==all_sentinels or not sel:
                if col_name in self.filters:
                    del self.filters[col_name]
            else:
                self.filters[col_name]= sel
            popup.destroy()
            self.refresh_table()

        bf= ctk.CTkFrame(frame)
        bf.pack(fill="x", pady=5)
        ctk.CTkButton(
            bf, text="Apply", command=apply_,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)
        ctk.CTkButton(
            bf, text="Cancel", command=popup.destroy,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=5)

    def clear_filters(self):
        remove_keys=[]
        for k in self.filters:
            if k in self.FILTERABLE:
                remove_keys.append(k)
        for rk in remove_keys:
            del self.filters[rk]
        self.refresh_table()

    def get_filtered_df(self)-> pd.DataFrame:
        return self.apply_filters()


# ----------------------------------------------------------------------------
# PDF REPORT => 8-charts
# ----------------------------------------------------------------------------
class EnhancedPDFReport:
    def __init__(self, df_current: pd.DataFrame, df_history: pd.DataFrame, config: Dict):
        self.df_current = df_current
        self.df_history = df_history
        self.config = config
        self.page_count = 0
        self.colors = {
            'primary': '#800020',
            'text': '#2C1810',
            'background': '#FFFFFF'
        }
        self.logo_path = self.config["paths"].get("LOGO_PATH","images/company_logo.png")

        self.PAGE_WIDTH = 8.5
        self.PAGE_HEIGHT= 11
        self.CHART_SCALE = 0.7

    def generate(self) -> Path:
        pdf_path= self._get_output_path()
        with PdfPages(pdf_path) as pdf:
            self._cover_page(pdf)
            self._summary_page(pdf)
            self._topdimsattrs_page(pdf)
            self._all_charts(pdf)
        logging.info(f"PDF => {pdf_path}")
        return pdf_path

    def _get_output_path(self)-> Path:
        stamp= datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_dir= Path("Reconciliation_pdf")
        pdf_dir.mkdir(parents=True, exist_ok=True)
        pdf_name= f"Reconciliationpdf_{stamp}.pdf"
        pdf_path= pdf_dir / pdf_name
        return pdf_path

    def _new_page(self)-> plt.Figure:
        self.page_count += 1
        fig= plt.figure(figsize=(self.PAGE_WIDTH,self.PAGE_HEIGHT))
        fig.patch.set_facecolor(self.colors['background'])
        plt.axis('off')
        if self.logo_path and os.path.exists(self.logo_path):
            try:
                import matplotlib.image as mpimg
                img= mpimg.imread(self.logo_path)
                ax_img= fig.add_axes([0.65,0.65, 0.25, 0.25], anchor='NE', zorder=10)
                ax_img.imshow(img, alpha=0.2)
                ax_img.axis('off')
            except Exception as e:
                logging.error(f"Logo => {e}")

        fig.text(0.5, 0.97, "Reconciliation Report", ha='center', fontsize=10, color='gray')
        fig.text(0.9, 0.03, f"Page {self.page_count}", ha='right', fontsize=8, color='gray')
        fig.text(0.5, 0.02, "© Ultra-Mega Reconciliation", ha='center', fontsize=8, color='gray')
        return fig

    def _cover_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.7, "Reconciliation Analysis Report",
                 ha='center', fontsize=24, fontweight='bold', color=self.colors['primary'],
                 transform=fig.transFigure)
        plt.text(0.5, 0.6, f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                 ha='center', fontsize=12, color=self.colors['text'], transform=fig.transFigure)
        plt.text(0.5, 0.1, "CONFIDENTIAL", ha='center', fontsize=9, color=self.colors['text'],
                 transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _summary_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.92, "Reconciliation Summary", ha='center', fontsize=18, fontweight='bold',
                 color=self.colors['primary'], transform=fig.transFigure)
        y= 0.75
        if self.df_current.empty:
            plt.text(0.5, y, "No mismatches found this run.",
                     ha='center', fontsize=14, color=self.colors['text'], transform=fig.transFigure)
        else:
            total= len(self.df_current)
            e= (self.df_current["Gap In"]=="ERP").sum()
            m= (self.df_current["Gap In"]=="MASTER").sum()
            summary= (
                f"Total Mismatches: {total}\n"
                f"Gap=ERP: {e}\n"
                f"Gap=MASTER: {m}"
            )
            plt.text(0.5, y, summary, ha='center', fontsize=14, color=self.colors['text'],
                     transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _topdimsattrs_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.92, "Top Dimensions & Attributes", ha='center', fontsize=18, fontweight='bold',
                 color=self.colors['primary'], transform=fig.transFigure)
        if self.df_current.empty:
            plt.text(0.5, 0.7, "No data available.", ha='center', fontsize=12, color=self.colors['text'],
                     transform=fig.transFigure)
        else:
            if "Dimension" in self.df_current.columns:
                dims= self.df_current["Dimension"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in dims.items()]
                dim_txt= "Top Dimensions:\n" + "\n".join(lines)
                plt.text(0.2, 0.7, dim_txt, fontsize=12, color=self.colors['text'],
                         transform=fig.transFigure)
            if "Attribute" in self.df_current.columns:
                attrs= self.df_current["Attribute"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in attrs.items()]
                attr_txt= "Top Attributes:\n" + "\n".join(lines)
                plt.text(0.6, 0.7, attr_txt, fontsize=12, color=self.colors['text'],
                         transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _chart_page(self, pdf: PdfPages, title: str, plot_func, **kwargs):
        fig= self._new_page()
        fig.suptitle(title, fontsize=14, fontweight='bold', color=self.colors['primary'], y=0.93)

        w_inch= self.PAGE_WIDTH * self.CHART_SCALE
        if "Heatmap" in title:
            h_inch= w_inch * 1.4
        else:
            h_inch= w_inch * (9.0/16.0)

        left_margin= (self.PAGE_WIDTH - w_inch)*0.5 + 0.2
        bottom_margin= (self.PAGE_HEIGHT - h_inch)*0.5

        left_rel= left_margin / self.PAGE_WIDTH
        bottom_rel= bottom_margin / self.PAGE_HEIGHT
        width_rel= w_inch / self.PAGE_WIDTH
        height_rel= h_inch / self.PAGE_HEIGHT

        ax_rect= [left_rel, bottom_rel, width_rel, height_rel]
        ax= fig.add_axes(ax_rect)

        try:
            plot_func(ax, **kwargs)
            pdf.savefig(fig)
        except Exception as e:
            logging.error(f"{title} => {e}")
        finally:
            plt.close(fig)

    def _all_charts(self, pdf: PdfPages):
        dfc= self.df_current.copy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]

        # Heatmap
        if not df_m.empty and {"Dimension","Attribute"}.issubset(df_m.columns):
            pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
            if not pivot.empty:
                self._chart_page(pdf, "Heatmap", self._plot_heatmap, pivot=pivot)

        # Lollipop
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim.empty:
            self._chart_page(pdf, "Lollipop", self._plot_lollipop, cdim=cdim)

        # Circular
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr.empty:
            self._chart_page(pdf, "Circular", self._plot_circular, cattr=cattr)

        # Scatter
        cdim_sc= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim_sc.sort_values("Count", ascending=False, inplace=True)
        cdim_sc= cdim_sc.head(10)
        if not cdim_sc.empty:
            self._chart_page(pdf, "Scatter", self._plot_scatter, cdim=cdim_sc)

        # Radar
        cdim_ra= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim_ra.empty and len(cdim_ra)>1:
            self._chart_page(pdf, "Radar", self._plot_radar, cdim=cdim_ra)

        # Pie
        dist= df_m["Gap In"].value_counts()
        if not dist.empty:
            self._chart_page(pdf, "Pie: Gap In distribution", self._plot_pie, dist=dist)

        # Bar
        cattr_b= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr_b.empty:
            self._chart_page(pdf, "Bar: Gap In attributes", self._plot_bar, cattr=cattr_b)

        # Bollinger
        if not self.df_history.empty and "RunDate" in self.df_history.columns:
            date_ct= self.df_history.groupby("RunDate")["Key"].count().reset_index(name="Count")
            date_ct.sort_values("RunDate", inplace=True)
            if not date_ct.empty:
                self._chart_page(pdf, "Bollinger Band Over Time", self._plot_bollinger, date_ct=date_ct)

    def _plot_heatmap(self, ax, pivot):
        im= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=45, ha="right")
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        plt.colorbar(im, ax=ax)

    def _plot_lollipop(self, ax, cdim):
        ax.hlines(y=cdim.index, xmin=0, xmax=cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_xlabel("Count")

    def _plot_circular(self, ax, cattr):
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)

    def _plot_scatter(self, ax, cdim_sc):
        xvals= np.arange(len(cdim_sc))
        yvals= cdim_sc["Count"].values
        ax.scatter(xvals, yvals, color="green")
        for i, row in cdim_sc.iterrows():
            ax.text(xvals[i], yvals[i], row["Dimension"], ha="center", va="bottom", rotation=60, fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("Count")

    def _plot_radar(self, ax, cdim):
        cat= cdim.index.tolist()
        val= cdim.values.tolist()
        angles= np.linspace(0, 2*np.pi, len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)

    def _plot_pie(self, ax, dist):
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)

    def _plot_bar(self, ax, cattr):
        bars= ax.bar(range(len(cattr)), cattr.values, color="blue")
        ax.set_xticks(range(len(cattr)))
        ax.set_xticklabels(cattr.index, rotation=45, ha="right", fontsize=8)
        ax.set_ylabel("Count")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, f"{int(h)}", ha="center", va="bottom")

    def _plot_bollinger(self, ax, date_ct):
        date_ct["RunDate_dt"] = pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)
        date_ct["rolling_mean"] = date_ct["Count"].rolling(window=3, min_periods=1).mean()
        date_ct["rolling_std"]  = date_ct["Count"].rolling(window=3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]   = date_ct["rolling_mean"] + 2*date_ct["rolling_std"]
        date_ct["lower_band"]   = date_ct["rolling_mean"] - 2*date_ct["rolling_std"]
        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"], color="blue", alpha=0.2, label="±2σ Band")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Count")
        ax.set_xticks(xvals)
        xlabels = [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_title("Bollinger Band Over Time")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Count")
        ax.legend()


# ----------------------------------------------------------------------------
# ADVANCED DASHBOARD with Bollinger chart
# ----------------------------------------------------------------------------
class AdvancedDashboard(ctk.CTkFrame):
    def __init__(self, parent, config: Dict):
        super().__init__(parent)
        dash_cfg= config.get("dashboard", {})
        self.selected_dims= set(dash_cfg.get("selected_dims", []))
        self.selected_attrs= set(dash_cfg.get("selected_attrs", []))
        self.top_n= dash_cfg.get("top_n", 10)

        self.df_current= pd.DataFrame()
        self.df_history= pd.DataFrame()

        topbar= ctk.CTkFrame(self)
        topbar.pack(fill="x", pady=5)

        self.metric_label= ctk.CTkLabel(topbar, text="Metrics: 0 missing, 0 dimension", width=300)
        self.metric_label.pack(side="left", padx=5)

        ctk.CTkButton(
            topbar, text="Toggle Top 10 / All", command=self.toggle_top_n,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=10)

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        self.chart_frames = []
        self.chart_titles = [
            "Heatmap","Lollipop","Circular","Scatter","Radar",
            "Pie","Bar","Bollinger"
        ]
        for title in self.chart_titles:
            fr= ctk.CTkFrame(self.notebook)
            fr.pack(fill="both", expand=True)
            self.notebook.add(fr, text=title)
            self.chart_frames.append(fr)

        self.figures = [None]*8
        self.canvases = [None]*8

    def toggle_top_n(self):
        if self.top_n==10:
            self.top_n= None
        else:
            self.top_n= 10
        self.update_data_filters()

    def set_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current= df_current.copy()
        self.df_history= df_history.copy()
        self.update_data_filters()

    def update_data_filters(self):
        mism= len(self.df_current)
        dims= self.df_current["Dimension"].nunique() if not self.df_current.empty and "Dimension" in self.df_current.columns else 0
        self.metric_label.configure(text=f"Metrics: {mism} missing, {dims} dimension")
        self.draw_charts()

    def draw_charts(self):
        dfc= self.df_current.copy()
        if dfc.empty:
            for i in range(len(self.chart_frames)):
                self.clear_chart(i, "No data.")
            return
        df_m= dfc[dfc["Gap In"]!=""]

        # Heatmap
        idx=0
        if not df_m.empty and {"Dimension","Attribute"}.issubset(df_m.columns):
            pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
            if not pivot.empty:
                self.show_heatmap(idx, pivot)
            else:
                self.clear_chart(idx, "No pivot data.")
        else:
            self.clear_chart(idx, "Not enough columns for Heatmap.")

        # Lollipop
        idx=1
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cdim= cdim.head(self.top_n)
        if not cdim.empty:
            self.show_lollipop(idx, cdim)
        else:
            self.clear_chart(idx, "No dimension data.")

        # Circular
        idx=2
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cattr= cattr.head(self.top_n)
        if not cattr.empty:
            self.show_circular(idx, cattr)
        else:
            self.clear_chart(idx, "No attribute data.")

        # Scatter
        idx=3
        cdim_sc= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim_sc.sort_values("Count", ascending=False, inplace=True)
        if self.top_n:
            cdim_sc= cdim_sc.head(self.top_n)
        if not cdim_sc.empty:
            self.show_scatter(idx, cdim_sc)
        else:
            self.clear_chart(idx, "No dimension to scatter.")

        # Radar
        idx=4
        cdim_ra= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cdim_ra= cdim_ra.head(self.top_n)
        if len(cdim_ra)>1:
            self.show_radar(idx, cdim_ra)
        else:
            self.clear_chart(idx, "Not enough data for Radar (need >=2).")

        # Pie
        idx=5
        dist= df_m["Gap In"].value_counts()
        if not dist.empty:
            self.show_pie(idx, dist)
        else:
            self.clear_chart(idx, "No 'Gap In' data.")

        # Bar
        idx=6
        cattr_b= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cattr_b= cattr_b.head(self.top_n)
        if not cattr_b.empty:
            self.show_bar(idx, cattr_b)
        else:
            self.clear_chart(idx, "No attribute data for Bar.")

        # Bollinger
        idx=7
        if not self.df_history.empty and "RunDate" in self.df_history.columns:
            date_ct= self.df_history.groupby("RunDate")["Key"].count().reset_index(name="Count")
            date_ct.sort_values("RunDate", inplace=True)
            if not date_ct.empty:
                self.show_bollinger(idx, date_ct)
            else:
                self.clear_chart(idx, "No historical data.")
        else:
            self.clear_chart(idx, "No RunDate in history.")

    def clear_chart(self, i: int, msg="No data"):
        if i < len(self.canvases) and self.canvases[i]:
            self.canvases[i].get_tk_widget().destroy()
        self.canvases[i]= None
        self.figures[i]= None
        for w in self.chart_frames[i].winfo_children():
            w.destroy()
        lbl= ctk.CTkLabel(self.chart_frames[i], text=msg)
        lbl.pack(expand=True)

    def create_canvas_ax(self, i: int):
        for w in self.chart_frames[i].winfo_children():
            w.destroy()
        fig= plt.Figure(figsize=(5,3), dpi=100)
        ax= fig.add_subplot(111)
        canvas= FigureCanvasTkAgg(fig, master=self.chart_frames[i])
        canvas_widget= canvas.get_tk_widget()
        canvas_widget.pack(fill="both", expand=True)
        self.canvases[i]= canvas
        self.figures[i]= fig
        return fig, ax, canvas

    def show_heatmap(self, i: int, pivot: pd.DataFrame):
        fig, ax, canvas= self.create_canvas_ax(i)
        im= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=45, ha="right")
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        fig.colorbar(im, ax=ax)
        ax.set_title("Heatmap")
        canvas.draw()

    def show_lollipop(self, i: int, cdim: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        ax.hlines(y=cdim.index, xmin=0, xmax=cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_xlabel("Count")
        ax.set_title("Lollipop - Dimension")
        canvas.draw()

    def show_circular(self, i: int, cattr: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular - Attribute")
        canvas.draw()

    def show_scatter(self, i: int, df: pd.DataFrame):
        fig, ax, canvas= self.create_canvas_ax(i)
        xvals= np.arange(len(df))
        yvals= df["Count"].values
        ax.scatter(xvals, yvals, color="green")
        for j, row in df.iterrows():
            ax.text(xvals[j], yvals[j], row["Dimension"], ha="center", va="bottom", rotation=60, fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("Count")
        ax.set_title("Scatter - Dimension")
        canvas.draw()

    def show_radar(self, i: int, s: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        cat= s.index.tolist()
        val= s.values.tolist()
        angles= np.linspace(0, 2*np.pi, len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)
        ax.set_title("Radar - Dimension")
        canvas.draw()

    def show_pie(self, i: int, dist: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Pie - Gap In Distribution")
        canvas.draw()

    def show_bar(self, i: int, s: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        bars= ax.bar(range(len(s)), s.values, color="blue")
        ax.set_xticks(range(len(s)))
        ax.set_xticklabels(s.index, rotation=45, ha="right", fontsize=8)
        ax.set_ylabel("Count")
        ax.set_title("Bar - Attribute")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, f"{int(h)}", ha="center", va="bottom")
        canvas.draw()

    def show_bollinger(self, i: int, date_ct: pd.DataFrame):
        fig, ax, canvas= self.create_canvas_ax(i)
        date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)
        date_ct["rolling_mean"]= date_ct["Count"].rolling(window=3, min_periods=1).mean()
        date_ct["rolling_std"]= date_ct["Count"].rolling(window=3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]= date_ct["rolling_mean"]+ 2*date_ct["rolling_std"]
        date_ct["lower_band"]= date_ct["rolling_mean"]- 2*date_ct["rolling_std"]
        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"], color="blue", alpha=0.2, label="±2σ")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Actual Count")
        ax.set_xticks(xvals)
        xlabels = [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Missing Count")
        ax.set_title("Bollinger Over Time")
        ax.legend()
        canvas.draw()


# ----------------------------------------------------------------------------
# HISTORY
# ----------------------------------------------------------------------------
class HistoryTab(ctk.CTkFrame):
    def __init__(self, parent, hist_dir: Path):
        super().__init__(parent)
        self.history_dir= hist_dir
        self.tree= None
        self.build_ui()

    def build_ui(self):
        lbl= ctk.CTkLabel(self, text="Reconciliation Runs History", font=("Arial",16))
        lbl.pack(pady=5)

        self.tree= ttk.Treeview(self, columns=("Filename",), show="headings", height=15)
        self.tree.heading("Filename", text="History File")
        self.tree.pack(fill="both", expand=True, padx=10, pady=10)

        self.tree.bind("<Double-1>", self.on_double_click)

        refresh_btn= ctk.CTkButton(self, text="Refresh", command=self.refresh_history,
                                   fg_color="#800020", hover_color="#a52a2a", text_color="white")
        refresh_btn.pack(pady=5)

        self.refresh_history()

    def refresh_history(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if not self.history_dir.is_dir():
            self.history_dir.mkdir(parents=True, exist_ok=True)
        files= sorted(self.history_dir.glob("*.json"), reverse=True)
        for f in files:
            self.tree.insert("", "end", values=(f.name,))

    def on_double_click(self, event):
        item_id= self.tree.focus()
        if not item_id:
            return
        filename= self.tree.item(item_id,"values")[0]
        path= self.history_dir / filename
        if not path.is_file():
            return
        try:
            with open(path, "r", encoding="utf-8") as f:
                content= f.read()
            popup= tk.Toplevel(self)
            popup.title(f"Viewing {filename}")
            txt= ctk.CTkTextbox(popup, width=800, height=600)
            txt.pack(fill="both", expand=True)
            txt.insert("end", content)
            txt.configure(state="disabled")
        except Exception as e:
            logging.error(f"Error opening {path} => {e}")


# ----------------------------------------------------------------------------
# MAIN APP => 2 sheets for missing items, but Dashboard uses Mismatch only.
# ----------------------------------------------------------------------------
class MainApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation: Two-Sheet Edition w/ Bollinger")
        self.geometry("1600x900")
        ctk.set_appearance_mode("light")

        self.protocol("WM_DELETE_WINDOW", self.on_close)

        self.config_dict= load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        self.param_dict= read_param_file(Path(self.config_dict["paths"].get("PARAMETER_PATH",DEFAULT_PATHS["PARAMETER_PATH"])))
        self.history_df= pd.DataFrame()
        self.hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH","history_runs"))

        self.tabs= ttk.Notebook(self)
        self.tabs.pack(fill="both", expand=True)

        # 1) Paths
        self.tab_paths= ctk.CTkFrame(self.tabs)
        self.build_paths_tab(self.tab_paths)
        self.tabs.add(self.tab_paths, text="Paths")

        # 2) ERP preview
        self.tab_erp= ctk.CTkFrame(self.tabs)
        erp_filters= self.config_dict.get("erp_grid",{}).get("filters",{})
        self.erp_preview= SimplePreview(self.tab_erp,"ERP",filters_dict=erp_filters)
        self.erp_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_erp, text="ERP Preview")

        # 3) Master preview
        self.tab_master= ctk.CTkFrame(self.tabs)
        mast_filters= self.config_dict.get("master_grid",{}).get("filters",{})
        self.master_preview= SimplePreview(self.tab_master,"Master",filters_dict=mast_filters)
        self.master_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_master, text="Master Preview")

        # 4) Compare
        self.tab_compare= ctk.CTkFrame(self.tabs)
        self.build_compare_tab(self.tab_compare)
        self.tabs.add(self.tab_compare, text="Compare")

        # 5) Dashboard
        self.dashboard_tab= AdvancedDashboard(self.tabs, self.config_dict)
        self.tabs.add(self.dashboard_tab, text="Dashboard")

        # 6) History
        self.history_tab= HistoryTab(self.tabs, self.hist_path)
        self.tabs.add(self.history_tab, text="History")

        self.log_box= ctk.CTkTextbox(self, height=120)
        self.log_box.pack(fill="both", side="bottom")
        self.log_box.configure(state="disabled")
        handler= TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        self.temp_csv_dir= Path(self.config_dict["paths"].get("MASTER_CSV_OUTPUT","temp_master_csv"))
        self.temp_csv_dir.mkdir(parents=True, exist_ok=True)

        self.load_all_runs()

        self.refresh_erp()
        self.refresh_master()

        ctk.CTkButton(self, text="Close Script", command=self.on_close,
                      fg_color="#800020", hover_color="#a52a2a", text_color="white").pack(pady=5)

        # push entire history => dash
        self.dashboard_tab.set_data(pd.DataFrame(), self.history_df)

    def build_paths_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)

        # We only show selected paths, hiding others.
        self.erp_var= tk.StringVar(value=self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mast_var= tk.StringVar(value=self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_var= tk.StringVar(value=self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_var= tk.StringVar(value=self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))

        def mkrow(lbl, var, is_dir=False):
            rowf= ctk.CTkFrame(frm)
            rowf.pack(fill="x", pady=5)
            ctk.CTkLabel(rowf, text=lbl, width=180).pack(side="left", padx=5)
            e= ctk.CTkEntry(rowf, textvariable=var, width=600)
            e.pack(side="left", padx=5)
            def br():
                if is_dir:
                    p= filedialog.askdirectory()
                else:
                    p= filedialog.askopenfilename()
                if p:
                    var.set(p)
            ctk.CTkButton(rowf, text="Browse", command=br,
                          fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)

        mkrow("ERP Excel:", self.erp_var)
        mkrow("Master ZIP:", self.mast_var)
        mkrow("Exception Path:", self.exc_var)
        mkrow("Missing Items Output:", self.out_var)

        bf= ctk.CTkFrame(frm)
        bf.pack(fill="x", pady=10)
        ctk.CTkButton(bf, text="Save Config", command=self.save_all_config,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Refresh ERP", command=self.refresh_erp,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)
        ctk.CTkButton(bf, text="Refresh Master", command=self.refresh_master,
                      fg_color="#800020", hover_color="#a52a2a").pack(side="left", padx=5)

    def build_compare_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Generate Missing Items", font=("Arial",16)).pack(pady=5)
        ctk.CTkButton(frm, text="Run Reconciliation", command=self.run_comparison,
                      fg_color="#800020", hover_color="#a52a2a", text_color="white").pack(pady=10)
        ctk.CTkButton(frm, text="Export PDF Report", command=self.export_pdf,
                      fg_color="#800020", hover_color="#a52a2a", text_color="white").pack(pady=10)

    def load_all_runs(self):
        if not self.hist_path.is_dir():
            return
        frames=[]
        for jf in self.hist_path.glob("run_*.json"):
            try:
                jdata= pd.read_json(jf, orient="records")
                frames.append(jdata)
            except Exception as e:
                logging.error(f"Error reading {jf} => {e}")
        if frames:
            big= pd.concat(frames, ignore_index=True)
            if self.history_df.empty:
                self.history_df= big
            else:
                self.history_df= pd.concat([self.history_df, big], ignore_index=True)
            self.history_df.drop_duplicates(inplace=True)
            logging.info(f"Loaded all runs => total {len(self.history_df)} records from {self.hist_path}")

    def refresh_erp(self):
        erp_path= Path(self.erp_var.get().strip())
        raw_erp= read_erp_excel(erp_path)
        if raw_erp.empty:
            self.erp_preview.set_data(pd.DataFrame())
            return
        meltdown= meltdown_erp_for_preview(raw_erp, self.param_dict)
        pivoted= pivot_for_preview(meltdown)
        self.erp_preview.set_data(pivoted)

    def refresh_master(self):
        zip_path= Path(self.mast_var.get().strip())
        out_dir= self.temp_csv_dir
        csvs= convert_master_txt_to_csv(zip_path, out_dir)
        raw_mast= unify_master_csvs(csvs)
        if raw_mast.empty:
            self.master_preview.set_data(pd.DataFrame())
            return
        meltdown= meltdown_master_for_preview(raw_mast, self.param_dict)
        pivoted= pivot_for_preview(meltdown)
        self.master_preview.set_data(pivoted)

    def run_comparison(self):
        df_erp_wide= self.erp_preview.get_filtered_df()
        df_mast_wide= self.master_preview.get_filtered_df()

        erp_long= melt_back(df_erp_wide)
        erp_long= build_keys(erp_long)
        mast_long= melt_back(df_mast_wide)
        mast_long= build_keys(mast_long)

        df_diff= compare_mode2(erp_long, mast_long)
        df_exc= read_exception_table(Path(self.exc_var.get().strip()))
        final= merge_exceptions(df_diff, df_exc)

        # separate case diffs
        mismatch, case_df= separate_case_diffs(final)

        out_path= Path(self.out_var.get().strip())
        write_two_sheet_excel(mismatch, case_df, out_path)

        run_timestamp= datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        mismatch["RunDate"]= run_timestamp
        case_df["RunDate"]= run_timestamp
        appended= pd.concat([mismatch, case_df], ignore_index=True)
        if self.history_df.empty:
            self.history_df= appended.copy()
        else:
            self.history_df= pd.concat([self.history_df, appended], ignore_index=True)

        self.hist_path.mkdir(parents=True, exist_ok=True)
        run_file= self.hist_path / f"run_{run_timestamp.replace(':','-').replace(' ','_')}.json"
        try:
            appended.to_json(run_file, orient="records", indent=2)
            logging.info(f"Saved run => {run_file}")
        except Exception as e:
            logging.error(f"Error writing JSON => {e}")

        # The Dashboard only uses the MISMATCH portion for viewing.
        self.dashboard_tab.set_data(mismatch, self.history_df)
        self.history_tab.refresh_history()

        self.tabs.select(self.dashboard_tab)
        messagebox.showinfo("Done", f"Missing items => {out_path}")

    def export_pdf(self):
        if self.history_df.empty:
            messagebox.showinfo("PDF Export", "No mismatch data => history is empty.")
            return
        if "RunDate" in self.history_df.columns:
            last_run= self.history_df["RunDate"].max()
            df_current= self.history_df[self.history_df["RunDate"]== last_run].copy()
        else:
            df_current= self.history_df.copy()
        df_history= self.history_df.copy()
        # We'll only pass mismatch portion or entire? We'll do entire for PDF.
        rep= EnhancedPDFReport(df_current, df_history, self.config_dict)
        pdf_path= rep.generate()
        messagebox.showinfo("PDF Export", f"PDF exported => {pdf_path}")

    def save_all_config(self):
        self.config_dict["paths"]["ERP_EXCEL_PATH"]= self.erp_var.get().strip()
        self.config_dict["paths"]["MASTER_ZIP_PATH"]= self.mast_var.get().strip()
        self.config_dict["paths"]["EXCEPTION_PATH"]= self.exc_var.get().strip()
        self.config_dict["paths"]["OUTPUT_PATH"]= self.out_var.get().strip()
        # hidden ones remain in config but not displayed

        self.config_dict.setdefault("erp_grid", {})
        self.config_dict["erp_grid"]["filters"]= self.erp_preview.filters

        self.config_dict.setdefault("master_grid", {})
        self.config_dict["master_grid"]["filters"]= self.master_preview.filters

        dash_cfg= self.config_dict.setdefault("dashboard", {})
        dash_cfg["selected_dims"]= list(self.dashboard_tab.selected_dims)
        dash_cfg["selected_attrs"]= list(self.dashboard_tab.selected_attrs)
        dash_cfg["top_n"]= self.dashboard_tab.top_n

        path_for_cfg = Path(self.config_dict["paths"].get("CONFIG_PATH","config/ui_config.json"))
        save_config(self.config_dict, path_for_cfg)

    def on_close(self):
        self.save_all_config()
        band_path= self.config_dict["paths"].get("BAND_CHART_JSON_PATH","")
        if band_path and not self.history_df.empty and "RunDate" in self.history_df.columns:
            try:
                outp= Path(band_path)
                date_ct= self.history_df.groupby("RunDate")["Key"].count().reset_index(name="Count")
                date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
                date_ct.sort_values("RunDate_dt", inplace=True)
                date_ct.reset_index(drop=True, inplace=True)
                date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
                date_ct["rolling_std"]= date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
                date_ct["upper_band"]= date_ct["rolling_mean"]+ 2*date_ct["rolling_std"]
                date_ct["lower_band"]= date_ct["rolling_mean"]- 2*date_ct["rolling_std"]
                date_ct["RunDate"]= date_ct["RunDate_dt"].dt.strftime("%Y-%m-%d %H:%M:%S")
                date_ct.drop(columns=["RunDate_dt"], inplace=True)
                date_ct.to_json(outp, orient="records", indent=2)
                logging.info(f"Bollinger data saved on close => {outp}")
            except Exception as e:
                logging.error(f"Bollinger JSON on close => {e}")
        self.destroy()


def main():
    app= MainApp()
    app.mainloop()

if __name__=="__main__":
    main()
