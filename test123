import os
import zipfile
import pandas as pd
from pathlib import Path
import logging
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font

# ---------------------------
# 0) SETUP LOGGING
# ---------------------------
def setup_logging(log_file: Path) -> None:
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    c_handler = logging.StreamHandler()
    f_handler = logging.FileHandler(log_file, mode='w')
    c_handler.setLevel(logging.INFO)
    f_handler.setLevel(logging.DEBUG)
    c_format = logging.Formatter('%(levelname)s: %(message)s')
    f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    c_handler.setFormatter(c_format)
    f_handler.setFormatter(f_format)
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)

# ---------------------------
# 1) PRE-MELT FILTER
# ---------------------------
def filter_pre_melt(df: pd.DataFrame, exclude_rules: list = None) -> pd.DataFrame:
    """
    Drops any row in a DataFrame if a specified column contains any of the bad values.
    """
    if not exclude_rules:
        return df
    combined_mask = pd.Series(False, index=df.index)
    for col, bad_values in exclude_rules:
        if col in df.columns:
            mask = df[col].isin(bad_values)
            logging.debug(f"[Pre-Melt] Excluding {mask.sum()} rows from column '{col}' with values in {bad_values}")
            combined_mask |= mask
        else:
            logging.warning(f"[Pre-Melt] Column '{col}' not found in DataFrame.")
    return df[~combined_mask]

# ---------------------------
# 2) POST-MELT FILTER (Exclude Bad Dimensions/Attributes)
# ---------------------------
def exclude_dimension_attribute(df: pd.DataFrame, bad_dimensions: list = None, bad_attributes: list = None) -> pd.DataFrame:
    """
    Removes rows if their Dimension or Attribute is in the list of bad values.
    """
    if bad_dimensions:
        initial = len(df)
        df = df[~df["Dimension"].isin(bad_dimensions)]
        logging.debug(f"[Post-Melt] Excluded {initial - len(df)} rows based on bad dimensions {bad_dimensions}")
    if bad_attributes:
        initial = len(df)
        df = df[~df["Attribute"].isin(bad_attributes)]
        logging.debug(f"[Post-Melt] Excluded {initial - len(df)} rows based on bad attributes {bad_attributes}")
    return df

# ---------------------------
# 3) TRANSFORM ALFA (EXCEL)
# ---------------------------
def transform_alfa(file_path: Path,
                   pre_melt_exclude_rules: list = None,
                   bad_dimensions: list = None,
                   bad_attributes: list = None,
                   dimension_rename: dict = None,
                   attribute_rename: dict = None,
                   sheet_name: str = "Sheet1",
                   skip_rows: int = 3) -> pd.DataFrame:
    """
    Reads and transforms the Alfa Excel data.
    
    Expects:
      - Headers in row 4.
      - A column 'Dimension_Name' (renamed to 'Dimension').
      - A column 'Name' that is the reference.
      - Other columns are considered attributes.
    
    The function adds a unique RecordID, melts the DataFrame (so that every attribute becomes a row),
    then extracts the reference name (from the row where Attribute == "Name") and creates a GroupKey = "Dimension | RefName".
    """
    try:
        if not file_path.is_file():
            logging.error(f"[Alfa] File not found: {file_path}")
            return pd.DataFrame()
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        logging.info(f"[Alfa] Loaded {len(df)} rows from Excel.")
        
        # Rename 'Dimension_Name' to 'Dimension' if it exists; otherwise assume 3rd column is Dimension.
        if "Dimension_Name" in df.columns:
            df.rename(columns={"Dimension_Name": "Dimension"}, inplace=True)
        else:
            df.rename(columns={df.columns[2]: "Dimension"}, inplace=True)
        
        # Ensure there is a 'Name' column; if not, assume 4th column is Name.
        if "Name" not in df.columns:
            df.rename(columns={df.columns[3]: "Name"}, inplace=True)
        
        # Add a unique RecordID (used to reassemble the original record after melting)
        df["RecordID"] = df.index.astype(str)
        
        # Apply pre-melt filtering.
        df = filter_pre_melt(df, pre_melt_exclude_rules)
        
        # Melt the DataFrame so that every column (other than 'Dimension' and 'RecordID') becomes an attribute.
        id_vars = ["Dimension", "RecordID"]
        value_vars = [col for col in df.columns if col not in id_vars]
        melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name="Attribute", value_name="Value")
        
        # Apply renaming dictionaries if provided.
        if dimension_rename:
            melted["Dimension"] = melted["Dimension"].replace(dimension_rename)
        if attribute_rename:
            melted["Attribute"] = melted["Attribute"].replace(attribute_rename)
        
        # Exclude rows with bad dimensions or attributes.
        melted = exclude_dimension_attribute(melted, bad_dimensions, bad_attributes)
        
        # For each record, determine the reference Name (from the row where Attribute == "Name")
        def get_ref_name(group):
            name_val = group.loc[group["Attribute"] == "Name", "Value"]
            return name_val.iloc[0] if not name_val.empty else ""
        ref_names = melted.groupby("RecordID").apply(get_ref_name)
        melted = melted.merge(ref_names.rename("RefName"), on="RecordID")
        
        # Create a GroupKey for comparison: "Dimension | RefName"
        melted["GroupKey"] = (melted["Dimension"].astype(str).str.strip() +
                              " | " +
                              melted["RefName"].astype(str).str.strip())
        
        # Build a full key string for traceability: "Dimension | RefName | Attribute | Value"
        def build_full_key(row):
            return f"{row['Dimension'].strip()} | {row['RefName'].strip()} | {row['Attribute'].strip()} | {str(row['Value']).strip()}"
        melted["Key"] = melted.apply(build_full_key, axis=1)
        
        melted.drop_duplicates(inplace=True)
        logging.info(f"[Alfa] Transformed data to {len(melted)} rows after melting.")
        return melted
    except Exception as e:
        logging.error(f"[Alfa] Error during transformation: {e}")
        return pd.DataFrame()

# ---------------------------
# 4) TRANSFORM GAMMA (ZIP with .txt files)
# ---------------------------
def transform_gamma(zip_file_path: Path,
                    pre_melt_exclude_rules: list = None,
                    bad_dimensions: list = None,
                    bad_attributes: list = None,
                    dimension_rename: dict = None,
                    attribute_rename: dict = None,
                    delimiter: str = ",",
                    remove_substring: str = "_ceaster.txt") -> pd.DataFrame:
    """
    Reads and transforms the Gamma data from a ZIP file.
    
    Expects:
      - Each .txt file is a CSV.
      - The file name (after removing remove_substring) is the Dimension.
      - The first column is renamed to 'Name' (the reference).
      - Other columns become attributes.
    
    As with Alfa, a RecordID is added, the DataFrame is melted,
    and the reference (RefName) is extracted to create a GroupKey.
    """
    try:
        if not zip_file_path.is_file():
            logging.error(f"[Gamma] ZIP file not found: {zip_file_path}")
            return pd.DataFrame()
        all_dfs = []
        with zipfile.ZipFile(zip_file_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.endswith(".txt")]
            if not txt_files:
                logging.warning("[Gamma] No .txt files found in the ZIP.")
                return pd.DataFrame()
            for txt_file in txt_files:
                try:
                    # Compute Dimension from the filename.
                    base = os.path.basename(txt_file)
                    if remove_substring in base:
                        base = base.replace(remove_substring, "")
                    else:
                        base, _ = os.path.splitext(base)
                    dimension = base.replace("_", " ").strip()
                    
                    # Read CSV data.
                    with z.open(txt_file) as fo:
                        df = pd.read_csv(fo, delimiter=delimiter)
                    if df.empty:
                        logging.warning(f"[Gamma] File {txt_file} is empty. Skipping.")
                        continue
                    
                    # Rename the first column to 'Name'.
                    first_col = df.columns[0]
                    df.rename(columns={first_col: "Name"}, inplace=True)
                    df["Name"] = df["Name"].fillna("Unknown")
                    
                    # Apply pre-melt filtering.
                    df = filter_pre_melt(df, pre_melt_exclude_rules)
                    
                    # Add the Dimension column (from the filename).
                    df["Dimension"] = dimension
                    
                    # Add a unique RecordID.
                    df["RecordID"] = df.index.astype(str)
                    
                    # Melt the DataFrame.
                    id_vars = ["Dimension", "RecordID"]
                    value_vars = [col for col in df.columns if col not in id_vars]
                    melted = df.melt(id_vars=id_vars, value_vars=value_vars, var_name="Attribute", value_name="Value")
                    
                    # Apply renaming if provided.
                    if dimension_rename:
                        melted["Dimension"] = melted["Dimension"].replace(dimension_rename)
                    if attribute_rename:
                        melted["Attribute"] = melted["Attribute"].replace(attribute_rename)
                    
                    # Exclude rows with bad dimensions or attributes.
                    melted = exclude_dimension_attribute(melted, bad_dimensions, bad_attributes)
                    
                    # For each record, determine the reference Name.
                    def get_ref_name(group):
                        name_val = group.loc[group["Attribute"] == "Name", "Value"]
                        return name_val.iloc[0] if not name_val.empty else ""
                    ref_names = melted.groupby("RecordID").apply(get_ref_name)
                    melted = melted.merge(ref_names.rename("RefName"), on="RecordID")
                    
                    # Create GroupKey: "Dimension | RefName"
                    melted["GroupKey"] = (melted["Dimension"].astype(str).str.strip() +
                                          " | " +
                                          melted["RefName"].astype(str).str.strip())
                    
                    # Build full key: "Dimension | RefName | Attribute | Value"
                    def build_full_key(row):
                        return f"{row['Dimension'].strip()} | {row['RefName'].strip()} | {row['Attribute'].strip()} | {str(row['Value']).strip()}"
                    melted["Key"] = melted.apply(build_full_key, axis=1)
                    
                    melted.drop_duplicates(inplace=True)
                    logging.info(f"[Gamma] Processed {txt_file} into {len(melted)} rows.")
                    all_dfs.append(melted)
                except Exception as inner_e:
                    logging.error(f"[Gamma] Error processing file {txt_file}: {inner_e}")
                    continue
        if all_dfs:
            df_gamma = pd.concat(all_dfs, ignore_index=True)
            logging.info(f"[Gamma] Combined transformed data has {len(df_gamma)} rows.")
            return df_gamma
        else:
            logging.warning("[Gamma] No valid data found in ZIP.")
            return pd.DataFrame()
    except Exception as e:
        logging.error(f"[Gamma] Error during transformation: {e}")
        return pd.DataFrame()

# ---------------------------
# 5) CREATE MISSING ITEMS EXCEL
# ---------------------------
def create_missing_items_excel(df_alfa: pd.DataFrame,
                               df_gamma: pd.DataFrame,
                               df_exceptions: pd.DataFrame,
                               output_path: Path) -> None:
    """
    Compares the transformed Alfa and Gamma data (grouped by GroupKey = "Dimension | RefName")
    and outputs only the differences (i.e. the missing attributes).
    
    Comparison logic:
      - For each record (GroupKey):
            • If one source does not contain the record (or its reference Name is missing),
              then output only the missing "Name" row.
            • Otherwise, compare the dictionaries of attributes.
              For each attribute (other than "Name"):
                - If an attribute exists in one source but not the other,
                  output that attribute row (as missing in the side where it is absent).
                - If an attribute exists in both but with different values,
                  output a row for each side (so you see, for example, that in Gamma "Type" is blank while in Alfa "Type" is "1").
    
    The resulting DataFrame is then merged with an exceptions table (if provided)
    and written to an Excel file with color formatting (blue for missing in Alfa, green for missing in Gamma).
    """
    try:
        # Build dictionaries for each record (GroupKey) from each source.
        def build_attr_dict(df):
            groups = {}
            for group_key, group_df in df.groupby("GroupKey"):
                attr_dict = {}
                # If multiple entries exist for an attribute, take the first.
                for attr, sub_df in group_df.groupby("Attribute"):
                    attr_dict[attr] = sub_df["Value"].iloc[0]
                groups[group_key] = attr_dict
            return groups
        
        groups_alfa = build_attr_dict(df_alfa)
        groups_gamma = build_attr_dict(df_gamma)
        
        all_group_keys = set(groups_alfa.keys()).union(set(groups_gamma.keys()))
        missing_items = []
        
        for group_key in all_group_keys:
            a_dict = groups_alfa.get(group_key)
            g_dict = groups_gamma.get(group_key)
            
            # Parse GroupKey into Dimension and RefName for output.
            parts = group_key.split(" | ")
            dimension = parts[0] if len(parts) > 0 else ""
            ref_name = parts[1] if len(parts) > 1 else ""
            
            # If one source is completely missing the record, report only the missing "Name".
            if a_dict is None and g_dict is not None:
                if "Name" in g_dict:
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": g_dict["Name"],
                        "Attribute": "Name",
                        "Value": g_dict["Name"],
                        "Missing In": "Alfa"
                    })
                continue
            if g_dict is None and a_dict is not None:
                if "Name" in a_dict:
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": a_dict["Name"],
                        "Attribute": "Name",
                        "Value": a_dict["Name"],
                        "Missing In": "Gamma"
                    })
                continue
            
            # Both records exist. If the reference "Name" is missing in one, report that and ignore other attributes.
            if "Name" not in a_dict or "Name" not in g_dict:
                if "Name" not in a_dict and "Name" in g_dict:
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": g_dict["Name"],
                        "Attribute": "Name",
                        "Value": g_dict.get("Name", ""),
                        "Missing In": "Alfa"
                    })
                if "Name" not in g_dict and "Name" in a_dict:
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": a_dict["Name"],
                        "Attribute": "Name",
                        "Value": a_dict.get("Name", ""),
                        "Missing In": "Gamma"
                    })
                continue
            
            # Both have a reference Name; now compare other attributes.
            all_attrs = set(a_dict.keys()).union(set(g_dict.keys()))
            all_attrs.discard("Name")  # exclude the reference attribute
            for attr in all_attrs:
                a_val = a_dict.get(attr)
                g_val = g_dict.get(attr)
                if a_val is None and g_val is not None:
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": a_dict["Name"],
                        "Attribute": attr,
                        "Value": g_val,
                        "Missing In": "Alfa"
                    })
                elif g_val is None and a_val is not None:
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": a_dict["Name"],
                        "Attribute": attr,
                        "Value": a_val,
                        "Missing In": "Gamma"
                    })
                elif a_val != g_val:
                    # Report the differing values as missing on each side.
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": a_dict["Name"],
                        "Attribute": attr,
                        "Value": a_val,
                        "Missing In": "Gamma"
                    })
                    missing_items.append({
                        "Dimension": dimension,
                        "Name": a_dict["Name"],
                        "Attribute": attr,
                        "Value": g_val,
                        "Missing In": "Alfa"
                    })
        
        df_missing = pd.DataFrame(missing_items)
        logging.info(f"[Missing Items] Total missing items: {len(df_missing)}")
        
        if df_missing.empty:
            logging.info("[Missing Items] No missing items found.")
            return
        
        # Merge with exceptions table if provided.
        if not df_exceptions.empty:
            # Create a Key column for merging.
            df_missing["Key"] = (df_missing["Dimension"].str.strip() + " | " +
                                 df_missing["Name"].str.strip() + " | " +
                                 df_missing["Attribute"].str.strip() + " | " +
                                 df_missing["Value"].astype(str).str.strip())
            df_exceptions["Key"] = df_exceptions["Key"].astype(str).str.strip()
            df_missing = df_missing.merge(df_exceptions, on="Key", how="left")
            df_missing["hide exception"] = df_missing["hide exception"].astype(str).str.lower().fillna("no")
            df_missing = df_missing[df_missing["hide exception"] != "yes"]
        
        # Add an Action Item column.
        df_missing["Action Item"] = ""
        final_columns = ["Dimension", "Name", "Attribute", "Value", "Comments_1", "Comments_2", "Action Item", "Missing In"]
        df_missing = df_missing.reindex(columns=final_columns)
        
        # Write the output to Excel.
        df_missing.to_excel(output_path, sheet_name="Missing_Items", index=False)
        logging.info(f"[Missing Items] Written output to {output_path}")
        
        # --- Apply Color Formatting ---
        try:
            wb = load_workbook(output_path)
            ws = wb["Missing_Items"]
            header_font = Font(bold=True)
            fill_header = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
            fill_gamma = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # green (indicates missing in Alfa)
            fill_alfa = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")   # blue (indicates missing in Gamma)

            header_row = next(ws.iter_rows(min_row=1, max_row=1))
            headers = {cell.value: cell.column for cell in header_row}
            for cell in header_row:
                cell.font = header_font
                cell.fill = fill_header

            required_cols = ["Dimension", "Name", "Attribute", "Value", "Missing In"]
            if not all(col in headers for col in required_cols):
                logging.warning("[Missing Items] One or more required columns for coloring are missing.")
            else:
                for row_idx in range(2, ws.max_row + 1):
                    missing_in = str(ws.cell(row=row_idx, column=headers["Missing In"]).value).strip().lower()
                    if missing_in == "gamma":
                        fill_color = fill_gamma
                    elif missing_in == "alfa":
                        fill_color = fill_alfa
                    else:
                        continue
                    ws.cell(row=row_idx, column=headers["Dimension"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Name"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Attribute"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Value"]).fill = fill_color

            ws.freeze_panes = ws["A2"]
            wb.save(output_path)
            logging.info("[Missing Items] Applied color formatting to Excel.")
        except Exception as e:
            logging.error(f"[Missing Items] Error applying color formatting: {e}")
    except Exception as e:
        logging.error(f"[Missing Items] Error during creation of missing items Excel: {e}")

# ---------------------------
# 6) READ EXCEPTION TABLE
# ---------------------------
def read_exception_table(exception_file: Path) -> pd.DataFrame:
    """
    Reads an Excel file containing exceptions.
    Expected columns: Key, Comments_1, Comments_2, hide exception.
    """
    try:
        if not exception_file.is_file():
            logging.warning(f"[Exception] Exception file {exception_file} not found.")
            return pd.DataFrame()
        df = pd.read_excel(exception_file, sheet_name="Sheet1")
        required_cols = {"Key", "Comments_1", "Comments_2", "hide exception"}
        if not required_cols.issubset(set(df.columns)):
            missing = required_cols - set(df.columns)
            logging.warning(f"[Exception] Missing required columns: {missing}")
            return pd.DataFrame()
        logging.info(f"[Exception] Loaded {len(df)} exception rows.")
        return df
    except Exception as e:
        logging.error(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame()

# ---------------------------
# 7) MAIN FUNCTION
# ---------------------------
def main() -> None:
    try:
        log_file = Path("script.log")
        setup_logging(log_file)
        logging.info("Script started.")
        
        # Read exception table (optional).
        exception_file = Path("Exception_Table.xlsx")
        df_exceptions = read_exception_table(exception_file)
        
        # --- ALFA configuration ---
        alfa_file = Path("AlfaData.xlsx")
        alfa_pre_exclude = [("SomeColumn", ["BadValue"])]   # adjust filtering as needed
        alfa_bad_dims = ["UnwantedDim"]
        alfa_bad_attrs = ["Debug"]
        alfa_dimension_rename = {"DimOld": "DimNew"}
        alfa_attribute_rename = {"First": "Name"}  # if renaming is required
        df_alfa = transform_alfa(
            alfa_file,
            pre_melt_exclude_rules=alfa_pre_exclude,
            bad_dimensions=alfa_bad_dims,
            bad_attributes=alfa_bad_attrs,
            dimension_rename=alfa_dimension_rename,
            attribute_rename=alfa_attribute_rename,
            sheet_name="Sheet1",
            skip_rows=3
        )
        logging.info(f"[Alfa] Transformed data contains {len(df_alfa)} rows.")
        
        # --- GAMMA configuration ---
        gamma_zip = Path("GammaData.zip")
        gamma_pre_exclude = [("SomeColumn", ["BadValue"])]
        gamma_bad_dims = ["TestDim"]
        gamma_bad_attrs = ["BadAttr"]
        gamma_dimension_rename = {"GammaOld": "GammaNew"}
        gamma_attribute_rename = {"First": "Name"}
        df_gamma = transform_gamma(
            gamma_zip,
            pre_melt_exclude_rules=gamma_pre_exclude,
            bad_dimensions=gamma_bad_dims,
            bad_attributes=gamma_bad_attrs,
            dimension_rename=gamma_dimension_rename,
            attribute_rename=gamma_attribute_rename,
            delimiter=",",
            remove_substring="_ceaster.txt"
        )
        logging.info(f"[Gamma] Transformed data contains {len(df_gamma)} rows.")
        
        # --- Create Missing Items Excel Report ---
        output_file = Path("Missing_Items.xlsx")
        create_missing_items_excel(df_alfa, df_gamma, df_exceptions, output_file)
        
        logging.info("Script completed successfully.")
    except Exception as e:
        logging.critical(f"[Main] Critical error: {e}")

if __name__ == "__main__":
    main()
