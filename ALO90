import os
import zipfile
import pandas as pd
from pathlib import Path
import logging
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font

# -----------------------------------------
# 0) CONFIGURE LOGGING
# -----------------------------------------
def setup_logging(log_file: Path):
    """
    Configures logging to output to both console and a log file.
    """
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Create handlers
    c_handler = logging.StreamHandler()
    f_handler = logging.FileHandler(log_file, mode='w')

    c_handler.setLevel(logging.INFO)
    f_handler.setLevel(logging.DEBUG)

    # Create formatters
    c_format = logging.Formatter('%(levelname)s: %(message)s')
    f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # Add formatters to handlers
    c_handler.setFormatter(c_format)
    f_handler.setFormatter(f_format)

    # Add handlers to the logger
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)

# -----------------------------------------
# 1) READ EXCLUSION TABLE
# -----------------------------------------
def read_exclusion_table(ex_table_path: Path) -> set:
    """
    Reads an Excel file with columns: [Key, hide exception].
    Returns a set of keys to exclude where hide exception == 'yes'.
    """
    try:
        if not ex_table_path.is_file():
            logging.warning(f"[Exclusion] File not found: {ex_table_path}. No exclusions used.")
            return set()

        df_ex = pd.read_excel(ex_table_path, sheet_name="Sheet1")
        if "Key" not in df_ex.columns or "hide exception" not in df_ex.columns:
            logging.warning("[Exclusion] Missing 'Key' or 'hide exception' columns. No exclusions.")
            return set()

        mask = df_ex["hide exception"].astype(str).str.lower() == "yes"
        excluded_keys = set(df_ex.loc[mask, "Key"].dropna().unique())
        logging.info(f"[Exclusion] Excluded Keys Count: {len(excluded_keys)}")
        return excluded_keys
    except Exception as e:
        logging.error(f"[Exclusion] Error reading exclusion table: {e}")
        return set()

# -----------------------------------------
# 1a) READ EXCEPTION TABLE
# -----------------------------------------
def read_exception_table(exception_table_path: Path) -> pd.DataFrame:
    """
    Reads an Excel file with columns: [Key, Comments_1, Comments_2, hide exception].
    Returns a DataFrame with these columns. Rows where 'hide exception' == 'yes'
    can be filtered out later in the process.
    """
    try:
        if not exception_table_path.is_file():
            logging.warning(f"[Exception] File not found: {exception_table_path}. No comments added.")
            return pd.DataFrame(columns=["Key", "Comments_1", "Comments_2", "hide exception"])

        df_exc = pd.read_excel(exception_table_path, sheet_name="Sheet1")
        required_columns = {"Key", "Comments_1", "Comments_2", "hide exception"}
        if not required_columns.issubset(df_exc.columns):
            missing_cols = required_columns - set(df_exc.columns)
            logging.warning(f"[Exception] Missing columns {missing_cols}. No comments added.")
            return pd.DataFrame(columns=list(required_columns))

        logging.info(f"[Exception] Loaded comments for {len(df_exc)} keys.")
        return df_exc
    except Exception as e:
        logging.error(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame(columns=["Key", "Comments_1", "Comments_2", "hide exception"])

# -----------------------------------------
# 2) PRE-MELT FILTER (Remove Rows by Column)
# -----------------------------------------
def filter_pre_melt(df: pd.DataFrame, exclude_rules: list = None) -> pd.DataFrame:
    """
    exclude_rules: list of (columnName, [values_to_remove]).
      We remove any row if df[columnName] is in values_to_remove (logical OR across rules).
    Example: [("ColA", ["X"]), ("ColB", ["Y"])] => remove if ColA=="X" OR ColB=="Y".
    """
    try:
        if not exclude_rules:
            return df  # No filtering if none

        combined_mask = pd.Series(False, index=df.index)
        for (col, bad_values) in exclude_rules:
            if col in df.columns:
                mask = df[col].isin(bad_values)
                logging.debug(f"[Pre-Melt] Excluding rows where {col} in {bad_values}: {mask.sum()} rows")
                combined_mask |= mask
            else:
                logging.warning(f"[Pre-Melt] Column '{col}' not found in DataFrame.")

        # Keep rows where combined_mask is False
        df_out = df[~combined_mask]
        logging.info(f"[Pre-Melt] Rows after filtering: {len(df_out)}")
        return df_out
    except Exception as e:
        logging.error(f"[Pre-Melt] Error during pre-melt filtering: {e}")
        return df

# -----------------------------------------
# 3) POST-MELT EXCLUDE (Dimension/Attribute)
# -----------------------------------------
def exclude_dimension_attribute(df: pd.DataFrame,
                                bad_dimensions: list = None,
                                bad_attributes: list = None) -> pd.DataFrame:
    """
    Remove any row whose Dimension is in bad_dimensions OR Attribute is in bad_attributes.
    """
    try:
        initial_count = len(df)
        if bad_dimensions:
            df = df[~df["Dimension"].isin(bad_dimensions)]
            excluded_dims = initial_count - len(df)
            logging.debug(f"[Post-Melt] Excluded Dimensions {bad_dimensions}: {excluded_dims} rows removed")
            initial_count = len(df)

        if bad_attributes:
            df = df[~df["Attribute"].isin(bad_attributes)]
            excluded_attrs = initial_count - len(df)
            logging.debug(f"[Post-Melt] Excluded Attributes {bad_attributes}: {excluded_attrs} rows removed")

        logging.info(f"[Post-Melt] Rows after exclusion: {len(df)}")
        return df
    except Exception as e:
        logging.error(f"[Post-Melt] Error during post-melt exclusion: {e}")
        return df

# -----------------------------------------
# 4) TRANSFORM ALFA
# -----------------------------------------
def transform_alfa(
    file_path: Path,
    excluded_keys: set,
    pre_melt_exclude_rules: list = None,
    post_melt_bad_dimensions: list = None,
    post_melt_bad_attributes: list = None,
    dimension_rename_dict: dict = None,
    attribute_rename_dict: dict = None,
    sheet_name: str = "Sheet1",
    skip_rows: int = 3
) -> pd.DataFrame:
    """
    Transforms the Alfa (Excel) data.
    """
    try:
        if not file_path.is_file():
            logging.error(f"[Alfa] File not found: {file_path}")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        # 1) Read Excel
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        logging.debug(f"[Alfa] Initial rows: {len(df)}")
        if df.shape[1] < 4:
            logging.warning("[Alfa] Fewer than 4 columns. Returning empty DataFrame.")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        # 2) Pre-melt exclude
        df = filter_pre_melt(df, pre_melt_exclude_rules)

        # 3) Rename columns => col[2]->Dimension, col[3]->NameID
        df.rename(columns={
            df.columns[2]: "Dimension",
            df.columns[3]: "NameID"
        }, inplace=True)
        logging.debug(f"[Alfa] Columns after renaming: {df.columns.tolist()}")

        # 4) Create 'Name' column equal to 'NameID' for inclusion as an attribute
        df['Name'] = df['NameID']
        logging.debug(f"[Alfa] Added 'Name' column:\n{df[['NameID', 'Name']].head()}")

        # 5) Melt
        id_vars = ["Dimension", "NameID"]
        val_vars = [c for c in df.columns if c not in id_vars]
        logging.debug(f"[Alfa] id_vars: {id_vars}, val_vars: {val_vars}")

        df_melt = df.melt(
            id_vars=id_vars,
            value_vars=val_vars,
            var_name="Attribute",
            value_name="Value"
        )
        logging.debug(f"[Alfa] Rows after melt: {len(df_melt)}")

        # 6) Rename dimension/attribute if needed
        if dimension_rename_dict:
            df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
            logging.debug(f"[Alfa] Dimensions after renaming: {df_melt['Dimension'].unique()}")
        if attribute_rename_dict:
            df_melt["Attribute"] = df_melt["Attribute"].replace(attribute_rename_dict)
            logging.debug(f"[Alfa] Attributes after renaming: {df_melt['Attribute'].unique()}")

        # 7) Exclude certain Dimensions or Attributes
        df_melt = exclude_dimension_attribute(
            df_melt,
            bad_dimensions=post_melt_bad_dimensions,
            bad_attributes=post_melt_bad_attributes
        )

        # 8) Build Key without NameID for grouping
        df_melt["Key_NoName"] = df_melt.apply(
            lambda row: f"{row['Dimension']} | {row['Attribute']} | {row['Value']}",
            axis=1
        )

        # 9) Exclude rows based on Keys
        before_exclusion = len(df_melt)
        df_melt = df_melt[~df_melt["Key_NoName"].isin(excluded_keys)]
        after_exclusion = len(df_melt)
        logging.info(f"[Alfa] Excluded {before_exclusion - after_exclusion} rows based on excluded_keys.")

        # 10) Create a 'Key' column (Dimension | NameID | Attribute | Value)
        df_melt["Key"] = df_melt.apply(
            lambda row: f"{row['Dimension']} | {row['NameID']} | {row['Attribute']} | {row['Value']}",
            axis=1
        )

        # 11) Remove duplicates if necessary
        before_dedup = len(df_melt)
        df_melt.drop_duplicates(subset=["Key", "Key_NoName"], inplace=True)
        after_dedup = len(df_melt)
        logging.info(f"[Alfa] Removed {before_dedup - after_dedup} duplicate rows.")

        # 12) Final DataFrame
        final_df = df_melt[["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"]]
        logging.info(f"[Alfa] Final rows: {len(final_df)}")
        return final_df
    except Exception as e:
        logging.error(f"[Alfa] Error during transformation: {e}")
        return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

# -----------------------------------------
# 5) TRANSFORM GAMMA (Similar to ALFA)
# -----------------------------------------
def transform_gamma(
    zip_file_path: Path,
    excluded_keys: set,
    pre_melt_exclude_rules: list = None,
    post_melt_bad_dimensions: list = None,
    post_melt_bad_attributes: list = None,
    dimension_rename_dict: dict = None,
    attribute_rename_dict: dict = None,
    delimiter: str = ",",
    remove_substring: str = "_ceaster.txt"
) -> pd.DataFrame:
    """
    Transforms the Gamma (ZIP) data.
    """
    try:
        if not zip_file_path.is_file():
            logging.error(f"[Gamma] ZIP not found: {zip_file_path}")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        def compute_dimension_name(filename: str, remove_sub: str) -> str:
            base = os.path.basename(filename)
            if remove_sub in base:
                base = base.replace(remove_sub, "")
            else:
                base, _ = os.path.splitext(base)
            return base.replace("_", " ")

        all_dfs = []
        with zipfile.ZipFile(zip_file_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.endswith(".txt")]
            if not txt_files:
                logging.warning("[Gamma] No .txt files found in the ZIP.")
                return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

            for txt_file in txt_files:
                try:
                    # 1. Compute Dimension from filename
                    dimension_name = compute_dimension_name(txt_file, remove_substring)
                    logging.debug(f"[Gamma] Processing file: {txt_file}, Dimension: {dimension_name}")

                    # 2. Read the file into a DataFrame
                    with z.open(txt_file) as fo:
                        df_in = pd.read_csv(fo, delimiter=delimiter)
                    logging.debug(f"[Gamma] Initial rows in {txt_file}: {len(df_in)}")

                    if df_in.empty:
                        logging.warning(f"[Gamma] File {txt_file} is empty. Skipping.")
                        continue

                    logging.debug(f"[Gamma] Columns in {txt_file}: {df_in.columns.tolist()}")

                    # 3. Rename the first column to 'NameID'
                    first_col = df_in.columns[0]
                    df_in.rename(columns={first_col: "NameID"}, inplace=True)

                    # 4. Handle missing NameID values
                    if df_in["NameID"].isnull().any():
                        logging.warning(f"[Gamma] Some NameID values are blank in {txt_file}. Filling with 'Unknown'.")
                        df_in["NameID"] = df_in["NameID"].fillna("Unknown")

                    # 5. Apply Pre-Melt Exclusions
                    df_in = filter_pre_melt(df_in, pre_melt_exclude_rules)
                    logging.debug(f"[Gamma] Rows after pre-melt filtering in {txt_file}: {len(df_in)}")

                    # 6. Add Dimension Column
                    df_in["Dimension"] = dimension_name

                    # 7. Create 'Name' column
                    df_in['Name'] = df_in['NameID']

                    # 8. Melt Other Columns into Attributes
                    id_vars = ["Dimension", "NameID"]
                    val_vars = [c for c in df_in.columns if c not in id_vars]
                    if not val_vars:
                        logging.warning(f"[Gamma] No value_vars to melt for {txt_file}. Skipping.")
                        continue

                    df_melt = df_in.melt(
                        id_vars=id_vars,
                        value_vars=val_vars,
                        var_name="Attribute",
                        value_name="Value"
                    )

                    # 9. Rename dimension/attribute if needed
                    if dimension_rename_dict:
                        df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
                    if attribute_rename_dict:
                        df_melt["Attribute"] = df_melt["Attribute"].replace(attribute_rename_dict)

                    # 10. Exclude certain Dimensions or Attributes
                    df_melt = exclude_dimension_attribute(
                        df_melt,
                        bad_dimensions=post_melt_bad_dimensions,
                        bad_attributes=post_melt_bad_attributes
                    )

                    # 11. Build Key without NameID
                    df_melt["Key_NoName"] = df_melt.apply(
                        lambda row: f"{row['Dimension']} | {row['Attribute']} | {row['Value']}",
                        axis=1
                    )

                    # 12. Exclude rows based on Keys
                    before_exclusion = len(df_melt)
                    df_melt = df_melt[~df_melt["Key_NoName"].isin(excluded_keys)]
                    after_exclusion = len(df_melt)
                    logging.info(f"[Gamma] Excluded {before_exclusion - after_exclusion} rows based on excluded_keys in {txt_file}.")

                    # 13. Create a 'Key' column (Dimension | NameID | Attribute | Value)
                    df_melt["Key"] = df_melt.apply(
                        lambda row: f"{row['Dimension']} | {row['NameID']} | {row['Attribute']} | {row['Value']}",
                        axis=1
                    )

                    # 14. Remove duplicates if necessary
                    before_dedup = len(df_melt)
                    df_melt.drop_duplicates(subset=["Key", "Key_NoName"], inplace=True)
                    after_dedup = len(df_melt)
                    logging.info(f"[Gamma] Removed {before_dedup - after_dedup} duplicate rows in {txt_file}.")

                    # 15. Final DataFrame
                    final_df = df_melt[["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"]]
                    all_dfs.append(final_df)
                except Exception as e:
                    logging.error(f"[Gamma] Error processing file {txt_file}: {e}")
                    continue

        df_gamma = pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])
        if all_dfs:
            try:
                df_gamma = pd.concat(all_dfs, ignore_index=True)
                logging.info(f"[Gamma] Total rows after concatenation: {len(df_gamma)}")
            except Exception as e:
                logging.error(f"[Gamma] Error concatenating DataFrames: {e}")
        else:
            logging.warning("[Gamma] No data collected from ZIP.")

        return df_gamma
    except Exception as e:
        logging.error(f"[Gamma] Error during transformation: {e}")
        return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

# -----------------------------------------
# 6) CREATE MISSING ITEMS EXCEL
# -----------------------------------------
def create_missing_items_excel(
    df_alfa: pd.DataFrame,
    df_gamma: pd.DataFrame,
    df_exceptions: pd.DataFrame,
    output_path: Path
):
    """
    Identifies all missing items in Alfa and Gamma, merges with exceptions (which
    have 'Comments_1', 'Comments_2', 'hide exception'), excludes rows where
    hide exception == 'yes', and compiles them into an Excel report.

    The final output has columns:
      - Key
      - Dimension
      - NameID
      - Attribute
      - Value
      - Comments_1
      - Comments_2
      - Action Item
      - Missing In  (used for color-coding)

    Rows missing in Gamma are colored green for Dimension/NameID/Attribute/Value.
    Rows missing in Alfa  are colored blue  for Dimension/NameID/Attribute/Value.
    """
    try:
        logging.info("[Missing Items] Identifying matching dimensions between Alfa and Gamma...")
        # Identify dimensions present in both Alfa and Gamma
        dimensions_alfa = set(df_alfa['Dimension'].unique())
        dimensions_gamma = set(df_gamma['Dimension'].unique())
        matching_dimensions = dimensions_alfa.intersection(dimensions_gamma)
        logging.info(f"[Missing Items] Matching Dimensions Count: {len(matching_dimensions)}")

        # Filter Alfa and Gamma to only include matching dimensions
        df_alfa_matching = df_alfa[df_alfa['Dimension'].isin(matching_dimensions)].copy()
        df_gamma_matching = df_gamma[df_gamma['Dimension'].isin(matching_dimensions)].copy()

        logging.info("[Missing Items] Identifying missing NameIDs and other missing items...")
        # Group Alfa by Key_NoName and get set of NameIDs
        alfa_group = df_alfa_matching.groupby("Key_NoName")["NameID"].apply(set).reset_index()
        alfa_group.rename(columns={"NameID": "NameIDs_Alfa"}, inplace=True)

        # Group Gamma by Key_NoName and get set of NameIDs
        gamma_group = df_gamma_matching.groupby("Key_NoName")["NameID"].apply(set).reset_index()
        gamma_group.rename(columns={"NameID": "NameIDs_Gamma"}, inplace=True)

        # Merge Alfa and Gamma groups
        merged_group = alfa_group.merge(gamma_group, on="Key_NoName", how="inner")

        # Identify missing NameIDs in Gamma
        merged_group["Missing_In_Gamma"] = merged_group.apply(
            lambda row: row["NameIDs_Alfa"] - row["NameIDs_Gamma"],
            axis=1
        )

        # Identify missing NameIDs in Alfa
        merged_group["Missing_In_Alfa"] = merged_group.apply(
            lambda row: row["NameIDs_Gamma"] - row["NameIDs_Alfa"],
            axis=1
        )

        # Prepare list for missing items
        missing_items = []

        for _, row in merged_group.iterrows():
            key_noname = row["Key_NoName"]
            missing_in_gamma = row["Missing_In_Gamma"]
            missing_in_alfa = row["Missing_In_Alfa"]

            # If there are missing NameIDs in Gamma or Alfa, collect them
            # and skip other property-level differences for that Key_NoName.
            if missing_in_gamma or missing_in_alfa:
                # For each missing NameID in Gamma
                for nameid in missing_in_gamma:
                    # Find the original details from Alfa
                    details = df_alfa_matching[
                        (df_alfa_matching['NameID'] == nameid) &
                        (df_alfa_matching['Key_NoName'] == key_noname)
                    ]
                    if not details.empty:
                        details = details.iloc[0]
                        missing_items.append({
                            "Key": details["Key"],
                            "Dimension": details["Dimension"],
                            "NameID": nameid,
                            "Attribute": details["Attribute"],
                            "Value": details["Value"],
                            "Missing In": "Gamma"
                        })

                # For each missing NameID in Alfa
                for nameid in missing_in_alfa:
                    # Find the original details from Gamma
                    details = df_gamma_matching[
                        (df_gamma_matching['NameID'] == nameid) &
                        (df_gamma_matching['Key_NoName'] == key_noname)
                    ]
                    if not details.empty:
                        details = details.iloc[0]
                        missing_items.append({
                            "Key": details["Key"],
                            "Dimension": details["Dimension"],
                            "NameID": nameid,
                            "Attribute": details["Attribute"],
                            "Value": details["Value"],
                            "Missing In": "Alfa"
                        })

                # Move to next Key_NoName
                continue

            # Else, if no NameID is missing, identify other missing items
            alfa_keys = set(df_alfa_matching[df_alfa_matching['Key_NoName'] == key_noname]['Key'])
            gamma_keys = set(df_gamma_matching[df_gamma_matching['Key_NoName'] == key_noname]['Key'])

            # Items in Alfa not in Gamma
            missing_in_gamma_other = alfa_keys - gamma_keys
            for key in missing_in_gamma_other:
                details = df_alfa_matching[df_alfa_matching['Key'] == key].iloc[0]
                missing_items.append({
                    "Key": details["Key"],
                    "Dimension": details["Dimension"],
                    "NameID": details["NameID"],
                    "Attribute": details["Attribute"],
                    "Value": details["Value"],
                    "Missing In": "Gamma"
                })

            # Items in Gamma not in Alfa
            missing_in_alfa_other = gamma_keys - alfa_keys
            for key in missing_in_alfa_other:
                details = df_gamma_matching[df_gamma_matching['Key'] == key].iloc[0]
                missing_items.append({
                    "Key": details["Key"],
                    "Dimension": details["Dimension"],
                    "NameID": details["NameID"],
                    "Attribute": details["Attribute"],
                    "Value": details["Value"],
                    "Missing In": "Alfa"
                })

        df_missing = pd.DataFrame(missing_items)
        logging.info(f"[Missing Items] Total missing items after processing: {len(df_missing)}")

        if df_missing.empty:
            logging.info("[Missing Items] No missing items to report.")
            return

        # Merge with exception comments (Comments_1, Comments_2, hide exception)
        df_missing = df_missing.merge(
            df_exceptions,
            on='Key',
            how='left'
        )
        logging.info(f"[Missing Items] Merged with exception table. Rows: {len(df_missing)}")

        # Exclude rows where 'hide exception' == 'yes'
        df_missing['hide exception'] = df_missing['hide exception'].astype(str).str.lower().fillna('no')
        before_hide = len(df_missing)
        df_missing = df_missing[df_missing['hide exception'] != 'yes']
        after_hide = len(df_missing)
        logging.info(f"[Missing Items] Excluded {before_hide - after_hide} rows where 'hide exception' == 'yes'.")

        # Build final DataFrame with both comment columns
        df_missing['Action Item'] = ''
        final_columns = [
            'Key',
            'Dimension',
            'NameID',
            'Attribute',
            'Value',
            'Comments_1',
            'Comments_2',
            'Action Item',
            'Missing In'
        ]
        df_missing = df_missing.reindex(columns=final_columns)

        logging.info(f"[Missing Items] Final DataFrame with selected columns has {len(df_missing)} rows.")

        # Write to Excel
        df_missing.to_excel(output_path, sheet_name="Missing_Items", index=False)
        logging.info(f"[Missing Items] Wrote missing items to {output_path}")

        # --- Apply Color Formatting for Missing Rows ---
        try:
            wb = load_workbook(output_path)
            ws = wb["Missing_Items"]

            # Define font, header fill, and color fills
            bold_font = Font(bold=True)
            fill_header = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")  
            # For row coloring:
            fill_gamma = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # green
            fill_alfa  = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")  # blue

            # Identify columns by header name
            header_row = next(ws.iter_rows(min_row=1, max_row=1))
            headers = {cell.value: cell.column for cell in header_row}

            # Bold + gray fill on header
            for cell in header_row:
                cell.font = bold_font
                cell.fill = fill_header

            # We'll color only Dimension, NameID, Attribute, Value columns
            col_dimension = headers.get("Dimension")
            col_nameid    = headers.get("NameID")
            col_attribute = headers.get("Attribute")
            col_value     = headers.get("Value")
            col_missing   = headers.get("Missing In")  # so we can read whether it's Gamma or Alfa

            if not all([col_dimension, col_nameid, col_attribute, col_value, col_missing]):
                logging.warning("[Missing Items] One or more required columns for coloring are missing in the sheet.")
            else:
                # Loop through rows to apply color
                for row_idx in range(2, ws.max_row + 1):
                    missing_in_value = ws.cell(row=row_idx, column=col_missing).value
                    if str(missing_in_value).lower() == "gamma":
                        fill_color = fill_gamma
                    elif str(missing_in_value).lower() == "alfa":
                        fill_color = fill_alfa
                    else:
                        # Not coloring if it's not specifically 'Gamma' or 'Alfa'
                        continue

                    # Color the Dimension/NameID/Attribute/Value cells
                    ws.cell(row=row_idx, column=col_dimension).fill = fill_color
                    ws.cell(row=row_idx, column=col_nameid).fill    = fill_color
                    ws.cell(row=row_idx, column=col_attribute).fill = fill_color
                    ws.cell(row=row_idx, column=col_value).fill     = fill_color

            # Freeze the top row
            ws.freeze_panes = ws["A2"]

            # Save the colored Excel
            wb.save(output_path)
            logging.info(f"[Missing Items] Applied color formatting and froze top pane in {output_path}")

        except Exception as e:
            logging.error(f"[Missing Items] Error applying color formatting: {e}")

    except Exception as e:
        logging.error(f"[Missing Items] Error creating missing items Excel: {e}")

# -----------------------------------------
# 7) MAIN
# -----------------------------------------
def main():
    try:
        # Setup logging
        log_file = Path("script.log")
        setup_logging(log_file)
        logging.info("Script started.")

        # 1) Read Exclusion Table
        ex_table_path = Path("Ex_Table.xlsx")
        excluded_keys = read_exclusion_table(ex_table_path)

        # 1a) Read Exception Table (includes Key, Comments_1, Comments_2, hide exception)
        exception_table_path = Path("Exception_Table.xlsx")
        df_exceptions = read_exception_table(exception_table_path)

        # 2) ALFA
        alfa_pre_exclude = [
            ("ColA", ["X"]),
            ("ColB", ["Y"])
        ]
        alfa_bad_dims = ["UnwantedDim"]
        alfa_bad_attrs = ["Debug"]
        alfa_dim_rename = {
            "DimOld": "DimNew"
        }
        alfa_attr_rename = {
            "First": "Name"
        }

        alfa_path = Path("AlfaData.xlsx")
        df_alfa = transform_alfa(
            file_path=alfa_path,
            excluded_keys=excluded_keys,
            pre_melt_exclude_rules=alfa_pre_exclude,
            post_melt_bad_dimensions=alfa_bad_dims,
            post_melt_bad_attributes=alfa_bad_attrs,
            dimension_rename_dict=alfa_dim_rename,
            attribute_rename_dict=alfa_attr_rename,
            sheet_name="Sheet1",
            skip_rows=3
        )
        logging.info(f"[Alfa] Final rows: {len(df_alfa)}")

        # 3) GAMMA
        gamma_pre_exclude = [
            ("RawCol", ["Z"])
        ]
        gamma_bad_dims = ["TestDim"]
        gamma_bad_attrs = ["BadAttr"]
        gamma_dim_rename = {
            "GammaOld": "GammaNew"
        }
        gamma_attr_rename = {
            "First": "Name"
        }

        gamma_zip = Path("GammaData.zip")
        df_gamma = transform_gamma(
            zip_file_path=gamma_zip,
            excluded_keys=excluded_keys,
            pre_melt_exclude_rules=gamma_pre_exclude,
            post_melt_bad_dimensions=gamma_bad_dims,
            post_melt_bad_attributes=gamma_bad_attrs,
            dimension_rename_dict=gamma_dim_rename,
            attribute_rename_dict=gamma_attr_rename,
            delimiter=",",
            remove_substring="_ceaster.txt"
        )
        logging.info(f"[Gamma] Final rows: {len(df_gamma)}")

        # 4) Create Missing Items Excel
        comparison_out = Path("Missing_Items.xlsx")
        create_missing_items_excel(df_alfa, df_gamma, df_exceptions, comparison_out)

        logging.info("Script completed successfully.")
    except Exception as e:
        logging.critical(f"[Main] Critical error: {e}")

if __name__ == "__main__":
    main()
