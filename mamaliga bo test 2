#!/usr/bin/env python3
"""
Ultra-Mega Reconciliation with:
 - Automatic loading of all history JSON runs on startup
 - ERP & Master Previews (auto End Date filter => blank/future)
 - Compare => missing_items.xlsx (two sheets: Mismatch & Case_Differences)
 - "Gap In" column instead of "Missing In"
 - 8-chart Dashboard with Bollinger band
 - Bollinger data saved on close (for entire history) to a user-chosen JSON
 - KEY = (Dimension|Name|Attribute|Master|ERP).upper() in Mismatch
 - Only check case differences for rows where Attribute=="Name"
 - **Paths tab is hidden** (we still load them, but no UI for it).
 - **No Start/End Date filter popups** in the Previews.
 - **When double-clicking a JSON run in History, show a summary plus a button for full content**.
 - **We store both Key (uppercase) and Key_Original to handle case differences**.
 - **CSV reading tries 'utf-8-sig' and 'utf-16-le' encodings**.
 - **Now replaces any NaN with "<<NaN>>"** in meltdown steps.
"""

import os
import sys
import json
import math
import logging
import zipfile
import shutil
import io
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Set, List, Tuple

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import customtkinter as ctk

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.backends.backend_pdf import PdfPages

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------------------------------------------------------
# DEFAULT CONFIG & SAVE/LOAD
# ----------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/missing_items.xlsx",
    "CONFIG_PATH": "config/ui_config.json",  # hidden in GUI
    "PARAMETER_PATH": "data/parameters.xlsx",
    "MASTER_CSV_OUTPUT": "temp_master_csv",  # hidden in GUI
    "PDF_EXPORT_PATH": "output/dashboard_report.pdf",
    "LOGO_PATH": "images/company_logo.png",  # hidden in GUI
    "HISTORY_PATH": "history_runs",          # hidden in GUI
    "BAND_CHART_JSON_PATH": "data/bollinger_data.json"  # hidden in GUI
}

def default_config() -> Dict:
    return {
        "paths": DEFAULT_PATHS.copy(),
        "erp_grid": {"filters": {}},
        "master_grid": {"filters": {}},
        "dashboard": {
            "selected_dims": [],
            "selected_attrs": [],
            "top_n": 10
        }
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config => {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        # Convert sets->lists in erp_grid
        if "erp_grid" in cfg and "filters" in cfg["erp_grid"]:
            newf = {}
            for col, svals in cfg["erp_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["erp_grid"]["filters"] = newf

        # Convert sets->lists in master_grid
        if "master_grid" in cfg and "filters" in cfg["master_grid"]:
            newf = {}
            for col, svals in cfg["master_grid"]["filters"].items():
                newf[col] = list(svals)
            cfg["master_grid"]["filters"] = newf

        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config => {e}")


# ----------------------------------------------------------------------------
# TEXT LOGGER HANDLER
# ----------------------------------------------------------------------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget
    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")


# ----------------------------------------------------------------------------
# READ PARAM
# ----------------------------------------------------------------------------
def read_param_file(path: Path) -> Dict[str, object]:
    """
    Reads two sheets from 'parameters.xlsx':
      - 'Dimension Parameters'
      - 'Attribute Parameters'

    to build these maps:

        param = {
          "dim_erp_keep": set(),    # which ERP V_S_C codes to keep
          "dim_erp_map": {},        # ERP side: V_S_C => dimension
          "dim_master_map": {},     # Master side: RawFileName => dimension
          "attr_erp_map": {},       # ERP attribute => final attribute
          "attr_master_map": {}     # Master attribute => final attribute
        }

    """
    param = {
        "dim_erp_keep": set(),
        "dim_erp_map": {},
        "dim_master_map": {},
        "attr_erp_map": {},
        "attr_master_map": {}
    }
    if not path.is_file():
        logging.warning(f"Param file not found => {path}")
        return param
    try:
        dim_df = pd.read_excel(path, sheet_name="Dimension Parameters")
        dim_df.columns = dim_df.columns.astype(str).str.strip()

        def s(x): return str(x).strip() if pd.notna(x) else ""

        for _, row in dim_df.iterrows():
            fn  = s(row.get("FileName", ""))       # e.g. "Products.txt"
            vsc = s(row.get("V S C", ""))          # e.g. "C", "S", ...
            dim = s(row.get("Dimension", ""))      # e.g. "Products Dimension"
            ev  = s(row.get("ERP Values", ""))     # "X" means keep

            # If ERP Values is 'X', we keep that vsc
            if ev.lower() == "x" and vsc and dim:
                param["dim_erp_keep"].add(vsc)

            # Map from vsc => dimension
            if vsc and dim:
                param["dim_erp_map"][vsc] = dim

            # Map from FileName => dimension
            if fn and dim and ev.lower() == "x":
                param["dim_master_map"][fn] = dim

        attr_df = pd.read_excel(path, sheet_name="Attribute Parameters")
        attr_df.columns = attr_df.columns.astype(str).str.strip()
        for _, row in attr_df.iterrows():
            e_orig = s(row.get("ERP Original Attributes", ""))     # e.g. "EffectiveStartDate"
            m_orig = s(row.get("Master Original Attributes", ""))  # e.g. "Start_Date_Col"
            final_ = s(row.get("Attribute", ""))                   # e.g. "Start Date"
            onoff  = s(row.get("On/Off", ""))                      # "X" = use

            if onoff.lower() == "x" and final_:
                if e_orig:
                    param["attr_erp_map"][e_orig] = final_
                if m_orig:
                    param["attr_master_map"][m_orig] = final_
        return param
    except Exception as e:
        logging.error(f"Error reading param file => {e}")
        return param


# ----------------------------------------------------------------------------
# HELPER: keep blank or strictly future End Date
# ----------------------------------------------------------------------------
def keep_valid_end_date(attr: str, val) -> bool:
    """
    Keep row if attribute != 'End Date', or if 'End Date' => blank or strictly future
    We'll parse val as 'YYYY-MM-DD' if present.
    """
    if attr != "End Date":
        return True
    v = str(val).strip()
    if not v:
        return True
    try:
        dt = datetime.strptime(v, "%Y-%m-%d").date()
        return dt > datetime.now().date()
    except:
        return False

def strip_t(val) -> str:
    """
    Strips trailing time portion if found, also
    convert NaN => empty string for safety.
    """
    if pd.isna(val):
        return ""
    s = str(val).strip()
    if "T" in s:
        s = s.split("T")[0]
    return s


# ----------------------------------------------------------------------------
# ERP READING
# ----------------------------------------------------------------------------
def read_erp_excel(path: Path) -> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"ERP Excel not found => {path}")
        return pd.DataFrame()
    try:
        # Example: we skip 3 rows for a header
        df = pd.read_excel(path, skiprows=3)
        df.columns = df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df = df[df["Enabled_Flag"] == "Enabled"]
        return df
    except Exception as e:
        logging.error(f"Error reading ERP => {e}")
        return pd.DataFrame()


# ----------------------------------------------------------------------------
# MASTER READING (TXT => CSV and unify)
# ----------------------------------------------------------------------------
def read_txt_2encodings(raw: bytes) -> pd.DataFrame:
    """
    Tries reading the bytes with 'utf-8-sig' and 'utf-16-le'.
    Returns the first successful parse or empty DataFrame if none works.
    """
    import io
    for enc in ["utf-8-sig","utf-16-le"]:
        try:
            buf = io.BytesIO(raw)
            df = pd.read_csv(buf, encoding=enc, on_bad_lines="skip", engine="python")
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            df.columns = df.columns.astype(str).str.strip()
            logging.info(f"[read_txt_2encodings] success with {enc}, shape={df.shape}")
            return df
        except Exception as e:
            logging.debug(f"[read_txt_2encodings] fail with {enc} => {e}")
    logging.error("[read_txt_2encodings] cannot parse => empty.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path):
    """
    Unzip all *.txt from Master ZIP,
    read each with read_txt_2encodings,
    store as CSV in out_dir,
    return list of CSV paths.
    """
    if not zip_path.is_file():
        logging.warning(f"[Master] ZIP not found => {zip_path}")
        return []
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    csvs = []
    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name = os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw = fo.read()
                df = read_txt_2encodings(raw)
                if df.empty:
                    continue
                df["RawFileName"] = base_name
                # rename first col => Name if we don't already have 'Name'
                if "Name" not in df.columns and len(df.columns) > 0:
                    first_col = df.columns[0]
                    if first_col != "Name":
                        df.rename(columns={first_col: "Name"}, inplace=True)
                out_csv = out_dir / (base_name.replace(".txt", ".csv"))
                df.to_csv(out_csv, index=False, encoding="utf-8")
                csvs.append(out_csv)
            except Exception as e:
                logging.error(f"[Master] error reading {txt_file} => {e}")
    return csvs

def read_csv_2encodings(path: Path) -> pd.DataFrame:
    """
    Similar approach for final CSVs: tries 'utf-8-sig' then 'utf-16-le'.
    """
    for enc in ["utf-8-sig","utf-16-le"]:
        try:
            df = pd.read_csv(path, encoding=enc, on_bad_lines="skip")
            df.columns = df.columns.astype(str).str.strip()
            return df
        except Exception as ex:
            logging.debug(f"Failed reading {path} with {enc}: {ex}")
    logging.error(f"Cannot read CSV => {path}")
    return pd.DataFrame()

def unify_master_csvs(csvs: List[Path]) -> pd.DataFrame:
    """
    Combines all CSVs extracted from the Master ZIP into one DataFrame.
    """
    frames = []
    for cp in csvs:
        if not cp.is_file():
            continue
        try:
            df = read_csv_2encodings(cp)
            frames.append(df)
        except Exception as e:
            logging.error(f"[unify_master_csvs] reading {cp} => {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()


# ----------------------------------------------------------------------------
# MELTDOWN => fill NaN => "<<NaN>>", plus keep end date blank/future
# ----------------------------------------------------------------------------
def meltdown_erp_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    """
    1) Keep only rows with V_S_C in param["dim_erp_keep"].
    2) Map V_S_C => param["dim_erp_map"] => "Dimension"
    3) Melt wide columns => (Dimension, Name, Attribute, Value)
    4) Use param["attr_erp_map"] to rename columns => standard attributes
    5) Convert any NaN => "<<NaN>>"
    6) Keep End Date blank/future
    """
    if "V_S_C" not in df.columns:
        return pd.DataFrame()
    keep = param["dim_erp_keep"]
    dmap = param["dim_erp_map"]
    amap = param["attr_erp_map"]

    df2 = df[df["V_S_C"].isin(keep)].copy()
    if df2.empty:
        return pd.DataFrame()

    skip_cols = {"V_S_C", "Enabled_Flag"}
    id_vars = []
    if "Value" in df2.columns:
        id_vars.append("Value")
        skip_cols.add("Value")

    # dimension
    df2["Dimension"] = df2["V_S_C"].map(dmap).fillna(df2["V_S_C"])
    skip_cols.add("Dimension")
    if "Value" in id_vars:
        id_vars.insert(0, "Dimension")
    else:
        id_vars.append("Dimension")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    if "Value" in id_vars:
        melted.rename(columns={"Value": "Name"}, inplace=True)
    else:
        melted["Name"] = ""

    # keep recognized attributes
    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)

    # strip T, then convert NaN => "<<NaN>>"
    def handle_value(a, val):
        # strip potential trailing time
        raw = strip_t(val)
        if pd.isna(raw) or raw == "":
            # fill with explicit placeholder
            raw = "<<NaN>>"

        return str(raw)

    melted["Value"] = melted.apply(lambda r: handle_value(r["Attribute"], r["ValX"]), axis=1)

    # keep valid end date (if Value is not "<<NaN>>")
    keep_rows = []
    for _, row in melted.iterrows():
        if row["Value"] == "<<NaN>>" and row["Attribute"] == "End Date":
            # This means it was missing data, but we treat missing as blank => OK
            keep_rows.append(row)
        elif keep_valid_end_date(row["Attribute"], row["Value"]):
            keep_rows.append(row)

    out = pd.DataFrame(keep_rows)
    return out[["Dimension","Name","Attribute","Value"]]

def meltdown_master_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    """
    1) Keep rows where 'RawFileName' is recognized in param["dim_master_map"].
    2) Map => dimension
    3) Melt => (Dimension, Name, Attribute, Value)
    4) Map attribute
    5) fill NaN => "<<NaN>>"
    6) keep valid end date
    """
    if df.empty or "RawFileName" not in df.columns:
        return pd.DataFrame()
    dmap = param["dim_master_map"]
    amap = param["attr_master_map"]

    df2 = df[df["RawFileName"].isin(dmap.keys())].copy()
    if df2.empty:
        return pd.DataFrame()

    df2["Dimension"] = df2["RawFileName"].map(dmap).fillna(df2["RawFileName"])

    skip_cols = {"RawFileName", "Dimension"}
    id_vars = ["Dimension"]
    if "Name" in df2.columns:
        id_vars.append("Name")
        skip_cols.add("Name")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(
        id_vars=id_vars,
        value_vars=meltdown_cols,
        var_name="OrigAttr",
        value_name="ValX"
    )

    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)

    def handle_value(a, val):
        raw = strip_t(val)
        if pd.isna(raw) or raw == "":
            raw = "<<NaN>>"
        return str(raw)

    melted["Value"] = melted.apply(lambda r: handle_value(r["Attribute"], r["ValX"]), axis=1)

    keep_rows = []
    for _, row in melted.iterrows():
        if row["Value"] == "<<NaN>>" and row["Attribute"] == "End Date":
            # missing End Date => treat as blank => keep
            keep_rows.append(row)
        elif keep_valid_end_date(row["Attribute"], row["Value"]):
            keep_rows.append(row)

    out = pd.DataFrame(keep_rows)
    if "Name" not in out.columns:
        out["Name"] = ""
    return out[["Dimension","Name","Attribute","Value"]]

def pivot_for_preview(df: pd.DataFrame) -> pd.DataFrame:
    """
    pivot => (Dimension, Name) x Attributes => Value
    """
    if not df.empty and {"Dimension","Name","Attribute"}.issubset(df.columns):
        df = df.drop_duplicates(subset=["Dimension","Name","Attribute"])
        try:
            df = df.pivot(index=["Dimension","Name"], columns="Attribute", values="Value").reset_index()
        except Exception as e:
            logging.error(f"Pivot error => {e}")
    return df


# ----------------------------------------------------------------------------
# COMPARE => produce "Gap In" (instead of "Missing In")
# ----------------------------------------------------------------------------
def melt_back(df: pd.DataFrame) -> pd.DataFrame:
    """
    Reverse pivot => (Dimension, Name, Attribute, Value).
    """
    if df.empty or {"Dimension","Name"}.difference(df.columns):
        return pd.DataFrame()
    meltdown_cols = [c for c in df.columns if c not in ("Dimension","Name")]
    melted = df.melt(id_vars=["Dimension","Name"], value_vars=meltdown_cols,
                     var_name="Attribute", value_name="Value")
    return melted[["Dimension","Name","Attribute","Value"]]

def build_keys(df: pd.DataFrame)-> pd.DataFrame:
    """
    Prepare for compare => fill blanks, add GroupKey placeholders, etc.
    Also note that we can have blank strings or actual NaN in DataFrame.
    We'll do fillna("") here.
    """
    df = df.copy()
    for c in ["Dimension","Name","Attribute","Value"]:
        if c not in df.columns:
            df[c] = ""
        df[c] = df[c].fillna("").astype(str).str.strip()
    df["GroupKey"] = df["Dimension"] + " | " + df["Name"]
    return df

def compare_mode2(df_erp: pd.DataFrame, df_mst: pd.DataFrame)-> pd.DataFrame:
    """
    Instead of 'Missing In', we produce 'Gap In' with logic:
       if Master="", ERP!="" => 'MASTER'
       elif ERP="", Master!="" => 'ERP'
       else => 'MISMATCH'
    We'll store 'Master'/'ERP' in separate columns.

    Also we store both Key_Original (exact dimension|name|attribute|master|erp)
    and Key (uppercase) to handle exceptions + case differences.
    """
    def to_dict(d):
        out={}
        for gk, grp in d.groupby("GroupKey"):
            rec={}
            nm= grp["Name"].iloc[0] if not grp.empty else ""
            rec["Name"] = nm
            for _, row in grp.iterrows():
                rec[row["Attribute"]] = row["Value"]
            out[gk] = rec
        return out

    e_dict = to_dict(df_erp)
    m_dict = to_dict(df_mst)
    all_gk = set(e_dict.keys()) | set(m_dict.keys())

    results=[]
    for gk in all_gk:
        dim = gk.split(" | ")[0]
        a_data= e_dict.get(gk,{})
        b_data= m_dict.get(gk,{})
        name_a= a_data.get("Name","")
        name_b= b_data.get("Name","")
        if name_a and name_b and (name_a==name_b):
            # Both share same "Name"
            union_attrs= (set(a_data.keys()) | set(b_data.keys())) - {"Name"}
            for at in union_attrs:
                va= a_data.get(at,"")
                vb= b_data.get(at,"")
                if va != vb:
                    if va and not vb:
                        results.append({"Dimension": dim,"Name": name_a,"Attribute": at,
                                        "Master": "", "ERP": va, "Gap In": "MASTER"})
                    elif vb and not va:
                        results.append({"Dimension": dim,"Name": name_a,"Attribute": at,
                                        "Master": vb, "ERP": "", "Gap In": "ERP"})
                    else:
                        results.append({"Dimension": dim,"Name": name_a,"Attribute": at,
                                        "Master": vb, "ERP": va, "Gap In": "MISMATCH"})
        else:
            # name differs or one is missing => handle at the "Name" attribute
            if name_a and not name_b:
                results.append({"Dimension": dim, "Name": name_a, "Attribute": "Name",
                                "Master": "", "ERP": name_a, "Gap In": "MASTER"})
            elif name_b and not name_a:
                results.append({"Dimension": dim, "Name": name_b, "Attribute": "Name",
                                "Master": name_b, "ERP": "", "Gap In": "ERP"})
            else:
                # both have name but differ => MISMATCH
                if name_a or name_b:
                    results.append({"Dimension": dim, "Name": name_a or name_b, "Attribute": "Name",
                                    "Master": name_b, "ERP": name_a, "Gap In": "MISMATCH"})

    df_res= pd.DataFrame(results)
    if not df_res.empty:
        def build_key_original(r):
            return f"{r['Dimension']}|{r['Name']}|{r['Attribute']}|{r['Master']}|{r['ERP']}"
        df_res["Key_Original"] = df_res.apply(build_key_original, axis=1)
        df_res["Key"] = df_res["Key_Original"].str.upper()
    return df_res

def read_exception_table(path: Path)-> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"Exception table not found => {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path)
        df.columns = df.columns.astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"Error reading exception => {e}")
        return pd.DataFrame()

def merge_exceptions(df: pd.DataFrame, df_exc: pd.DataFrame)-> pd.DataFrame:
    """
    Merge on 'Key' (uppercase). If 'hide exception' == 'yes', exclude that row.
    """
    if df.empty or df_exc.empty or "Key" not in df.columns:
        return df
    keep_cols= [c for c in ["Key","Comments_1","Comments_2","hide exception"] if c in df_exc.columns]
    if not keep_cols:
        return df
    exc= df_exc[keep_cols].copy()
    exc["Key"]= exc["Key"].astype(str).str.strip()

    merged= df.merge(exc, on="Key", how="left", suffixes=("","_exc"))
    merged["hide exception"]= merged.get("hide exception","").fillna("").str.lower()
    final= merged[merged["hide exception"]!="yes"].copy()

    if "Comments_1_exc" in final.columns:
        final["Comments_1"]= np.where(final["Comments_1_exc"].notna(), final["Comments_1_exc"], final["Comments_1"])
        final.drop(columns=["Comments_1_exc"], inplace=True)
    if "Comments_2_exc" in final.columns:
        final["Comments_2"]= np.where(final["Comments_2_exc"].notna(), final["Comments_2_exc"], final["Comments_2"])
        final.drop(columns=["Comments_2_exc"], inplace=True)
    if "hide exception" in final.columns:
        final.drop(columns=["hide exception"], inplace=True)

    return final

# ----------------------------------------------------------------------------
# CASE DIFFS
# ----------------------------------------------------------------------------
def separate_case_diffs(df: pd.DataFrame) -> Tuple[pd.DataFrame,pd.DataFrame]:
    """
    Only for rows where Attribute=="Name" AND Master.lower()==ERP.lower()
    but Master != ERP => we put into 'Case_Differences'.
    Remainder => 'Mismatch'.
    """
    if df.empty:
        return df, pd.DataFrame()

    mask_attr = df["Attribute"]=="Name"
    same_lower = df["Master"].str.lower() == df["ERP"].str.lower()
    diff_actual = df["Master"] != df["ERP"]
    case_mask = mask_attr & same_lower & diff_actual

    case_df = df[case_mask].copy()
    mismatch_df= df[~case_mask].copy()

    if not case_df.empty:
        case_df.rename(columns={"Key":"Key_Upper","Key_Original":"Key"}, inplace=True)

    return mismatch_df, case_df


# ----------------------------------------------------------------------------
# WRITE => 2 sheets => Mismatch & Case_Differences
# ----------------------------------------------------------------------------
def write_two_sheet_excel(mismatch: pd.DataFrame, case_only: pd.DataFrame, out_path: Path):
    out_path.parent.mkdir(parents=True, exist_ok=True)
    base_cols = ["Key","Dimension","Name","Attribute","Master","ERP","Comments_1","Comments_2","Gap In"]
    extra_cols_mismatch = []
    extra_cols_case = []

    if "Key_Original" in mismatch.columns:
        extra_cols_mismatch.append("Key_Original")
    if "Key_Upper" in case_only.columns:
        extra_cols_case.append("Key_Upper")

    def final_cols(df: pd.DataFrame, extra: List[str]) -> List[str]:
        all_cols = base_cols + extra
        existing = [c for c in all_cols if c in df.columns]
        return existing

    mismatch_cols = final_cols(mismatch, extra_cols_mismatch)
    case_cols = final_cols(case_only, extra_cols_case)

    def finalize(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
        dfc= df.copy()
        for c in cols:
            if c not in dfc.columns:
                dfc[c]= ""
        return dfc[cols]

    mismatch_f= finalize(mismatch, mismatch_cols)
    case_f= finalize(case_only, case_cols)

    wb= Workbook()
    ws_m= wb.active
    ws_m.title= "Mismatch"
    ws_m.append(mismatch_cols)
    for rowvals in mismatch_f.itertuples(index=False):
        ws_m.append(rowvals)

    header_font= Font(bold=True)
    fill= PatternFill(start_color="DDDDDD", end_color="DDDDDD", fill_type="solid")
    for cell in ws_m[1]:
        cell.font= header_font
        cell.fill= fill
        cell.alignment= Alignment(horizontal="center")
    for col in ws_m.columns:
        mx= 0
        let= col[0].column_letter
        for cell in col:
            val= str(cell.value) if cell.value else ""
            mx= max(mx, len(val))
        ws_m.column_dimensions[let].width= mx+2
    ws_m.freeze_panes= "A2"

    ws_c= wb.create_sheet("Case_Differences")
    ws_c.append(case_cols)
    for rowvals in case_f.itertuples(index=False):
        ws_c.append(rowvals)
    for cell in ws_c[1]:
        cell.font= header_font
        cell.fill= fill
        cell.alignment= Alignment(horizontal="center")
    for col in ws_c.columns:
        mx= 0
        let= col[0].column_letter
        for cell in col:
            val= str(cell.value) if cell.value else ""
            mx= max(mx, len(val))
        ws_c.column_dimensions[let].width= mx+2
    ws_c.freeze_panes= "A2"

    wb.save(out_path)
    logging.info(f"Missing items => {out_path}")


# ----------------------------------------------------------------------------
# SIMPLE PREVIEW (No date filter popups)
# ----------------------------------------------------------------------------
class SimplePreview(ctk.CTkFrame):
    """
    We show meltdown data pivoted in a read-only table (no date filter).
    If the config had filters, we store them but ignore them.
    """
    def __init__(self, parent, name: str, filters_dict=None):
        super().__init__(parent)
        self.name= name
        self.df= pd.DataFrame()
        self.filters: Dict[str, Set[str]] = {}
        if filters_dict:
            for col, val_list in filters_dict.items():
                if isinstance(val_list, list):
                    self.filters[col] = set(val_list)
                else:
                    self.filters[col] = set()

        self.create_toolbar()
        self.create_table()
        self.create_statusbar()

    def create_toolbar(self):
        bar= ctk.CTkFrame(self, corner_radius=10, fg_color="#f0f0f0")
        bar.pack(fill="x", padx=5, pady=5)

        ctk.CTkLabel(
            bar, text=f"{self.name} Preview",
            fg_color="#800020", corner_radius=8,
            text_color="white",
            font=ctk.CTkFont(size=14, weight="bold")
        ).pack(side="left", padx=5)

    def create_table(self):
        container= ctk.CTkFrame(self)
        container.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree= ttk.Treeview(container, show="headings")
        vsb= ttk.Scrollbar(container, orient="vertical", command=self.tree.yview)
        hsb= ttk.Scrollbar(container, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        vsb.grid(row=0, column=1, sticky="ns")
        hsb.grid(row=1, column=0, sticky="ew")
        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

    def create_statusbar(self):
        self.status_label= ctk.CTkLabel(self, text="0 rows", fg_color="#f0f0f0", text_color="black")
        self.status_label.pack(fill="x")

    def set_data(self, df: pd.DataFrame):
        self.df= df.copy()
        self.refresh_table()

    def refresh_table(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if self.df.empty:
            self.tree["columns"] = []
            self.status_label.configure(text="0 rows")
            return

        cols= list(self.df.columns)
        self.tree["columns"]= cols
        for c in cols:
            self.tree.heading(c, text=c, anchor="w")
            self.tree.column(c, anchor="w", width=150)

        # no filters => just show
        for _, row in self.df.iterrows():
            rowvals= [row[c] for c in cols]
            self.tree.insert("", "end", values=rowvals)
        self.status_label.configure(text=f"{len(self.df)} rows")

    def get_filtered_df(self)-> pd.DataFrame:
        # We simply return the entire df
        return self.df


# ----------------------------------------------------------------------------
# PDF REPORT => 8-charts
# ----------------------------------------------------------------------------
class EnhancedPDFReport:
    def __init__(self, df_current: pd.DataFrame, df_history: pd.DataFrame, config: Dict):
        self.df_current = df_current
        self.df_history = df_history
        self.config = config
        self.page_count = 0
        self.colors = {
            'primary': '#800020',
            'text': '#2C1810',
            'background': '#FFFFFF'
        }
        self.logo_path = self.config["paths"].get("LOGO_PATH","images/company_logo.png")

        self.PAGE_WIDTH = 8.5
        self.PAGE_HEIGHT= 11
        self.CHART_SCALE = 0.7

    def generate(self) -> Path:
        pdf_path= self._get_output_path()
        with PdfPages(pdf_path) as pdf:
            self._cover_page(pdf)
            self._summary_page(pdf)
            self._topdimsattrs_page(pdf)
            self._all_charts(pdf)
        logging.info(f"PDF => {pdf_path}")
        return pdf_path

    def _get_output_path(self)-> Path:
        stamp= datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_dir= Path("Reconciliation_pdf")
        pdf_dir.mkdir(parents=True, exist_ok=True)
        pdf_name= f"Reconciliationpdf_{stamp}.pdf"
        pdf_path= pdf_dir / pdf_name
        return pdf_path

    def _new_page(self)-> plt.Figure:
        self.page_count += 1
        fig= plt.figure(figsize=(self.PAGE_WIDTH,self.PAGE_HEIGHT))
        fig.patch.set_facecolor(self.colors['background'])
        plt.axis('off')
        if self.logo_path and os.path.exists(self.logo_path):
            try:
                import matplotlib.image as mpimg
                img= mpimg.imread(self.logo_path)
                ax_img= fig.add_axes([0.65,0.65, 0.25, 0.25], anchor='NE', zorder=10)
                ax_img.imshow(img, alpha=0.2)
                ax_img.axis('off')
            except Exception as e:
                logging.error(f"Logo => {e}")

        fig.text(0.5, 0.97, "Reconciliation Report", ha='center', fontsize=10, color='gray')
        fig.text(0.9, 0.03, f"Page {self.page_count}", ha='right', fontsize=8, color='gray')
        fig.text(0.5, 0.02, "© Ultra-Mega Reconciliation", ha='center', fontsize=8, color='gray')
        return fig

    def _cover_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.7, "Reconciliation Analysis Report",
                 ha='center', fontsize=24, fontweight='bold', color=self.colors['primary'],
                 transform=fig.transFigure)
        plt.text(0.5, 0.6, f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                 ha='center', fontsize=12, color=self.colors['text'], transform=fig.transFigure)
        plt.text(0.5, 0.1, "CONFIDENTIAL", ha='center', fontsize=9, color=self.colors['text'],
                 transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _summary_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.92, "Reconciliation Summary", ha='center', fontsize=18, fontweight='bold',
                 color=self.colors['primary'], transform=fig.transFigure)
        y= 0.75
        if self.df_current.empty:
            plt.text(0.5, y, "No mismatches found this run.",
                     ha='center', fontsize=14, color=self.colors['text'], transform=fig.transFigure)
        else:
            total= len(self.df_current)
            e= (self.df_current["Gap In"]=="ERP").sum()
            m= (self.df_current["Gap In"]=="MASTER").sum()
            summary= (
                f"Total Mismatches: {total}\n"
                f"Gap=ERP: {e}\n"
                f"Gap=MASTER: {m}"
            )
            plt.text(0.5, y, summary, ha='center', fontsize=14, color=self.colors['text'],
                     transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _topdimsattrs_page(self, pdf: PdfPages):
        fig= self._new_page()
        plt.text(0.5, 0.92, "Top Dimensions & Attributes", ha='center', fontsize=18, fontweight='bold',
                 color=self.colors['primary'], transform=fig.transFigure)
        if self.df_current.empty:
            plt.text(0.5, 0.7, "No data available.", ha='center', fontsize=12, color=self.colors['text'],
                     transform=fig.transFigure)
        else:
            if "Dimension" in self.df_current.columns:
                dims= self.df_current["Dimension"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in dims.items()]
                dim_txt= "Top Dimensions:\n" + "\n".join(lines)
                plt.text(0.2, 0.7, dim_txt, fontsize=12, color=self.colors['text'],
                         transform=fig.transFigure)
            if "Attribute" in self.df_current.columns:
                attrs= self.df_current["Attribute"].value_counts().head(5)
                lines= [f"- {k} ({v})" for k,v in attrs.items()]
                attr_txt= "Top Attributes:\n" + "\n".join(lines)
                plt.text(0.6, 0.7, attr_txt, fontsize=12, color=self.colors['text'],
                         transform=fig.transFigure)
        pdf.savefig(fig)
        plt.close(fig)

    def _chart_page(self, pdf: PdfPages, title: str, plot_func, **kwargs):
        fig= self._new_page()
        fig.suptitle(title, fontsize=14, fontweight='bold', color=self.colors['primary'], y=0.93)

        w_inch= self.PAGE_WIDTH * self.CHART_SCALE
        if "Heatmap" in title:
            h_inch= w_inch * 1.4
        else:
            h_inch= w_inch * (9.0/16.0)

        left_margin= (self.PAGE_WIDTH - w_inch)*0.5 + 0.2
        bottom_margin= (self.PAGE_HEIGHT - h_inch)*0.5

        left_rel= left_margin / self.PAGE_WIDTH
        bottom_rel= bottom_margin / self.PAGE_HEIGHT
        width_rel= w_inch / self.PAGE_WIDTH
        height_rel= h_inch / self.PAGE_HEIGHT

        ax_rect= [left_rel, bottom_rel, width_rel, height_rel]
        ax= fig.add_axes(ax_rect)

        try:
            plot_func(ax, **kwargs)
            pdf.savefig(fig)
        except Exception as e:
            logging.error(f"{title} => {e}")
        finally:
            plt.close(fig)

    def _all_charts(self, pdf: PdfPages):
        dfc= self.df_current.copy()
        if dfc.empty:
            return
        df_m= dfc[dfc["Gap In"]!=""]

        # Heatmap
        if not df_m.empty and {"Dimension","Attribute"}.issubset(df_m.columns):
            pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
            if not pivot.empty:
                self._chart_page(pdf, "Heatmap", self._plot_heatmap, pivot=pivot)

        # Lollipop
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim.empty:
            self._chart_page(pdf, "Lollipop", self._plot_lollipop, cdim=cdim)

        # Circular
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr.empty:
            self._chart_page(pdf, "Circular", self._plot_circular, cattr=cattr)

        # Scatter
        cdim_sc= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim_sc.sort_values("Count", ascending=False, inplace=True)
        cdim_sc= cdim_sc.head(10)
        if not cdim_sc.empty:
            self._chart_page(pdf, "Scatter", self._plot_scatter, cdim=cdim_sc)

        # Radar
        cdim_ra= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False).head(10)
        if not cdim_ra.empty and len(cdim_ra)>1:
            self._chart_page(pdf, "Radar", self._plot_radar, cdim=cdim_ra)

        # Pie
        dist= df_m["Gap In"].value_counts()
        if not dist.empty:
            self._chart_page(pdf, "Pie: Gap In distribution", self._plot_pie, dist=dist)

        # Bar
        cattr_b= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False).head(10)
        if not cattr_b.empty:
            self._chart_page(pdf, "Bar: Gap In attributes", self._plot_bar, cattr=cattr_b)

        # Bollinger
        if not self.df_history.empty and "RunDate" in self.df_history.columns:
            date_ct= self.df_history.groupby("RunDate")["Key"].count().reset_index(name="Count")
            date_ct.sort_values("RunDate", inplace=True)
            if not date_ct.empty:
                self._chart_page(pdf, "Bollinger Band Over Time", self._plot_bollinger, date_ct=date_ct)

    # chart plot helpers
    def _plot_heatmap(self, ax, pivot):
        im= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=45, ha="right")
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        plt.colorbar(im, ax=ax)

    def _plot_lollipop(self, ax, cdim):
        ax.hlines(y=cdim.index, xmin=0, xmax=cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_xlabel("Count")

    def _plot_circular(self, ax, cattr):
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)

    def _plot_scatter(self, ax, cdim):
        xvals= np.arange(len(cdim))
        yvals= cdim["Key"].values
        ax.scatter(xvals, yvals, color="green")
        for i, (ix, val) in enumerate(zip(xvals, yvals)):
            # dimension name
            name= cdim.index[i]
            ax.text(ix, val, name, ha="center", va="bottom", rotation=60, fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("Count")

    def _plot_radar(self, ax, cdim):
        cat= cdim.index.tolist()
        val= cdim.values.tolist()
        angles= np.linspace(0, 2*np.pi, len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)

    def _plot_pie(self, ax, dist):
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)

    def _plot_bar(self, ax, cattr):
        bars= ax.bar(range(len(cattr)), cattr.values, color="blue")
        ax.set_xticks(range(len(cattr)))
        ax.set_xticklabels(cattr.index, rotation=45, ha="right", fontsize=8)
        ax.set_ylabel("Count")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, f"{int(h)}", ha="center", va="bottom")

    def _plot_bollinger(self, ax, date_ct):
        date_ct["RunDate_dt"] = pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)
        date_ct["rolling_mean"] = date_ct["Count"].rolling(window=3, min_periods=1).mean()
        date_ct["rolling_std"]  = date_ct["Count"].rolling(window=3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]   = date_ct["rolling_mean"] + 2*date_ct["rolling_std"]
        date_ct["lower_band"]   = date_ct["rolling_mean"] - 2*date_ct["rolling_std"]
        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"], color="blue", alpha=0.2, label="±2σ Band")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Count")
        ax.set_xticks(xvals)
        xlabels = [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_title("Bollinger Band Over Time")
        ax.set_xlabel("RunDate")
        ax.set_ylabel("Count")
        ax.legend()


# ----------------------------------------------------------------------------
# DASHBOARD
# ----------------------------------------------------------------------------
class AdvancedDashboard(ctk.CTkFrame):
    """
    Displays 8 live charts (Heatmap, Lollipop, Circular, Scatter, Radar, Pie, Bar, Bollinger)
    using mismatch data in df_current and entire df_history.
    Toggles top_n=10 vs all.
    """
    def __init__(self, parent, config: Dict):
        super().__init__(parent)
        dash_cfg = config.get("dashboard", {})
        self.selected_dims= set(dash_cfg.get("selected_dims", []))
        self.selected_attrs= set(dash_cfg.get("selected_attrs", []))
        self.top_n= dash_cfg.get("top_n", 10)

        self.df_current= pd.DataFrame()
        self.df_history= pd.DataFrame()

        topbar= ctk.CTkFrame(self)
        topbar.pack(fill="x", pady=5)

        self.metric_label= ctk.CTkLabel(topbar, text="Metrics: 0 missing, 0 dimension", width=300)
        self.metric_label.pack(side="left", padx=5)

        ctk.CTkButton(
            topbar, text="Toggle Top 10 / All", command=self.toggle_top_n,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="left", padx=10)

        self.notebook= ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True)

        self.chart_frames = []
        self.chart_titles = [
            "Heatmap","Lollipop","Circular","Scatter","Radar",
            "Pie","Bar","Bollinger"
        ]
        for title in self.chart_titles:
            fr= ctk.CTkFrame(self.notebook)
            fr.pack(fill="both", expand=True)
            self.notebook.add(fr, text=title)
            self.chart_frames.append(fr)

        self.figures = [None]*8
        self.canvases = [None]*8

    def toggle_top_n(self):
        if self.top_n==10:
            self.top_n= None
        else:
            self.top_n= 10
        self.update_data_filters()

    def set_data(self, df_current: pd.DataFrame, df_history: pd.DataFrame):
        self.df_current= df_current.copy()
        self.df_history= df_history.copy()
        self.update_data_filters()

    def update_data_filters(self):
        mism= len(self.df_current)
        dims= self.df_current["Dimension"].nunique() if not self.df_current.empty and "Dimension" in self.df_current.columns else 0
        self.metric_label.configure(text=f"Metrics: {mism} missing, {dims} dimension")

        self.draw_charts()

    def draw_charts(self):
        dfc= self.df_current.copy()
        if dfc.empty:
            for i in range(len(self.chart_frames)):
                self.clear_chart(i, "No data.")
            return
        df_m= dfc[dfc["Gap In"]!=""].copy()

        # Heatmap
        idx=0
        if not df_m.empty and {"Dimension","Attribute"}.issubset(df_m.columns):
            pivot= df_m.groupby(["Dimension","Attribute"]).size().unstack(fill_value=0)
            if not pivot.empty:
                self.show_heatmap(idx, pivot)
            else:
                self.clear_chart(idx, "No pivot data.")
        else:
            self.clear_chart(idx, "Not enough columns for Heatmap.")

        # Lollipop
        idx=1
        cdim= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cdim= cdim.head(self.top_n)
        if not cdim.empty:
            self.show_lollipop(idx, cdim)
        else:
            self.clear_chart(idx, "No dimension data.")

        # Circular
        idx=2
        cattr= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cattr= cattr.head(self.top_n)
        if not cattr.empty:
            self.show_circular(idx, cattr)
        else:
            self.clear_chart(idx, "No attribute data.")

        # Scatter
        idx=3
        cdim_sc= df_m.groupby("Dimension")["Key"].count().reset_index(name="Count")
        cdim_sc.sort_values("Count", ascending=False, inplace=True)
        if self.top_n:
            cdim_sc= cdim_sc.head(self.top_n)
        if not cdim_sc.empty:
            self.show_scatter(idx, cdim_sc)
        else:
            self.clear_chart(idx, "No dimension to scatter.")

        # Radar
        idx=4
        cdim_ra= df_m.groupby("Dimension")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cdim_ra= cdim_ra.head(self.top_n)
        if len(cdim_ra)>1:
            self.show_radar(idx, cdim_ra)
        else:
            self.clear_chart(idx, "Not enough data for Radar (need >=2).")

        # Pie
        idx=5
        dist= df_m["Gap In"].value_counts()
        if not dist.empty:
            self.show_pie(idx, dist)
        else:
            self.clear_chart(idx, "No 'Gap In' data.")

        # Bar
        idx=6
        cattr_b= df_m.groupby("Attribute")["Key"].count().sort_values(ascending=False)
        if self.top_n:
            cattr_b= cattr_b.head(self.top_n)
        if not cattr_b.empty:
            self.show_bar(idx, cattr_b)
        else:
            self.clear_chart(idx, "No attribute data for Bar.")

        # Bollinger
        idx=7
        if not self.df_history.empty and "RunDate" in self.df_history.columns:
            date_ct= self.df_history.groupby("RunDate")["Key"].count().reset_index(name="Count")
            date_ct.sort_values("RunDate", inplace=True)
            if not date_ct.empty:
                self.show_bollinger(idx, date_ct)
            else:
                self.clear_chart(idx, "No historical data.")
        else:
            self.clear_chart(idx, "No RunDate in history.")

    def clear_chart(self, i: int, msg="No data"):
        if self.canvases[i]:
            self.canvases[i].get_tk_widget().destroy()
        self.canvases[i]= None
        self.figures[i]= None
        for w in self.chart_frames[i].winfo_children():
            w.destroy()
        lbl= ctk.CTkLabel(self.chart_frames[i], text=msg)
        lbl.pack(expand=True)

    def create_canvas_ax(self, i: int):
        for w in self.chart_frames[i].winfo_children():
            w.destroy()
        fig= plt.Figure(figsize=(5,3), dpi=100)
        ax= fig.add_subplot(111)
        canvas= FigureCanvasTkAgg(fig, master=self.chart_frames[i])
        canvas_widget= canvas.get_tk_widget()
        canvas_widget.pack(fill="both", expand=True)
        self.canvases[i]= canvas
        self.figures[i]= fig
        return fig, ax, canvas

    def show_heatmap(self, i: int, pivot: pd.DataFrame):
        fig, ax, canvas= self.create_canvas_ax(i)
        im= ax.imshow(pivot, aspect="auto", cmap="Reds")
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns, rotation=45, ha="right")
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index)
        fig.colorbar(im, ax=ax)
        ax.set_title("Heatmap")
        canvas.draw()

    def show_lollipop(self, i: int, cdim: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        ax.hlines(y=cdim.index, xmin=0, xmax=cdim.values, color="skyblue")
        ax.plot(cdim.values, cdim.index, "o", color="skyblue")
        ax.set_xlabel("Count")
        ax.set_title("Lollipop - Dimension")
        canvas.draw()

    def show_circular(self, i: int, cattr: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        angles= np.linspace(0, 2*np.pi, len(cattr), endpoint=False)
        ax.set_xticks(angles)
        ax.set_xticklabels(cattr.index, fontsize=9)
        ax.bar(angles, cattr.values, width=0.4, color="orange", alpha=0.6)
        ax.set_title("Circular - Attribute")
        canvas.draw()

    def show_scatter(self, i: int, df: pd.DataFrame):
        fig, ax, canvas= self.create_canvas_ax(i)
        xvals= np.arange(len(df))
        yvals= df["Count"].values
        ax.scatter(xvals, yvals, color="green")
        for j, row in df.iterrows():
            ax.text(xvals[j], yvals[j], row["Dimension"], ha="center", va="bottom", rotation=60, fontsize=8)
        ax.set_xticks([])
        ax.set_ylabel("Count")
        ax.set_title("Scatter - Dimension")
        canvas.draw()

    def show_radar(self, i: int, s: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        cat= s.index.tolist()
        val= s.values.tolist()
        angles= np.linspace(0, 2*np.pi, len(cat), endpoint=False).tolist()
        angles+= angles[:1]
        val+= val[:1]
        ax.set_theta_offset(np.pi/2)
        ax.set_theta_direction(-1)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(cat, fontsize=9)
        ax.plot(angles, val, color="red", linewidth=2)
        ax.fill(angles, val, color="red", alpha=0.3)
        ax.set_title("Radar - Dimension")
        canvas.draw()

    def show_pie(self, i: int, dist: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        ax.pie(dist.values, labels=dist.index, autopct="%.1f%%", startangle=140)
        ax.set_title("Pie - Gap In Distribution")
        canvas.draw()

    def show_bar(self, i: int, s: pd.Series):
        fig, ax, canvas= self.create_canvas_ax(i)
        bars= ax.bar(range(len(s)), s.values, color="blue")
        ax.set_xticks(range(len(s)))
        ax.set_xticklabels(s.index, rotation=45, ha="right", fontsize=8)
        ax.set_ylabel("Count")
        ax.set_title("Bar - Attribute")
        for bar in bars:
            h= bar.get_height()
            ax.text(bar.get_x()+ bar.get_width()/2., h, f"{int(h)}", ha="center", va="bottom")
        canvas.draw()

    def show_bollinger(self, i: int, date_ct: pd.DataFrame):
        fig, ax, canvas= self.create_canvas_ax(i)
        date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
        date_ct.sort_values("RunDate_dt", inplace=True)
        date_ct.reset_index(drop=True, inplace=True)
        date_ct["rolling_mean"]= date_ct["Count"].rolling(window=3, min_periods=1).mean()
        date_ct["rolling_std"]= date_ct["Count"].rolling(window=3, min_periods=1).std(ddof=0)
        date_ct["upper_band"]= date_ct["rolling_mean"]+ 2*date_ct["rolling_std"]
        date_ct["lower_band"]= date_ct["rolling_mean"]- 2*date_ct["rolling_std"]
        xvals= np.arange(len(date_ct))
        ax.plot(xvals, date_ct["rolling_mean"], color="blue", label="Rolling Mean")
        ax.fill_between(xvals, date_ct["lower_band"], date_ct["upper_band"], color="blue", alpha=0.2, label="±2σ")
        ax.scatter(xvals, date_ct["Count"], color="red", label="Count")
        ax.set_xticks(xvals)
        xlabels = [d.strftime("%Y-%m-%d") if not pd.isna(d) else "" for d in date_ct["RunDate_dt"]]
        ax.set_xticklabels(xlabels, rotation=45, ha="right")
        ax.set_title("Bollinger Over Time")
        ax.legend()
        canvas.draw()


# ----------------------------------------------------------------------------
# HISTORY
# ----------------------------------------------------------------------------
class HistoryTab(ctk.CTkFrame):
    """
    Shows all run_*.json from 'history_runs'. Double-click => summary or full JSON.
    """
    def __init__(self, parent, hist_dir: Path):
        super().__init__(parent)
        self.history_dir= hist_dir
        self.build()

    def build(self):
        lbl= ctk.CTkLabel(self, text="Reconciliation Runs History", font=("Arial",16))
        lbl.pack(pady=5)

        self.tree= ttk.Treeview(self, columns=("File",), show="headings", height=15)
        self.tree.heading("File", text="History File")
        self.tree.pack(fill="both", expand=True, padx=10, pady=10)
        self.tree.bind("<Double-1>", self.on_dbclick)

        btn= ctk.CTkButton(self, text="Refresh", command=self.refresh,
                           fg_color="#800020", hover_color="#a52a2a", text_color="white")
        btn.pack(pady=5)

        self.refresh()

    def refresh(self):
        for i in self.tree.get_children():
            self.tree.delete(i)
        if not self.history_dir.is_dir():
            self.history_dir.mkdir(parents=True, exist_ok=True)
        fs= sorted(self.history_dir.glob("run_*.json"), reverse=True)
        for f in fs:
            self.tree.insert("", "end", values=(f.name,))

    def on_dbclick(self, event):
        sel= self.tree.focus()
        if not sel:
            return
        fname= self.tree.item(sel,"values")[0]
        path= self.history_dir/fname
        if not path.is_file():
            return
        pop= tk.Toplevel(self)
        pop.title(f"Summary for {fname}")
        pop.geometry("600x400")

        try:
            with open(path, "r", encoding="utf-8") as ff:
                data = json.load(ff)
            df = pd.DataFrame(data)

            if df.empty:
                summary_text= f"{fname} => No data in file."
            else:
                total= len(df)
                dims= df["Dimension"].nunique() if "Dimension" in df.columns else 0
                gaps= df["Gap In"].value_counts() if "Gap In" in df.columns else {}
                lines = [f"Total Rows: {total}",
                         f"Distinct Dimensions: {dims}"]
                for g, c in gaps.items():
                    lines.append(f"Gap In={g}: {c}")
                summary_text= "\n".join(lines)

            summary_lbl= ctk.CTkLabel(pop, text=summary_text, justify="left")
            summary_lbl.pack(pady=5)

            def show_full():
                fullpop= tk.Toplevel(pop)
                fullpop.title(f"Full JSON for {fname}")
                fullpop.geometry("800x600")
                txt= ctk.CTkTextbox(fullpop)
                txt.pack(fill="both", expand=True)
                txt.insert("end", json.dumps(data, indent=2))
                txt.configure(state="disabled")

            ctk.CTkButton(pop, text="Show Full File", command=show_full,
                          fg_color="#800020", hover_color="#a52a2a", text_color="white").pack(pady=10)
        except Exception as e:
            logging.error(f"History => {e}")
            ctk.CTkLabel(pop, text=f"Error reading {path} => {e}").pack()


# ----------------------------------------------------------------------------
# MAIN APP
# ----------------------------------------------------------------------------
class MainApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("Ultra-Mega Reconciliation: Two-Sheet & Gap In Edition")
        self.geometry("1600x900")
        ctk.set_appearance_mode("light")

        # Use a frame for the main content, so the bottom button stays visible
        main_frame= ctk.CTkFrame(self)
        main_frame.pack(fill="both", expand=True)

        self.protocol("WM_DELETE_WINDOW", self.on_close)

        # 1) load config
        self.config_dict= load_config(Path(DEFAULT_PATHS["CONFIG_PATH"]))
        # 2) read param file for dimension/attribute mappings
        self.param_dict= read_param_file(Path(self.config_dict["paths"].get("PARAMETER_PATH",DEFAULT_PATHS["PARAMETER_PATH"])))
        # 3) prepare empty history
        self.history_df= pd.DataFrame()

        # We do NOT show a "Paths" tab. We read paths from config behind the scenes.
        self.erp_path = Path(self.config_dict["paths"].get("ERP_EXCEL_PATH", DEFAULT_PATHS["ERP_EXCEL_PATH"]))
        self.mast_zip = Path(self.config_dict["paths"].get("MASTER_ZIP_PATH", DEFAULT_PATHS["MASTER_ZIP_PATH"]))
        self.exc_path = Path(self.config_dict["paths"].get("EXCEPTION_PATH", DEFAULT_PATHS["EXCEPTION_PATH"]))
        self.out_path = Path(self.config_dict["paths"].get("OUTPUT_PATH", DEFAULT_PATHS["OUTPUT_PATH"]))

        self.cfg_path = Path(self.config_dict["paths"].get("CONFIG_PATH", DEFAULT_PATHS["CONFIG_PATH"]))
        self.par_path = Path(self.config_dict["paths"].get("PARAMETER_PATH", DEFAULT_PATHS["PARAMETER_PATH"]))
        self.csv_dir = Path(self.config_dict["paths"].get("MASTER_CSV_OUTPUT", DEFAULT_PATHS["MASTER_CSV_OUTPUT"]))
        self.pdf_path= Path(self.config_dict["paths"].get("PDF_EXPORT_PATH", DEFAULT_PATHS["PDF_EXPORT_PATH"]))
        self.logo_path= Path(self.config_dict["paths"].get("LOGO_PATH", DEFAULT_PATHS["LOGO_PATH"]))
        self.hist_path= Path(self.config_dict["paths"].get("HISTORY_PATH", DEFAULT_PATHS["HISTORY_PATH"]))
        self.band_json_path= Path(self.config_dict["paths"].get("BAND_CHART_JSON_PATH", DEFAULT_PATHS["BAND_CHART_JSON_PATH"]))

        # Tab control
        self.tabs= ttk.Notebook(main_frame)
        self.tabs.pack(fill="both", expand=True)

        # 1) ERP Preview
        self.tab_erp= ctk.CTkFrame(self.tabs)
        erp_filters= self.config_dict.get("erp_grid",{}).get("filters",{})
        self.erp_preview= SimplePreview(self.tab_erp,"ERP",filters_dict=erp_filters)
        self.erp_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_erp, text="ERP Preview")

        # 2) Master Preview
        self.tab_master= ctk.CTkFrame(self.tabs)
        mast_filters= self.config_dict.get("master_grid",{}).get("filters",{})
        self.master_preview= SimplePreview(self.tab_master,"Master",filters_dict=mast_filters)
        self.master_preview.pack(fill="both", expand=True)
        self.tabs.add(self.tab_master, text="Master Preview")

        # 3) Compare
        self.tab_compare= ctk.CTkFrame(self.tabs)
        self.build_compare_tab(self.tab_compare)
        self.tabs.add(self.tab_compare, text="Compare")

        # 4) Dashboard
        self.dashboard_tab= AdvancedDashboard(self.tabs, self.config_dict)
        self.tabs.add(self.dashboard_tab, text="Dashboard")

        # 5) History
        self.history_tab= HistoryTab(self.tabs, self.hist_path)
        self.tabs.add(self.history_tab, text="History")

        # bottom log box
        self.log_box= ctk.CTkTextbox(main_frame, height=120)
        self.log_box.pack(fill="both", side="bottom")
        self.log_box.configure(state="disabled")
        handler= TextHandler(self.log_box)
        handler.setLevel(logging.INFO)
        logging.getLogger().addHandler(handler)

        # temp CSV dir for Master
        self.temp_csv_dir= self.csv_dir
        self.temp_csv_dir.mkdir(parents=True, exist_ok=True)

        # load older runs from JSON => self.history_df
        self.load_all_runs()

        # meltdown => show preview
        self.refresh_erp()
        self.refresh_master()

        # bottom button frame
        bottom_frame= ctk.CTkFrame(self)
        bottom_frame.pack(side="bottom", fill="x", pady=5)
        ctk.CTkButton(
            bottom_frame, text="Close Script", command=self.on_close,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(side="right", padx=10)

        # push entire history => dash
        self.dashboard_tab.set_data(pd.DataFrame(), self.history_df)

    def build_compare_tab(self, parent):
        frm= ctk.CTkFrame(parent)
        frm.pack(fill="both", expand=True, padx=10, pady=10)
        ctk.CTkLabel(frm, text="Generate Missing Items", font=("Arial",16)).pack(pady=5)

        ctk.CTkButton(
            frm, text="Run Reconciliation",
            command=self.run_comparison,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(pady=10)

        ctk.CTkButton(
            frm, text="Export PDF Report",
            command=self.export_pdf,
            fg_color="#800020", hover_color="#a52a2a", text_color="white"
        ).pack(pady=10)

    def load_all_runs(self):
        if not self.hist_path.is_dir():
            return
        frames=[]
        for jf in self.hist_path.glob("run_*.json"):
            try:
                jdata= pd.read_json(jf, orient="records")
                frames.append(jdata)
            except Exception as e:
                logging.error(f"Error reading {jf} => {e}")
        if frames:
            big= pd.concat(frames, ignore_index=True)
            if self.history_df.empty:
                self.history_df= big
            else:
                self.history_df= pd.concat([self.history_df, big], ignore_index=True)
            self.history_df.drop_duplicates(inplace=True)
            logging.info(f"Loaded all runs => total {len(self.history_df)} records from {self.hist_path}")

    def refresh_erp(self):
        raw_erp= read_erp_excel(self.erp_path)
        if raw_erp.empty:
            self.erp_preview.set_data(pd.DataFrame())
            return
        meltdown= meltdown_erp_for_preview(raw_erp, self.param_dict)
        pivoted= pivot_for_preview(meltdown)
        self.erp_preview.set_data(pivoted)

    def refresh_master(self):
        csvs= convert_master_txt_to_csv(self.mast_zip, self.temp_csv_dir)
        raw_mast= unify_master_csvs(csvs)
        if raw_mast.empty:
            self.master_preview.set_data(pd.DataFrame())
            return
        meltdown= meltdown_master_for_preview(raw_mast, self.param_dict)
        pivoted= pivot_for_preview(meltdown)
        self.master_preview.set_data(pivoted)

    def run_comparison(self):
        df_erp_wide= self.erp_preview.get_filtered_df()
        df_mast_wide= self.master_preview.get_filtered_df()

        # meltdown back
        erp_long= melt_back(df_erp_wide)
        erp_long= build_keys(erp_long)
        mast_long= melt_back(df_mast_wide)
        mast_long= build_keys(mast_long)

        # compare => "Gap In"
        df_diff= compare_mode2(erp_long, mast_long)

        # merge exceptions
        df_exc= read_exception_table(self.exc_path)
        final= merge_exceptions(df_diff, df_exc)

        # separate case diffs
        mismatch, case_df= separate_case_diffs(final)

        # write 2 sheets
        write_two_sheet_excel(mismatch, case_df, self.out_path)

        # Add to history
        run_timestamp= datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        mismatch["RunDate"]= run_timestamp
        case_df["RunDate"]= run_timestamp
        appended= pd.concat([mismatch, case_df], ignore_index=True)
        if self.history_df.empty:
            self.history_df= appended.copy()
        else:
            self.history_df= pd.concat([self.history_df, appended], ignore_index=True)

        # save JSON
        self.hist_path.mkdir(parents=True, exist_ok=True)
        run_file= self.hist_path / f"run_{run_timestamp.replace(':','-').replace(' ','_')}.json"
        try:
            appended.to_json(run_file, orient="records", indent=2)
            logging.info(f"Saved run => {run_file}")
        except Exception as e:
            logging.error(f"Error writing JSON => {e}")

        # update dash
        self.dashboard_tab.set_data(appended, self.history_df)
        self.history_tab.refresh()

        self.tabs.select(self.dashboard_tab)
        messagebox.showinfo("Done", f"Missing items => {self.out_path}")

    def export_pdf(self):
        if self.history_df.empty:
            messagebox.showinfo("PDF Export", "No mismatch data => history is empty.")
            return
        if "RunDate" in self.history_df.columns:
            last_run= self.history_df["RunDate"].max()
            df_current= self.history_df[self.history_df["RunDate"]== last_run].copy()
        else:
            df_current= self.history_df.copy()

        df_history= self.history_df.copy()
        rep= EnhancedPDFReport(df_current, df_history, self.config_dict)
        pdf_path= rep.generate()
        messagebox.showinfo("PDF Export", f"PDF exported => {pdf_path}")

    def save_all_config(self):
        cfg = self.config_dict
        cfg["paths"]["ERP_EXCEL_PATH"] = str(self.erp_path)
        cfg["paths"]["MASTER_ZIP_PATH"] = str(self.mast_zip)
        cfg["paths"]["EXCEPTION_PATH"] = str(self.exc_path)
        cfg["paths"]["OUTPUT_PATH"] = str(self.out_path)
        cfg["paths"]["CONFIG_PATH"] = str(self.cfg_path)
        cfg["paths"]["PARAMETER_PATH"] = str(self.par_path)
        cfg["paths"]["MASTER_CSV_OUTPUT"] = str(self.csv_dir)
        cfg["paths"]["PDF_EXPORT_PATH"] = str(self.pdf_path)
        cfg["paths"]["LOGO_PATH"] = str(self.logo_path)
        cfg["paths"]["HISTORY_PATH"] = str(self.hist_path)
        cfg["paths"]["BAND_CHART_JSON_PATH"] = str(self.band_json_path)

        cfg.setdefault("erp_grid", {})
        cfg["erp_grid"]["filters"] = self.erp_preview.filters

        cfg.setdefault("master_grid", {})
        cfg["master_grid"]["filters"] = self.master_preview.filters

        dash_cfg= cfg.setdefault("dashboard", {})
        dash_cfg["selected_dims"]= list(self.dashboard_tab.selected_dims)
        dash_cfg["selected_attrs"]= list(self.dashboard_tab.selected_attrs)
        dash_cfg["top_n"]= self.dashboard_tab.top_n

        save_config(cfg, self.cfg_path)

    def on_close(self):
        # 1) Save config
        self.save_all_config()

        # 2) store Bollinger from entire history
        if not self.history_df.empty and "RunDate" in self.history_df.columns:
            try:
                outp= self.band_json_path
                date_ct= self.history_df.groupby("RunDate")["Key"].count().reset_index(name="Count")
                date_ct["RunDate_dt"]= pd.to_datetime(date_ct["RunDate"], errors="coerce")
                date_ct.sort_values("RunDate_dt", inplace=True)
                date_ct.reset_index(drop=True, inplace=True)
                date_ct["rolling_mean"]= date_ct["Count"].rolling(3, min_periods=1).mean()
                date_ct["rolling_std"]= date_ct["Count"].rolling(3, min_periods=1).std(ddof=0)
                date_ct["upper_band"]= date_ct["rolling_mean"]+2*date_ct["rolling_std"]
                date_ct["lower_band"]= date_ct["rolling_mean"]-2*date_ct["rolling_std"]
                date_ct["RunDate"]= date_ct["RunDate_dt"].dt.strftime("%Y-%m-%d %H:%M:%S")
                date_ct.drop(columns=["RunDate_dt"], inplace=True)
                date_ct.to_json(outp, orient="records", indent=2)
                logging.info(f"Bollinger data saved on close => {outp}")
            except Exception as e:
                logging.error(f"Bollinger JSON => {e}")
        self.destroy()


def main():
    app= MainApp()
    app.mainloop()

if __name__=="__main__":
    main()
