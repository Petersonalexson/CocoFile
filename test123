#!/usr/bin/env python3
"""
Revised script to compare Alfa and Gamma data.
It uses one exceptions table that provides:
  - A key (in the format "Dimension | NameID | Attribute | Value")
  - Comments (Comments_1 and Comments_2)
  - A hide flag ("hide exception" == "yes") to filter out exceptions.

Missing items (differences between Alfa and Gamma) are reported in an Excel file
with color formatting (blue for items missing in Alfa, green for items missing in Gamma).
"""

import os
import zipfile
import pandas as pd
from pathlib import Path
import logging
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font

# -----------------------------------------
# 0) SETUP LOGGING
# -----------------------------------------
def setup_logging(log_file: Path) -> None:
    """Configures logging to output to both console and a log file."""
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Create handlers
    c_handler = logging.StreamHandler()
    f_handler = logging.FileHandler(log_file, mode='w')

    c_handler.setLevel(logging.INFO)
    f_handler.setLevel(logging.DEBUG)

    # Create formatters and add them to the handlers
    c_format = logging.Formatter('%(levelname)s: %(message)s')
    f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    c_handler.setFormatter(c_format)
    f_handler.setFormatter(f_format)

    # Add handlers to the logger
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)


# -----------------------------------------
# 1) READ EXCEPTION TABLE (ONLY ONE TABLE)
# -----------------------------------------
def read_exception_table(exception_table_path: Path) -> pd.DataFrame:
    """
    Reads an Excel file with columns: Key, Comments_1, Comments_2, hide exception.
    Returns a DataFrame with these columns.
    """
    try:
        if not exception_table_path.is_file():
            logging.warning(f"[Exception] File not found: {exception_table_path}. No exceptions applied.")
            return pd.DataFrame(columns=["Key", "Comments_1", "Comments_2", "hide exception"])

        df_exc = pd.read_excel(exception_table_path, sheet_name="Sheet1")
        required_columns = {"Key", "Comments_1", "Comments_2", "hide exception"}
        if not required_columns.issubset(df_exc.columns):
            missing_cols = required_columns - set(df_exc.columns)
            logging.warning(f"[Exception] Missing columns {missing_cols}. No exceptions applied.")
            return pd.DataFrame(columns=list(required_columns))

        logging.info(f"[Exception] Loaded exceptions for {len(df_exc)} keys.")
        return df_exc
    except Exception as e:
        logging.error(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame(columns=["Key", "Comments_1", "Comments_2", "hide exception"])


# -----------------------------------------
# 2) PRE-MELT FILTER (Remove Rows by Column)
# -----------------------------------------
def filter_pre_melt(df: pd.DataFrame, exclude_rules: list = None) -> pd.DataFrame:
    """
    Excludes rows if a specified column's value is in a given list.
    exclude_rules: list of tuples: (columnName, [values_to_remove])
    Example: [("ColA", ["X"]), ("ColB", ["Y"])] => remove if ColA=="X" OR ColB=="Y".
    """
    if not exclude_rules:
        return df

    combined_mask = pd.Series(False, index=df.index)
    for (col, bad_values) in exclude_rules:
        if col in df.columns:
            mask = df[col].isin(bad_values)
            logging.debug(f"[Pre-Melt] Excluding rows where {col} in {bad_values}: {mask.sum()} rows")
            combined_mask |= mask
        else:
            logging.warning(f"[Pre-Melt] Column '{col}' not found in DataFrame.")
    df_out = df[~combined_mask]
    logging.info(f"[Pre-Melt] Rows after filtering: {len(df_out)}")
    return df_out


# -----------------------------------------
# 3) POST-MELT EXCLUSION (Dimension/Attribute)
# -----------------------------------------
def exclude_dimension_attribute(df: pd.DataFrame,
                                bad_dimensions: list = None,
                                bad_attributes: list = None) -> pd.DataFrame:
    """
    Removes rows whose Dimension is in bad_dimensions OR whose Attribute is in bad_attributes.
    """
    initial_count = len(df)
    if bad_dimensions:
        df = df[~df["Dimension"].isin(bad_dimensions)]
        excluded_dims = initial_count - len(df)
        logging.debug(f"[Post-Melt] Excluded Dimensions {bad_dimensions}: {excluded_dims} rows removed")
        initial_count = len(df)
    if bad_attributes:
        df = df[~df["Attribute"].isin(bad_attributes)]
        excluded_attrs = initial_count - len(df)
        logging.debug(f"[Post-Melt] Excluded Attributes {bad_attributes}: {excluded_attrs} rows removed")
    logging.info(f"[Post-Melt] Rows after exclusion: {len(df)}")
    return df


# -----------------------------------------
# Helper function: Build Keys
# -----------------------------------------
def build_key(row: pd.Series, include_name: bool = True) -> str:
    """
    Constructs a key string.
      If include_name is True: "Dimension | NameID | Attribute | Value"
      Otherwise: "Dimension | Attribute | Value"
    Each component is stripped and converted to string.
    """
    dimension = str(row.get("Dimension", "")).strip()
    nameid = str(row.get("NameID", "")).strip()
    attribute = str(row.get("Attribute", "")).strip()
    value = str(row.get("Value", "")).strip()
    if include_name:
        return f"{dimension} | {nameid} | {attribute} | {value}"
    else:
        return f"{dimension} | {attribute} | {value}"


# -----------------------------------------
# 4) TRANSFORM ALFA
# -----------------------------------------
def transform_alfa(
    file_path: Path,
    pre_melt_exclude_rules: list = None,
    post_melt_bad_dimensions: list = None,
    post_melt_bad_attributes: list = None,
    dimension_rename_dict: dict = None,
    attribute_rename_dict: dict = None,
    sheet_name: str = "Sheet1",
    skip_rows: int = 3
) -> pd.DataFrame:
    """
    Transforms Alfa (Excel) data.
    Reads the data, applies pre-melt filtering, renames columns, melts the DataFrame,
    applies post-melt exclusions/renames, builds keys, and removes duplicates.
    """
    try:
        if not file_path.is_file():
            logging.error(f"[Alfa] File not found: {file_path}")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        # 1) Read Excel and check number of columns
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        logging.debug(f"[Alfa] Initial rows: {len(df)}")
        if df.shape[1] < 4:
            logging.warning("[Alfa] Fewer than 4 columns. Returning empty DataFrame.")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        # 2) Apply pre-melt filtering if needed
        df = filter_pre_melt(df, pre_melt_exclude_rules)

        # 3) Rename columns: assume the third column is Dimension and the fourth is NameID
        df.rename(columns={
            df.columns[2]: "Dimension",
            df.columns[3]: "NameID"
        }, inplace=True)
        logging.debug(f"[Alfa] Columns after renaming: {df.columns.tolist()}")

        # 4) Create a 'Name' column (if needed for later use)
        df['Name'] = df['NameID']
        logging.debug(f"[Alfa] Added 'Name' column.")

        # 5) Melt the DataFrame: id_vars are Dimension and NameID; melt the rest into Attribute/Value
        id_vars = ["Dimension", "NameID"]
        val_vars = [col for col in df.columns if col not in id_vars]
        logging.debug(f"[Alfa] id_vars: {id_vars}, val_vars: {val_vars}")
        df_melt = df.melt(id_vars=id_vars, value_vars=val_vars, var_name="Attribute", value_name="Value")
        logging.debug(f"[Alfa] Rows after melt: {len(df_melt)}")

        # 6) Rename dimensions/attributes if dictionaries are provided
        if dimension_rename_dict:
            df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
            logging.debug(f"[Alfa] Dimensions after renaming: {df_melt['Dimension'].unique()}")
        if attribute_rename_dict:
            df_melt["Attribute"] = df_melt["Attribute"].replace(attribute_rename_dict)
            logging.debug(f"[Alfa] Attributes after renaming: {df_melt['Attribute'].unique()}")

        # 7) Exclude rows based on bad dimensions or attributes
        df_melt = exclude_dimension_attribute(df_melt, bad_dimensions=post_melt_bad_dimensions, bad_attributes=post_melt_bad_attributes)

        # 8) Build keys (do not exclude items now; filtering based on exceptions will be done later)
        df_melt["Key_NoName"] = df_melt.apply(lambda row: build_key(row, include_name=False), axis=1)
        df_melt["Key"]       = df_melt.apply(lambda row: build_key(row, include_name=True), axis=1)

        # 9) Remove duplicate rows (if any)
        before_dedup = len(df_melt)
        df_melt.drop_duplicates(subset=["Key", "Key_NoName"], inplace=True)
        after_dedup = len(df_melt)
        logging.info(f"[Alfa] Removed {before_dedup - after_dedup} duplicate rows.")

        final_df = df_melt[["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"]]
        logging.info(f"[Alfa] Final rows: {len(final_df)}")
        return final_df

    except Exception as e:
        logging.error(f"[Alfa] Error during transformation: {e}")
        return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])


# -----------------------------------------
# 5) TRANSFORM GAMMA (Similar to ALFA)
# -----------------------------------------
def transform_gamma(
    zip_file_path: Path,
    pre_melt_exclude_rules: list = None,
    post_melt_bad_dimensions: list = None,
    post_melt_bad_attributes: list = None,
    dimension_rename_dict: dict = None,
    attribute_rename_dict: dict = None,
    delimiter: str = ",",
    remove_substring: str = "_ceaster.txt"
) -> pd.DataFrame:
    """
    Transforms Gamma data stored in a ZIP file.
    Each .txt file inside is processed: the first column is renamed to NameID,
    a Dimension is derived from the filename, and the file is melted into long form.
    """
    try:
        if not zip_file_path.is_file():
            logging.error(f"[Gamma] ZIP not found: {zip_file_path}")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        def compute_dimension_name(filename: str, remove_sub: str) -> str:
            base = os.path.basename(filename)
            if remove_sub in base:
                base = base.replace(remove_sub, "")
            else:
                base, _ = os.path.splitext(base)
            return base.replace("_", " ")

        all_dfs = []
        with zipfile.ZipFile(zip_file_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.endswith(".txt")]
            if not txt_files:
                logging.warning("[Gamma] No .txt files found in the ZIP.")
                return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

            for txt_file in txt_files:
                try:
                    # 1) Compute Dimension from filename
                    dimension_name = compute_dimension_name(txt_file, remove_substring)
                    logging.debug(f"[Gamma] Processing file: {txt_file}, Dimension: {dimension_name}")

                    # 2) Read the text file into a DataFrame
                    with z.open(txt_file) as fo:
                        df_in = pd.read_csv(fo, delimiter=delimiter)
                    logging.debug(f"[Gamma] Initial rows in {txt_file}: {len(df_in)}")
                    if df_in.empty:
                        logging.warning(f"[Gamma] File {txt_file} is empty. Skipping.")
                        continue

                    # 3) Rename the first column to 'NameID'
                    first_col = df_in.columns[0]
                    df_in.rename(columns={first_col: "NameID"}, inplace=True)

                    # 4) Replace missing NameID values
                    if df_in["NameID"].isnull().any():
                        logging.warning(f"[Gamma] Some NameID values are blank in {txt_file}. Filling with 'Unknown'.")
                        df_in["NameID"] = df_in["NameID"].fillna("Unknown")

                    # 5) Apply pre-melt filtering if provided
                    df_in = filter_pre_melt(df_in, pre_melt_exclude_rules)
                    logging.debug(f"[Gamma] Rows after pre-melt filtering in {txt_file}: {len(df_in)}")

                    # 6) Add Dimension column
                    df_in["Dimension"] = dimension_name

                    # 7) Create a 'Name' column
                    df_in['Name'] = df_in['NameID']

                    # 8) Melt all other columns into Attribute/Value pairs
                    id_vars = ["Dimension", "NameID"]
                    val_vars = [col for col in df_in.columns if col not in id_vars]
                    if not val_vars:
                        logging.warning(f"[Gamma] No columns to melt for {txt_file}. Skipping.")
                        continue

                    df_melt = df_in.melt(id_vars=id_vars, value_vars=val_vars, var_name="Attribute", value_name="Value")

                    # 9) Rename dimensions/attributes if dictionaries are provided
                    if dimension_rename_dict:
                        df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
                    if attribute_rename_dict:
                        df_melt["Attribute"] = df_melt["Attribute"].replace(attribute_rename_dict)

                    # 10) Exclude rows based on bad dimensions/attributes
                    df_melt = exclude_dimension_attribute(df_melt, bad_dimensions=post_melt_bad_dimensions, bad_attributes=post_melt_bad_attributes)

                    # 11) Build keys
                    df_melt["Key_NoName"] = df_melt.apply(lambda row: build_key(row, include_name=False), axis=1)
                    df_melt["Key"]       = df_melt.apply(lambda row: build_key(row, include_name=True), axis=1)

                    # 12) Remove duplicates
                    before_dedup = len(df_melt)
                    df_melt.drop_duplicates(subset=["Key", "Key_NoName"], inplace=True)
                    after_dedup = len(df_melt)
                    logging.info(f"[Gamma] Removed {before_dedup - after_dedup} duplicate rows in {txt_file}.")

                    final_df = df_melt[["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"]]
                    all_dfs.append(final_df)
                except Exception as inner_e:
                    logging.error(f"[Gamma] Error processing file {txt_file}: {inner_e}")
                    continue

        if all_dfs:
            df_gamma = pd.concat(all_dfs, ignore_index=True)
            logging.info(f"[Gamma] Total rows after concatenation: {len(df_gamma)}")
        else:
            logging.warning("[Gamma] No data collected from ZIP.")
            df_gamma = pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        return df_gamma

    except Exception as e:
        logging.error(f"[Gamma] Error during transformation: {e}")
        return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])


# -----------------------------------------
# 6) CREATE MISSING ITEMS EXCEL
# -----------------------------------------
def create_missing_items_excel(
    df_alfa: pd.DataFrame,
    df_gamma: pd.DataFrame,
    df_exceptions: pd.DataFrame,
    output_path: Path
) -> None:
    """
    Identifies missing items between Alfa and Gamma, then merges with the exceptions table
    to include any Comments_1 and Comments_2. Rows with hide exception == 'yes' are filtered out.
    The final Excel report contains:
      - Key, Dimension, NameID, Attribute, Value, Comments_1, Comments_2, Action Item, Missing In
    Missing rows are colored:
      - Green if missing in Gamma
      - Blue if missing in Alfa
    """
    try:
        logging.info("[Missing Items] Starting comparison between Alfa and Gamma.")

        # 1) Identify dimensions present in both Alfa and Gamma
        dimensions_alfa = set(df_alfa['Dimension'].unique())
        dimensions_gamma = set(df_gamma['Dimension'].unique())
        matching_dimensions = dimensions_alfa.intersection(dimensions_gamma)
        logging.info(f"[Missing Items] Matching Dimensions Count: {len(matching_dimensions)}")

        # 2) Filter to only matching dimensions
        df_alfa_matching = df_alfa[df_alfa['Dimension'].isin(matching_dimensions)].copy()
        df_gamma_matching = df_gamma[df_gamma['Dimension'].isin(matching_dimensions)].copy()

        # 3) Group by Key_NoName to compare NameIDs between Alfa and Gamma
        alfa_group = df_alfa_matching.groupby("Key_NoName")["NameID"].apply(set).reset_index().rename(columns={"NameID": "NameIDs_Alfa"})
        gamma_group = df_gamma_matching.groupby("Key_NoName")["NameID"].apply(set).reset_index().rename(columns={"NameID": "NameIDs_Gamma"})

        merged_group = pd.merge(alfa_group, gamma_group, on="Key_NoName", how="inner")
        merged_group["Missing_In_Gamma"] = merged_group.apply(lambda row: row["NameIDs_Alfa"] - row["NameIDs_Gamma"], axis=1)
        merged_group["Missing_In_Alfa"]  = merged_group.apply(lambda row: row["NameIDs_Gamma"] - row["NameIDs_Alfa"], axis=1)

        # 4) Collect missing items
        missing_items = []
        for _, row in merged_group.iterrows():
            key_noname = row["Key_NoName"]
            missing_in_gamma = row["Missing_In_Gamma"]
            missing_in_alfa  = row["Missing_In_Alfa"]

            # If there are missing NameIDs, process these first.
            if missing_in_gamma or missing_in_alfa:
                for nameid in missing_in_gamma:
                    details = df_alfa_matching[(df_alfa_matching['NameID'] == nameid) &
                                               (df_alfa_matching['Key_NoName'] == key_noname)]
                    if not details.empty:
                        details = details.iloc[0]
                        missing_items.append({
                            "Key": details["Key"],
                            "Dimension": details["Dimension"],
                            "NameID": nameid,
                            "Attribute": details["Attribute"],
                            "Value": details["Value"],
                            "Missing In": "Gamma"
                        })
                for nameid in missing_in_alfa:
                    details = df_gamma_matching[(df_gamma_matching['NameID'] == nameid) &
                                                (df_gamma_matching['Key_NoName'] == key_noname)]
                    if not details.empty:
                        details = details.iloc[0]
                        missing_items.append({
                            "Key": details["Key"],
                            "Dimension": details["Dimension"],
                            "NameID": nameid,
                            "Attribute": details["Attribute"],
                            "Value": details["Value"],
                            "Missing In": "Alfa"
                        })
                continue  # Skip further property-level comparisons for this key

            # If all NameIDs match, then compare full keys
            alfa_keys = set(df_alfa_matching[df_alfa_matching['Key_NoName'] == key_noname]['Key'])
            gamma_keys = set(df_gamma_matching[df_gamma_matching['Key_NoName'] == key_noname]['Key'])
            missing_keys_in_gamma = alfa_keys - gamma_keys
            for key in missing_keys_in_gamma:
                details = df_alfa_matching[df_alfa_matching['Key'] == key].iloc[0]
                missing_items.append({
                    "Key": details["Key"],
                    "Dimension": details["Dimension"],
                    "NameID": details["NameID"],
                    "Attribute": details["Attribute"],
                    "Value": details["Value"],
                    "Missing In": "Gamma"
                })
            missing_keys_in_alfa = gamma_keys - alfa_keys
            for key in missing_keys_in_alfa:
                details = df_gamma_matching[df_gamma_matching['Key'] == key].iloc[0]
                missing_items.append({
                    "Key": details["Key"],
                    "Dimension": details["Dimension"],
                    "NameID": details["NameID"],
                    "Attribute": details["Attribute"],
                    "Value": details["Value"],
                    "Missing In": "Alfa"
                })

        df_missing = pd.DataFrame(missing_items)
        logging.info(f"[Missing Items] Total missing items after processing: {len(df_missing)}")

        if df_missing.empty:
            logging.info("[Missing Items] No missing items to report.")
            return

        # 5) Normalize keys (strip and lower) for merging with exceptions.
        df_missing["Key"] = df_missing["Key"].astype(str).str.strip().str.lower()
        df_exceptions["Key"] = df_exceptions["Key"].astype(str).str.strip().str.lower()

        # Merge missing items with exception table (to attach Comments_1, Comments_2 and hide flag)
        df_missing = df_missing.merge(df_exceptions, on='Key', how='left')
        logging.info(f"[Missing Items] Merged with exception table. Rows: {len(df_missing)}")

        # 6) Exclude rows where hide exception is "yes"
        df_missing['hide exception'] = df_missing['hide exception'].astype(str).str.lower().fillna('no')
        before_filter = len(df_missing)
        df_missing = df_missing[df_missing['hide exception'] != 'yes']
        after_filter = len(df_missing)
        logging.info(f"[Missing Items] Excluded {before_filter - after_filter} rows based on hide exception flag.")

        # 7) Build final DataFrame with additional columns
        df_missing['Action Item'] = ''
        final_columns = ['Key', 'Dimension', 'NameID', 'Attribute', 'Value', 'Comments_1', 'Comments_2', 'Action Item', 'Missing In']
        df_missing = df_missing.reindex(columns=final_columns)
        logging.info(f"[Missing Items] Final DataFrame prepared with {len(df_missing)} rows.")

        # Write the result to Excel
        df_missing.to_excel(output_path, sheet_name="Missing_Items", index=False)
        logging.info(f"[Missing Items] Wrote missing items to {output_path}")

        # --- Apply Color Formatting ---
        try:
            wb = load_workbook(output_path)
            ws = wb["Missing_Items"]

            # Define fonts and fills
            header_font = Font(bold=True)
            fill_header = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
            fill_gamma  = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # green
            fill_alfa   = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")  # blue

            # Identify header row and map header names to column indices
            header_row = next(ws.iter_rows(min_row=1, max_row=1))
            headers = {cell.value: cell.column for cell in header_row}
            for cell in header_row:
                cell.font = header_font
                cell.fill = fill_header

            # Required columns for coloring
            req_cols = ["Dimension", "NameID", "Attribute", "Value", "Missing In"]
            if not all(col in headers for col in req_cols):
                logging.warning("[Missing Items] One or more required columns for coloring are missing.")
            else:
                for row_idx in range(2, ws.max_row + 1):
                    missing_in = str(ws.cell(row=row_idx, column=headers["Missing In"]).value).strip().lower()
                    if missing_in == "gamma":
                        fill_color = fill_gamma
                    elif missing_in == "alfa":
                        fill_color = fill_alfa
                    else:
                        continue
                    # Apply the fill to the specified columns
                    ws.cell(row=row_idx, column=headers["Dimension"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["NameID"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Attribute"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Value"]).fill = fill_color

            # Freeze the header row
            ws.freeze_panes = ws["A2"]
            wb.save(output_path)
            logging.info(f"[Missing Items] Applied color formatting and froze top row in {output_path}")

        except Exception as e:
            logging.error(f"[Missing Items] Error applying color formatting: {e}")

    except Exception as e:
        logging.error(f"[Missing Items] Error creating missing items Excel: {e}")


# -----------------------------------------
# 7) MAIN FUNCTION
# -----------------------------------------
def main() -> None:
    try:
        # Setup logging
        log_file = Path("script.log")
        setup_logging(log_file)
        logging.info("Script started.")

        # 1) Read Exception Table (used for filtering/hiding and adding comments)
        exception_table_path = Path("Exception_Table.xlsx")
        df_exceptions = read_exception_table(exception_table_path)

        # 2) Process ALFA data
        alfa_pre_exclude = [("ColA", ["X"]), ("ColB", ["Y"])]
        alfa_bad_dims = ["UnwantedDim"]
        alfa_bad_attrs = ["Debug"]
        alfa_dim_rename = {"DimOld": "DimNew"}
        alfa_attr_rename = {"First": "Name"}
        alfa_path = Path("AlfaData.xlsx")
        df_alfa = transform_alfa(
            file_path=alfa_path,
            pre_melt_exclude_rules=alfa_pre_exclude,
            post_melt_bad_dimensions=alfa_bad_dims,
            post_melt_bad_attributes=alfa_bad_attrs,
            dimension_rename_dict=alfa_dim_rename,
            attribute_rename_dict=alfa_attr_rename,
            sheet_name="Sheet1",
            skip_rows=3
        )
        logging.info(f"[Alfa] Final rows: {len(df_alfa)}")

        # 3) Process GAMMA data
        gamma_pre_exclude = [("RawCol", ["Z"])]
        gamma_bad_dims = ["TestDim"]
        gamma_bad_attrs = ["BadAttr"]
        gamma_dim_rename = {"GammaOld": "GammaNew"}
        gamma_attr_rename = {"First": "Name"}
        gamma_zip = Path("GammaData.zip")
        df_gamma = transform_gamma(
            zip_file_path=gamma_zip,
            pre_melt_exclude_rules=gamma_pre_exclude,
            post_melt_bad_dimensions=gamma_bad_dims,
            post_melt_bad_attributes=gamma_bad_attrs,
            dimension_rename_dict=gamma_dim_rename,
            attribute_rename_dict=gamma_attr_rename,
            delimiter=",",
            remove_substring="_ceaster.txt"
        )
        logging.info(f"[Gamma] Final rows: {len(df_gamma)}")

        # 4) Create Missing Items Excel report
        output_excel = Path("Missing_Items.xlsx")
        create_missing_items_excel(df_alfa, df_gamma, df_exceptions, output_excel)

        logging.info("Script completed successfully.")

    except Exception as e:
        logging.critical(f"[Main] Critical error: {e}")


if __name__ == "__main__":
    main()
