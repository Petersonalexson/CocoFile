import logging
import pandas as pd
import warnings
from openpyxl import load_workbook

# -----------------------------------------------------------------------------
# 1. Setup Logging
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# -----------------------------------------------------------------------------
# 2. File Paths & Sheet Names
# -----------------------------------------------------------------------------
MAP_FILE_PATH    = r"C:\Users\alexp\OneDrive\Desktop\MAP.xlsx"
BANI_FILE_PATH   = r"C:\Users\alexp\OneDrive\Desktop\BANI.xlsx"

MAPPING_SHEET    = "Mapping Main"
XRP_SHEET        = "XRP"

AGGREGATED_FILE  = r"C:\Users\alexp\OneDrive\Desktop\Aggregated_Copywrite.xlsx"

# We'll filter for "Copywrite" in "Description 2"
TARGET_DESC2_VALUE = "Copywrite"

# -----------------------------------------------------------------------------
# 3. Load Mapping & Filter to `Description 2 == "Copywrite"`
# -----------------------------------------------------------------------------
logging.info("Loading mapping from MAP.xlsx...")
map_df = pd.read_excel(MAP_FILE_PATH, sheet_name=MAPPING_SHEET)

# Drop rows with no Account
map_df = map_df.dropna(subset=["Account"])

# Convert to string
map_df["Account"]       = map_df["Account"].astype(str).str.strip()
map_df["Description 2"] = map_df["Description 2"].astype(str).str.strip()

# Filter
copy_map = map_df[map_df["Description 2"] == TARGET_DESC2_VALUE]
if copy_map.empty:
    logging.warning(f"No rows found in mapping where 'Description 2' == '{TARGET_DESC2_VALUE}'. Exiting.")
    exit()

logging.info(f"Found {len(copy_map)} row(s) for '{TARGET_DESC2_VALUE}' in the mapping.")

# Build set of accounts
copywrite_accounts = set(copy_map["Account"].unique())

# -----------------------------------------------------------------------------
# 4. Load XRP Data
#    We'll keep: Nat Account, Voice, Amount, XO Number, Journal Description
# -----------------------------------------------------------------------------
logging.info(f"Loading XRP data from {BANI_FILE_PATH}...")
xrp_df = pd.read_excel(BANI_FILE_PATH, sheet_name=XRP_SHEET)

needed_cols = ["Nat Account", "Voice", "Amount", "XO Number", "Journal Description"]
xrp_df = xrp_df[needed_cols].dropna(subset=["Nat Account", "Voice", "Amount"])

# Convert types
xrp_df["Nat Account"]        = xrp_df["Nat Account"].astype(str).str.strip()
xrp_df["Voice"]              = xrp_df["Voice"].astype(str).str.strip()
xrp_df["XO Number"]          = xrp_df["XO Number"].fillna("").astype(str).str.strip()
xrp_df["Journal Description"] = xrp_df["Journal Description"].fillna("").astype(str).str.strip()
xrp_df["Amount"]             = xrp_df["Amount"].astype(float)

logging.info(f"Loaded {len(xrp_df)} rows from XRP sheet.")

# -----------------------------------------------------------------------------
# 5. Filter to "Copywrite" accounts, then merge in "Description 2"
#    This gives each row: (Account, Description2, Voice, Amount, XO Number, Journal Desc)
# -----------------------------------------------------------------------------
filtered_df = xrp_df[xrp_df["Nat Account"].isin(copywrite_accounts)].copy()
if filtered_df.empty:
    logging.info("No XRP rows match 'Copywrite' accounts. Exiting.")
    exit()

logging.info(f"Filtered to {len(filtered_df)} row(s) for Copywrite accounts.")

# We want each row to also have its "Description 2"
# Easiest is a left-merge on "Account"
filtered_df = filtered_df.merge(
    copy_map[["Account", "Description 2"]].drop_duplicates(),
    left_on="Nat Account",
    right_on="Account",
    how="left"
)

# Now we have columns:
#   Nat Account, Voice, Amount, XO Number, Journal Description,
#   Account (from merge), Description 2
#
# We'll rename "Account" to something else so we don't have duplicates
filtered_df.drop(columns=["Account"], inplace=True)  # Because "Nat Account" = "Account"

# We have "Nat Account" (the actual account) and "Description 2" from the mapping.

# -----------------------------------------------------------------------------
# 6. Group by (Nat Account, Description 2, Voice).
#    For each group, figure out a single XO Number (like the "Voice-based aggregator" approach).
# -----------------------------------------------------------------------------
grouped = filtered_df.groupby(["Nat Account", "Description 2", "Voice"], dropna=False)

# Example fallback from Journal Description
desc_to_xo_map = {
    "royalty": "9999",
    "special": "8888",
    # add more if needed
}

# We'll build these structures:
#   group_key -> chosen_xo
#   group_key -> sum_amount
group_key_to_xo = {}
group_key_to_amount = {}

for group_key, group_df in grouped:
    # group_key is (nat_account, desc2_value, voice_val)
    # group_df is all rows with that combination

    nat_acc, desc2_val, voice_val = group_key

    # Step A: find the XO Number for this group
    # If ANY row has a non-blank XO Number, we pick the first distinct
    distinct_xos = group_df["XO Number"][group_df["XO Number"] != ""].unique()
    chosen_xo = ""
    if len(distinct_xos) > 0:
        chosen_xo = distinct_xos[0]
        if len(distinct_xos) > 1:
            logging.warning(
                f"Group (Acc={nat_acc}, Desc2='{desc2_val}', Voice='{voice_val}') "
                f"has multiple XO Numbers {list(distinct_xos)}. Picking first: {chosen_xo}."
            )
    else:
        # if none found, try the Journal Description approach
        fallback_xo = ""
        for _, row in group_df.iterrows():
            jdesc_lower = row["Journal Description"].lower()
            for kw, forced_xo in desc_to_xo_map.items():
                if kw in jdesc_lower:
                    fallback_xo = forced_xo
                    break
            if fallback_xo:
                break
        chosen_xo = fallback_xo

    # Step B: sum the amounts
    total_amt = group_df["Amount"].sum()

    group_key_to_xo[group_key] = chosen_xo
    group_key_to_amount[group_key] = total_amt

logging.info(f"Processed {len(group_key_to_xo)} group(s).")

# -----------------------------------------------------------------------------
# 7. Now build a final aggregator DataFrame with columns:
#    [Nat Account, Description2, Voice, Final XO Number, Sum Amount]
# -----------------------------------------------------------------------------
rows = []
for group_key, sum_amt in group_key_to_amount.items():
    nat_acc, desc2_val, voice_val = group_key
    chosen_xo = group_key_to_xo[group_key]
    rows.append([
        nat_acc,
        desc2_val,
        voice_val,
        chosen_xo,  # final XO
        sum_amt
    ])

agg_df = pd.DataFrame(rows, columns=[
    "Account", "Description2", "Voice", "Final XO Number", "Sum Amount"
])

# If you want to skip groups that have no XO at all, filter them out:
# agg_df = agg_df[agg_df["Final XO Number"] != ""]

# Sort for readability
agg_df.sort_values(by=["Account", "Description2", "Voice", "Final XO Number"], inplace=True)

# -----------------------------------------------------------------------------
# 8. Write aggregator DataFrame to a new Excel
# -----------------------------------------------------------------------------
logging.info(f"Writing aggregator to '{AGGREGATED_FILE}'...")
agg_df.to_excel(AGGREGATED_FILE, sheet_name="Aggregator", index=False)
logging.info("Aggregated file created successfully.")

# Done
logging.info("All steps complete.")
