import os
import zipfile
import pandas as pd
from pathlib import Path
import logging
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font

# -----------------------------------------
# 0) CONFIGURE LOGGING
# -----------------------------------------
def setup_logging(log_file: Path):
    """
    Configures logging to output to both console and a log file.
    """
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Create handlers
    c_handler = logging.StreamHandler()
    f_handler = logging.FileHandler(log_file, mode='w')

    c_handler.setLevel(logging.INFO)
    f_handler.setLevel(logging.DEBUG)

    # Create formatters
    c_format = logging.Formatter('%(levelname)s: %(message)s')
    f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # Add formatters to handlers
    c_handler.setFormatter(c_format)
    f_handler.setFormatter(f_format)

    # Add handlers to the logger
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)

# -----------------------------------------
# 1) READ EXCLUSION TABLE
# -----------------------------------------
def read_exclusion_table(ex_table_path: Path) -> set:
    """
    Reads an Excel file with columns: [Key, hide exception].
    Returns a set of keys to exclude where hide exception == 'yes'.
    """
    try:
        if not ex_table_path.is_file():
            logging.warning(f"[Exclusion] File not found: {ex_table_path}. No exclusions used.")
            return set()

        df_ex = pd.read_excel(ex_table_path, sheet_name="Sheet1")
        if "Key" not in df_ex.columns or "hide exception" not in df_ex.columns:
            logging.warning("[Exclusion] Missing 'Key' or 'hide exception' columns. No exclusions.")
            return set()

        mask = df_ex["hide exception"].astype(str).str.lower() == "yes"
        excluded_keys = set(df_ex.loc[mask, "Key"].dropna().unique())
        logging.info(f"[Exclusion] Excluded Keys Count: {len(excluded_keys)}")
        return excluded_keys
    except Exception as e:
        logging.error(f"[Exclusion] Error reading exclusion table: {e}")
        return set()

# -----------------------------------------
# 1a) READ EXCEPTION TABLE
# -----------------------------------------
def read_exception_table(exception_table_path: Path) -> pd.DataFrame:
    """
    Reads an Excel file with columns: [Key, Comments_Gamma, Comments_Alfa].
    Returns a DataFrame with comments for each key.
    """
    try:
        if not exception_table_path.is_file():
            logging.warning(f"[Exception] File not found: {exception_table_path}. No comments added.")
            return pd.DataFrame(columns=["Key", "Comments_Gamma", "Comments_Alfa"])

        df_exc = pd.read_excel(exception_table_path, sheet_name="Sheet1")
        required_columns = {"Key", "Comments_Gamma", "Comments_Alfa"}
        if not required_columns.issubset(df_exc.columns):
            logging.warning("[Exception] Missing one or more required columns: 'Key', 'Comments_Gamma', 'Comments_Alfa'. No comments added.")
            return pd.DataFrame(columns=["Key", "Comments_Gamma", "Comments_Alfa"])

        logging.info(f"[Exception] Loaded comments for {len(df_exc)} keys.")
        return df_exc
    except Exception as e:
        logging.error(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame(columns=["Key", "Comments_Gamma", "Comments_Alfa"])

# -----------------------------------------
# 2) PRE-MELT FILTER (Remove Rows by Column)
# -----------------------------------------
def filter_pre_melt(df: pd.DataFrame, exclude_rules: list = None) -> pd.DataFrame:
    """
    exclude_rules: list of (columnName, [values_to_remove]).
      We remove any row if df[columnName] is in values_to_remove (logical OR across rules).
    Example: [("ColA", ["X"]), ("ColB", ["Y"])] => remove if ColA=="X" OR ColB=="Y".
    """
    try:
        if not exclude_rules:
            return df  # No filtering if none

        combined_mask = pd.Series(False, index=df.index)
        for (col, bad_values) in exclude_rules:
            if col in df.columns:
                mask = df[col].isin(bad_values)
                logging.debug(f"[Pre-Melt] Excluding rows where {col} in {bad_values}: {mask.sum()} rows")
                combined_mask |= mask
            else:
                logging.warning(f"[Pre-Melt] Column '{col}' not found in DataFrame.")

        # Keep rows where combined_mask is False
        df_out = df[~combined_mask]
        logging.info(f"[Pre-Melt] Rows after filtering: {len(df_out)}")
        return df_out
    except Exception as e:
        logging.error(f"[Pre-Melt] Error during pre-melt filtering: {e}")
        return df

# -----------------------------------------
# 3) POST-MELT EXCLUDE (Dimension/Attribute)
# -----------------------------------------
def exclude_dimension_attribute(df: pd.DataFrame,
                                bad_dimensions: list = None,
                                bad_attributes: list = None) -> pd.DataFrame:
    """
    Remove any row whose Dimension is in bad_dimensions OR Attribute is in bad_attributes.
    """
    try:
        initial_count = len(df)
        if bad_dimensions:
            df = df[~df["Dimension"].isin(bad_dimensions)]
            excluded_dims = initial_count - len(df)
            logging.debug(f"[Post-Melt] Excluded Dimensions {bad_dimensions}: {excluded_dims} rows removed")
        if bad_attributes:
            df = df[~df["Attribute"].isin(bad_attributes)]
            excluded_attrs = initial_count - len(df)
            logging.debug(f"[Post-Melt] Excluded Attributes {bad_attributes}: {excluded_attrs} rows removed")
        logging.info(f"[Post-Melt] Rows after exclusion: {len(df)}")
        return df
    except Exception as e:
        logging.error(f"[Post-Melt] Error during post-melt exclusion: {e}")
        return df

# -----------------------------------------
# 4) TRANSFORM ALFA
# -----------------------------------------
def transform_alfa(
    file_path: Path,
    excluded_keys: set,
    pre_melt_exclude_rules: list = None,
    post_melt_bad_dimensions: list = None,
    post_melt_bad_attributes: list = None,
    dimension_rename_dict: dict = None,
    attribute_rename_dict: dict = None,
    sheet_name: str = "Sheet1",
    skip_rows: int = 3
) -> pd.DataFrame:
    """
    Transforms the Alfa (Excel) data.
    """
    try:
        if not file_path.is_file():
            logging.error(f"[Alfa] File not found: {file_path}")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        # 1) Read Excel
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        logging.debug(f"[Alfa] Initial rows: {len(df)}")
        if df.shape[1] < 4:
            logging.warning("[Alfa] Fewer than 4 columns. Returning empty DataFrame.")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        # 2) Pre-melt exclude
        df = filter_pre_melt(df, pre_melt_exclude_rules)

        # 3) Rename columns => col[2]->Dimension, col[3]->NameID
        df.rename(columns={
            df.columns[2]: "Dimension",
            df.columns[3]: "NameID"
        }, inplace=True)
        logging.debug(f"[Alfa] Columns after renaming: {df.columns.tolist()}")

        # 4) Create 'Name' column equal to 'NameID' for inclusion as an attribute
        df['Name'] = df['NameID']
        logging.debug(f"[Alfa] Added 'Name' column:\n{df[['NameID', 'Name']].head()}")

        # 5) Melt
        id_vars = ["Dimension", "NameID"]
        val_vars = [c for c in df.columns if c not in id_vars]
        logging.debug(f"[Alfa] id_vars: {id_vars}, val_vars: {val_vars}")

        df_melt = df.melt(
            id_vars=id_vars,
            value_vars=val_vars,
            var_name="Attribute",
            value_name="Value"
        )
        logging.debug(f"[Alfa] Rows after melt: {len(df_melt)}")

        # 6) Rename dimension/attribute if needed
        if dimension_rename_dict:
            df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
            logging.debug(f"[Alfa] Dimensions after renaming: {df_melt['Dimension'].unique()}")
        if attribute_rename_dict:
            df_melt["Attribute"] = df_melt["Attribute"].replace(attribute_rename_dict)
            logging.debug(f"[Alfa] Attributes after renaming: {df_melt['Attribute'].unique()}")

        # 7) Exclude certain Dimensions or Attributes
        df_melt = exclude_dimension_attribute(
            df_melt,
            bad_dimensions=post_melt_bad_dimensions,
            bad_attributes=post_melt_bad_attributes
        )

        # 8) Build Key without NameID for grouping
        df_melt["Key_NoName"] = df_melt.apply(
            lambda row: f"{row['Dimension']} | {row['Attribute']} | {row['Value']}",
            axis=1
        )
        logging.debug(f"[Alfa] Sample Keys without NameID:\n{df_melt['Key_NoName'].head()}")

        # 9) Exclude rows based on Keys
        before_exclusion = len(df_melt)
        df_melt = df_melt[~df_melt["Key_NoName"].isin(excluded_keys)]
        after_exclusion = len(df_melt)
        logging.info(f"[Alfa] Excluded {before_exclusion - after_exclusion} rows based on excluded_keys.")

        # 10) Remove duplicates if necessary
        before_dedup = len(df_melt)
        df_melt.drop_duplicates(subset=["Key", "Key_NoName"], inplace=True)
        after_dedup = len(df_melt)
        logging.info(f"[Alfa] Removed {before_dedup - after_dedup} duplicate rows.")

        # 11) Final DataFrame
        final_df = df_melt[["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"]]
        logging.info(f"[Alfa] Final rows: {len(final_df)}")
        return final_df
    except Exception as e:
        logging.error(f"[Alfa] Error during transformation: {e}")
        return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

# -----------------------------------------
# 5) TRANSFORM GAMMA (Similar to ALFA)
# -----------------------------------------
def transform_gamma(
    zip_file_path: Path,
    excluded_keys: set,
    pre_melt_exclude_rules: list = None,
    post_melt_bad_dimensions: list = None,
    post_melt_bad_attributes: list = None,
    dimension_rename_dict: dict = None,
    attribute_rename_dict: dict = None,
    delimiter: str = ",",
    remove_substring: str = "_ceaster.txt"
) -> pd.DataFrame:
    """
    Transforms the Gamma (ZIP) data.
    """
    try:
        if not zip_file_path.is_file():
            logging.error(f"[Gamma] ZIP not found: {zip_file_path}")
            return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

        def compute_dimension_name(filename: str, remove_sub: str) -> str:
            base = os.path.basename(filename)
            if remove_sub in base:
                base = base.replace(remove_sub, "")
            else:
                base, _ = os.path.splitext(base)
            return base.replace("_", " ")

        all_dfs = []
        with zipfile.ZipFile(zip_file_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.endswith(".txt")]
            if not txt_files:
                logging.warning("[Gamma] No .txt files found in the ZIP.")
                return pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])

            for txt_file in txt_files:
                try:
                    # 1. Compute Dimension from filename
                    dimension_name = compute_dimension_name(txt_file, remove_substring)
                    logging.debug(f"[Gamma] Processing file: {txt_file}, Dimension: {dimension_name}")

                    # 2. Read the file into a DataFrame
                    with z.open(txt_file) as fo:
                        df_in = pd.read_csv(fo, delimiter=delimiter)
                    logging.debug(f"[Gamma] Initial rows in {txt_file}: {len(df_in)}")

                    if df_in.empty:
                        logging.warning(f"[Gamma] File {txt_file} is empty. Skipping.")
                        continue

                    # 3. Inspect columns
                    logging.debug(f"[Gamma] Columns in {txt_file}: {df_in.columns.tolist()}")

                    # 4. Rename the first column to 'NameID'
                    first_col = df_in.columns[0]
                    df_in.rename(columns={first_col: "NameID"}, inplace=True)
                    logging.debug(f"[Gamma] Columns after renaming: {df_in.columns.tolist()}")

                    # 5. Handle missing NameID values
                    if df_in["NameID"].isnull().any():
                        logging.warning(f"[Gamma] Some NameID values are blank in {txt_file}. Filling with 'Unknown'.")
                        df_in["NameID"] = df_in["NameID"].fillna("Unknown")

                    # 6. Apply Pre-Melt Exclusions
                    df_in = filter_pre_melt(df_in, pre_melt_exclude_rules)
                    logging.debug(f"[Gamma] Rows after pre-melt filtering in {txt_file}: {len(df_in)}")

                    # 7. Add Dimension Column
                    df_in["Dimension"] = dimension_name

                    # 8. Create 'Name' column equal to 'NameID' for inclusion as an attribute
                    df_in['Name'] = df_in['NameID']
                    logging.debug(f"[Gamma] Added 'Name' column in {txt_file}:\n{df_in[['NameID', 'Name']].head()}")

                    # 9. Melt Other Columns into Attributes
                    id_vars = ["Dimension", "NameID"]
                    val_vars = [c for c in df_in.columns if c not in id_vars]
                    logging.debug(f"[Gamma] id_vars: {id_vars}, val_vars: {val_vars}")

                    if not val_vars:
                        logging.warning(f"[Gamma] No value_vars to melt for {txt_file}. Skipping.")
                        continue

                    df_melt = df_in.melt(
                        id_vars=id_vars,
                        value_vars=val_vars,
                        var_name="Attribute",
                        value_name="Value"
                    )
                    logging.debug(f"[Gamma] Rows after melt in {txt_file}: {len(df_melt)}")

                    # 10. Rename dimension/attribute if needed
                    if dimension_rename_dict:
                        df_melt["Dimension"] = df_melt["Dimension"].replace(dimension_rename_dict)
                        logging.debug(f"[Gamma] Dimensions after renaming: {df_melt['Dimension'].unique()}")
                    if attribute_rename_dict:
                        df_melt["Attribute"] = df_melt["Attribute"].replace(attribute_rename_dict)
                        logging.debug(f"[Gamma] Attributes after renaming: {df_melt['Attribute'].unique()}")

                    # 11. Exclude certain Dimensions or Attributes
                    df_melt = exclude_dimension_attribute(
                        df_melt,
                        bad_dimensions=post_melt_bad_dimensions,
                        bad_attributes=post_melt_bad_attributes
                    )

                    # 12. Build Key without NameID for grouping
                    df_melt["Key_NoName"] = df_melt.apply(
                        lambda row: f"{row['Dimension']} | {row['Attribute']} | {row['Value']}",
                        axis=1
                    )
                    logging.debug(f"[Gamma] Sample Keys without NameID:\n{df_melt['Key_NoName'].head()}")

                    # 13. Exclude rows based on Keys
                    before_exclusion = len(df_melt)
                    df_melt = df_melt[~df_melt["Key_NoName"].isin(excluded_keys)]
                    after_exclusion = len(df_melt)
                    logging.info(f"[Gamma] Excluded {before_exclusion - after_exclusion} rows based on excluded_keys in {txt_file}.")

                    # 14. Remove duplicates if necessary
                    before_dedup = len(df_melt)
                    df_melt.drop_duplicates(subset=["Key", "Key_NoName"], inplace=True)
                    after_dedup = len(df_melt)
                    logging.info(f"[Gamma] Removed {before_dedup - after_dedup} duplicate rows in {txt_file}.")

                    # 15. Final DataFrame
                    final_df = df_melt[["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"]]
                    all_dfs.append(final_df)
                except Exception as e:
                    logging.error(f"[Gamma] Error processing file {txt_file}: {e}")
                    continue

        df_gamma = pd.DataFrame(columns=["Key_NoName", "Key", "Dimension", "NameID", "Attribute", "Value"])
        if all_dfs:
            try:
                df_gamma = pd.concat(all_dfs, ignore_index=True)
                logging.info(f"[Gamma] Total rows after concatenation: {len(df_gamma)}")
            except Exception as e:
                logging.error(f"[Gamma] Error concatenating DataFrames: {e}")
        else:
            logging.warning("[Gamma] No data collected from ZIP.")

        return df_gamma
    # -----------------------------------------
    # 6) CREATE MISSING ITEMS EXCEL
    # -----------------------------------------
    def create_missing_items_excel(
        df_alfa: pd.DataFrame,
        df_gamma: pd.DataFrame,
        df_exceptions: pd.DataFrame,
        output_path: Path
    ):
        """
        Identifies all missing items in Alfa and Gamma, includes comments, and compiles them into a single table.
        The table includes:
        - Key
        - Dimension
        - NameID
        - Attribute
        - Value
        - Comments
        - Action Item
        Only includes:
        - Missing NameID entries within matching dimensions.
        - Missing other properties only if NameID is present in both datasets.
        """
        try:
            logging.info("[Missing Items] Identifying matching dimensions between Alfa and Gamma...")
            # Identify dimensions present in both Alfa and Gamma
            dimensions_alfa = set(df_alfa['Dimension'].unique())
            dimensions_gamma = set(df_gamma['Dimension'].unique())
            matching_dimensions = dimensions_alfa.intersection(dimensions_gamma)
            logging.info(f"[Missing Items] Matching Dimensions Count: {len(matching_dimensions)}")

            # Filter Alfa and Gamma to only include matching dimensions
            df_alfa_matching = df_alfa[df_alfa['Dimension'].isin(matching_dimensions)].copy()
            df_gamma_matching = df_gamma[df_gamma['Dimension'].isin(matching_dimensions)].copy()

            logging.info("[Missing Items] Identifying missing NameIDs and other missing items...")
            # Group Alfa by Key_NoName and get set of NameIDs
            alfa_group = df_alfa_matching.groupby("Key_NoName")["NameID"].apply(set).reset_index()
            alfa_group.rename(columns={"NameID": "NameIDs_Alfa"}, inplace=True)

            # Group Gamma by Key_NoName and get set of NameIDs
            gamma_group = df_gamma_matching.groupby("Key_NoName")["NameID"].apply(set).reset_index()
            gamma_group.rename(columns={"NameID": "NameIDs_Gamma"}, inplace=True)

            # Merge Alfa and Gamma groups
            merged_group = alfa_group.merge(gamma_group, on="Key_NoName", how="inner")
            logging.debug(f"[Missing Items] Merged group sample:\n{merged_group.head()}")

            # Identify missing NameIDs in Gamma
            merged_group["Missing_In_Gamma"] = merged_group.apply(
                lambda row: row["NameIDs_Alfa"] - row["NameIDs_Gamma"],
                axis=1
            )

            # Identify missing NameIDs in Alfa
            merged_group["Missing_In_Alfa"] = merged_group.apply(
                lambda row: row["NameIDs_Gamma"] - row["NameIDs_Alfa"],
                axis=1
            )

            # Prepare list for missing items
            missing_items = []

            for _, row in merged_group.iterrows():
                key_noname = row["Key_NoName"]
                missing_in_gamma = row["Missing_In_Gamma"]
                missing_in_alfa = row["Missing_In_Alfa"]

                # If there are missing NameIDs in Gamma or Alfa, collect them and skip other missing items in this Key_NoName
                if missing_in_gamma or missing_in_alfa:
                    # For each missing NameID in Gamma
                    for nameid in missing_in_gamma:
                        # Find the original details from Alfa
                        details = df_alfa_matching[
                            (df_alfa_matching['NameID'] == nameid) &
                            (df_alfa_matching['Key_NoName'] == key_noname)
                        ]
                        if not details.empty:
                            details = details.iloc[0]
                            missing_items.append({
                                "Key": details["Key"],
                                "Dimension": details["Dimension"],
                                "NameID": nameid,
                                "Attribute": details["Attribute"],
                                "Value": details["Value"],
                                "Missing In": "Gamma"
                            })

                    # For each missing NameID in Alfa
                    for nameid in missing_in_alfa:
                        # Find the original details from Gamma
                        details = df_gamma_matching[
                            (df_gamma_matching['NameID'] == nameid) &
                            (df_gamma_matching['Key_NoName'] == key_noname)
                        ]
                        if not details.empty:
                            details = details.iloc[0]
                            missing_items.append({
                                "Key": details["Key"],
                                "Dimension": details["Dimension"],
                                "NameID": nameid,
                                "Attribute": details["Attribute"],
                                "Value": details["Value"],
                                "Missing In": "Alfa"
                            })
                    # Skip processing other missing items in this Key_NoName since NameID is missing
                    continue

                # Else, if no NameID is missing, identify other missing items
                # Compare the full keys (Key_NoName | NameID) to find missing items
                alfa_keys = set(df_alfa_matching[df_alfa_matching['Key_NoName'] == key_noname]['Key'])
                gamma_keys = set(df_gamma_matching[df_gamma_matching['Key_NoName'] == key_noname]['Key'])

                # Items in Alfa not in Gamma
                missing_in_gamma_other = alfa_keys - gamma_keys
                for key in missing_in_gamma_other:
                    details = df_alfa_matching[df_alfa_matching['Key'] == key].iloc[0]
                    missing_items.append({
                        "Key": details["Key"],
                        "Dimension": details["Dimension"],
                        "NameID": details["NameID"],
                        "Attribute": details["Attribute"],
                        "Value": details["Value"],
                        "Missing In": "Gamma"
                    })

                # Items in Gamma not in Alfa
                missing_in_alfa_other = gamma_keys - alfa_keys
                for key in missing_in_alfa_other:
                    details = df_gamma_matching[df_gamma_matching['Key'] == key].iloc[0]
                    missing_items.append({
                        "Key": details["Key"],
                        "Dimension": details["Dimension"],
                        "NameID": details["NameID"],
                        "Attribute": details["Attribute"],
                        "Value": details["Value"],
                        "Missing In": "Alfa"
                    })

            # Create DataFrame from missing items
            df_missing = pd.DataFrame(missing_items)
            logging.info(f"[Missing Items] Total missing items after processing: {len(df_missing)}")

            if df_missing.empty:
                logging.info("[Missing Items] No missing items to report.")
                return

            # Merge with exception comments
            df_missing = df_missing.merge(
                df_exceptions,
                on='Key',
                how='left'
            )
            logging.info(f"[Missing Items] Merged with exception comments. Rows: {len(df_missing)}")

            # Assign 'Comments' based on where it's missing
            df_missing['Comments'] = df_missing.apply(
                lambda row: row['Comments_Gamma'] if row['Missing In'] == 'Gamma' else row['Comments_Alfa'],
                axis=1
            )

            # Select relevant columns and add 'Action Item'
            df_missing = df_missing[['Key', 'Dimension', 'NameID', 'Attribute', 'Value', 'Comments']]
            df_missing['Action Item'] = ''

            # Reorder columns
            final_columns = ['Key', 'Dimension', 'NameID', 'Attribute', 'Value', 'Comments', 'Action Item']
            df_missing = df_missing[final_columns]
            logging.info(f"[Missing Items] Final DataFrame with selected columns has {len(df_missing)} rows.")

            # Write to Excel
            df_missing.to_excel(output_path, sheet_name="Missing_Items", index=False)
            logging.info(f"[Missing Items] Wrote missing items to {output_path}")

            # --- Apply Color Formatting ---
            try:
                wb = load_workbook(output_path)
                ws = wb["Missing_Items"]

                # Define fills
                fill_header = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")  # Light Gray for header
                fill_key = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")     # Light Blue for Key column

                # Define font
                bold_font = Font(bold=True)

                # Map headers to column indices
                header_row = next(ws.iter_rows(min_row=1, max_row=1))
                headers = {cell.value: cell.column for cell in header_row}

                # Apply bold font and header fill
                for cell in header_row:
                    cell.font = bold_font
                    cell.fill = fill_header

                # Color 'Key' column
                key_col = headers.get("Key")
                if key_col:
                    for row in ws.iter_rows(min_row=2, min_col=key_col, max_col=key_col, max_row=ws.max_row):
                        cell = row[0]
                        cell.fill = fill_key

                # Ensure 'Comments' and 'Action Item' columns have white background
                comments_col = headers.get("Comments")
                action_col = headers.get("Action Item")
                white_fill = PatternFill(start_color="FFFFFF", end_color="FFFFFF", fill_type="solid")  # White

                if comments_col:
                    for row in ws.iter_rows(min_row=2, min_col=comments_col, max_col=comments_col, max_row=ws.max_row):
                        cell = row[0]
                        cell.fill = white_fill

                if action_col:
                    for row in ws.iter_rows(min_row=2, min_col=action_col, max_col=action_col, max_row=ws.max_row):
                        cell = row[0]
                        cell.fill = white_fill

                # Freeze the top row
                ws.freeze_panes = ws["A2"]

                # Save the colored Excel
                wb.save(output_path)
                logging.info(f"[Missing Items] Applied color formatting and froze top pane in {output_path}")
            except Exception as e:
                logging.error(f"[Missing Items] Error applying color formatting: {e}")

        except Exception as e:
            logging.error(f"[Missing Items] Error creating missing items Excel: {e}")

# -----------------------------------------
# 7) MAIN
# -----------------------------------------
def main():
    try:
        # Setup logging
        log_file = Path("script.log")
        setup_logging(log_file)
        logging.info("Script started.")

        # 1) Read Exclusion Table
        ex_table_path = Path("Ex_Table.xlsx")
        excluded_keys = read_exclusion_table(ex_table_path)

        # 1a) Read Exception Table
        exception_table_path = Path("Exception_Table.xlsx")  # Ensure this file exists with required columns
        df_exceptions = read_exception_table(exception_table_path)

        # 2) ALFA
        #    Example: remove rows pre-melt if 'ColA'=="X" or 'ColB'=="Y"
        alfa_pre_exclude = [
            ("ColA", ["X"]),
            ("ColB", ["Y"])
        ]
        #    Remove Dimension="UnwantedDim", Attribute="Debug" post-melt
        alfa_bad_dims = ["UnwantedDim"]
        alfa_bad_attrs = ["Debug"]
        #    Rename dimension or attribute if needed
        alfa_dim_rename = {
            "DimOld": "DimNew"
        }
        alfa_attr_rename = {
            "First": "Name"  # Ensures 'Name' is an attribute
        }

        alfa_path = Path("AlfaData.xlsx")
        df_alfa = transform_alfa(
            file_path=alfa_path,
            excluded_keys=excluded_keys,
            pre_melt_exclude_rules=alfa_pre_exclude,
            post_melt_bad_dimensions=alfa_bad_dims,
            post_melt_bad_attributes=alfa_bad_attrs,
            dimension_rename_dict=alfa_dim_rename,
            attribute_rename_dict=alfa_attr_rename,
            sheet_name="Sheet1",
            skip_rows=3
        )
        logging.info(f"[Alfa] Final rows: {len(df_alfa)}")

        # 3) GAMMA
        #    Example: remove rows pre-melt if 'RawCol'=="Z"
        gamma_pre_exclude = [
            ("RawCol", ["Z"])
        ]
        #    Remove dimension="TestDim" or attribute="BadAttr" post-melt
        gamma_bad_dims = ["TestDim"]
        gamma_bad_attrs = ["BadAttr"]
        gamma_dim_rename = {
            "GammaOld": "GammaNew"
        }
        gamma_attr_rename = {
            "First": "Name"  # Ensures 'Name' is an attribute
        }

        gamma_zip = Path("GammaData.zip")
        df_gamma = transform_gamma(
            zip_file_path=gamma_zip,
            excluded_keys=excluded_keys,
            pre_melt_exclude_rules=gamma_pre_exclude,
            post_melt_bad_dimensions=gamma_bad_dims,
            post_melt_bad_attributes=gamma_bad_attrs,
            dimension_rename_dict=gamma_dim_rename,
            attribute_rename_dict=gamma_attr_rename,
            delimiter=",", 
            remove_substring="_ceaster.txt"
        )
        logging.info(f"[Gamma] Final rows: {len(df_gamma)}")

        # 4) Create Missing Items Excel
        comparison_out = Path("Missing_Items.xlsx")
        create_missing_items_excel(df_alfa, df_gamma, df_exceptions, comparison_out)

        logging.info("Script completed successfully.")
    except Exception as e:
        logging.critical(f"[Main] Critical error: {e}")

if __name__ == "__main__":
    main()
