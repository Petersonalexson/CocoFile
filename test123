
"""
Script to compare Alfa and Gamma source data.

Data is read from:
  - An Excel file (Alfa) where the third and fourth columns are assumed to be 
    Dimension and NameID respectively.
  - A ZIP file (Gamma) containing .txt (CSV) files.

Each record is "melted" so that all attributes appear as rows. In particular,
a "Name" attribute is forced (set from the NameID) to serve as a reference.
Data is filtered before melting (pre-exclude) and after melting (bad dims/attrs are dropped)
using configurable rules.

In the final output (an Excel report), records are grouped by Dimension and NameID.
If the "Name" row is missing in one source then only that missing "Name" row is reported.
Otherwise, differences in the other (non-"Name") attributes are reported.

Optionally, an exceptions table (with Key, Comments_1, Comments_2, and hide exception flag)
is merged so that you can hide or annotate certain rows.

Color formatting is applied:
  - Blue for rows missing in Alfa.
  - Green for rows missing in Gamma.

Modify the parameters in the main() function as needed.
"""

import os
import zipfile
import pandas as pd
from pathlib import Path
import logging
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font

# ---------------------------
# 0) SETUP LOGGING
# ---------------------------
def setup_logging(log_file: Path) -> None:
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    c_handler = logging.StreamHandler()
    f_handler = logging.FileHandler(log_file, mode='w')
    c_handler.setLevel(logging.INFO)
    f_handler.setLevel(logging.DEBUG)
    c_format = logging.Formatter('%(levelname)s: %(message)s')
    f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    c_handler.setFormatter(c_format)
    f_handler.setFormatter(f_format)
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)

# ---------------------------
# 1) PRE-MELT FILTER
# ---------------------------
def filter_pre_melt(df: pd.DataFrame, exclude_rules: list = None) -> pd.DataFrame:
    """
    Apply pre-melt filtering rules.
    
    Parameters:
      - exclude_rules: list of tuples (column_name, [bad_values])
        Any row with a value in the given column equal to one of the bad_values is dropped.
    """
    if not exclude_rules:
        return df
    combined_mask = pd.Series(False, index=df.index)
    for col, bad_values in exclude_rules:
        if col in df.columns:
            mask = df[col].isin(bad_values)
            logging.debug(f"[Pre-Melt] Excluding {mask.sum()} rows from column '{col}' with values in {bad_values}")
            combined_mask |= mask
        else:
            logging.warning(f"[Pre-Melt] Column '{col}' not found in DataFrame.")
    return df[~combined_mask]

# ---------------------------
# 2) POST-MELT FILTER (Exclude Bad Dimensions/Attributes)
# ---------------------------
def exclude_dimension_attribute(df: pd.DataFrame, bad_dimensions: list = None, bad_attributes: list = None) -> pd.DataFrame:
    """
    Exclude rows based on bad dimensions or attributes.
    
    Parameters:
      - bad_dimensions: list of dimension names to drop.
      - bad_attributes: list of attribute names to drop.
    """
    if bad_dimensions:
        initial = len(df)
        df = df[~df["Dimension"].isin(bad_dimensions)]
        logging.debug(f"[Post-Melt] Excluded {initial - len(df)} rows based on bad dimensions {bad_dimensions}")
    if bad_attributes:
        initial = len(df)
        df = df[~df["Attribute"].isin(bad_attributes)]
        logging.debug(f"[Post-Melt] Excluded {initial - len(df)} rows based on bad attributes {bad_attributes}")
    return df

# ---------------------------
# 3) TRANSFORM ALFA
# ---------------------------
def transform_alfa(file_path: Path,
                   pre_melt_exclude_rules: list = None,
                   bad_dimensions: list = None,
                   bad_attributes: list = None,
                   dimension_rename: dict = None,
                   attribute_rename: dict = None,
                   sheet_name: str = "Sheet1",
                   skip_rows: int = 3) -> pd.DataFrame:
    """
    Reads and transforms Alfa (Excel) data.
    
    Assumes that:
      - The third column is Dimension.
      - The fourth column is NameID.
    A "Name" column is forced (equal to NameID) so that when melting,
    a row with Attribute "Name" is produced.
    
    Pre-melt filtering is applied first. After melting, rows with bad dimensions or attributes
    are dropped. Finally, renaming dictionaries are applied.
    """
    try:
        if not file_path.is_file():
            logging.error(f"[Alfa] File not found: {file_path}")
            return pd.DataFrame()
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)
        logging.info(f"[Alfa] Loaded {len(df)} rows from Excel.")
        
        # Pre-melt filtering.
        df = filter_pre_melt(df, pre_melt_exclude_rules)
        
        # Rename columns: assume third column is Dimension and fourth is NameID.
        df.rename(columns={df.columns[2]: "Dimension", df.columns[3]: "NameID"}, inplace=True)
        
        # Force a "Name" column from NameID.
        df["Name"] = df["NameID"]
        
        # Melt the DataFrame.
        id_vars = ["Dimension", "NameID"]
        value_vars = [col for col in df.columns if col not in id_vars]
        melted = df.melt(id_vars=["Dimension", "NameID"], value_vars=value_vars,
                         var_name="Attribute", value_name="Value")
        
        # Apply renaming (if provided).
        if dimension_rename:
            melted["Dimension"] = melted["Dimension"].replace(dimension_rename)
        if attribute_rename:
            melted["Attribute"] = melted["Attribute"].replace(attribute_rename)
        
        # Exclude rows with bad dimensions or attributes.
        melted = exclude_dimension_attribute(melted, bad_dimensions, bad_attributes)
        
        # Create a grouping key: Dimension | NameID.
        melted["GroupKey"] = (melted["Dimension"].astype(str).str.strip() +
                              " | " +
                              melted["NameID"].astype(str).str.strip())
        
        # Build a full key string for each row: "Dimension | NameID | Attribute | Value"
        def build_full_key(row):
            return f"{row['Dimension'].strip()} | {row['NameID'].strip()} | {row['Attribute'].strip()} | {str(row['Value']).strip()}"
        melted["Key"] = melted.apply(build_full_key, axis=1)
        
        melted.drop_duplicates(inplace=True)
        logging.info(f"[Alfa] Transformed data to {len(melted)} rows after melting.")
        return melted
    except Exception as e:
        logging.error(f"[Alfa] Error during transformation: {e}")
        return pd.DataFrame()

# ---------------------------
# 4) TRANSFORM GAMMA
# ---------------------------
def transform_gamma(zip_file_path: Path,
                    pre_melt_exclude_rules: list = None,
                    bad_dimensions: list = None,
                    bad_attributes: list = None,
                    dimension_rename: dict = None,
                    attribute_rename: dict = None,
                    delimiter: str = ",",
                    remove_substring: str = "_ceaster.txt") -> pd.DataFrame:
    """
    Reads and transforms Gamma data from a ZIP file containing .txt (CSV) files.
    
    For each file:
      - The filename (with an optional substring removed) defines the Dimension.
      - The first column is renamed to "NameID" and used to force a "Name" attribute.
      - Pre-melt filtering is applied.
      - The DataFrame is melted, then filtered for bad dimensions/attributes,
        and renaming dictionaries are applied.
    """
    try:
        if not zip_file_path.is_file():
            logging.error(f"[Gamma] ZIP file not found: {zip_file_path}")
            return pd.DataFrame()
        all_dfs = []
        with zipfile.ZipFile(zip_file_path, "r") as z:
            txt_files = [f for f in z.namelist() if f.endswith(".txt")]
            if not txt_files:
                logging.warning("[Gamma] No .txt files found in the ZIP.")
                return pd.DataFrame()
            for txt_file in txt_files:
                try:
                    # Compute Dimension from the filename.
                    base = os.path.basename(txt_file)
                    if remove_substring in base:
                        base = base.replace(remove_substring, "")
                    else:
                        base, _ = os.path.splitext(base)
                    dimension = base.replace("_", " ").strip()
                    
                    # Read CSV data.
                    with z.open(txt_file) as fo:
                        df = pd.read_csv(fo, delimiter=delimiter)
                    if df.empty:
                        logging.warning(f"[Gamma] File {txt_file} is empty. Skipping.")
                        continue
                    
                    # Rename the first column to "NameID".
                    first_col = df.columns[0]
                    df.rename(columns={first_col: "NameID"}, inplace=True)
                    df["NameID"] = df["NameID"].fillna("Unknown")
                    
                    # Pre-melt filtering.
                    df = filter_pre_melt(df, pre_melt_exclude_rules)
                    
                    # Add Dimension column.
                    df["Dimension"] = dimension
                    
                    # Force a "Name" column.
                    df["Name"] = df["NameID"]
                    
                    # Melt the DataFrame.
                    melted = df.melt(id_vars=["Dimension", "NameID"],
                                     var_name="Attribute", value_name="Value")
                    
                    # Apply renaming if provided.
                    if dimension_rename:
                        melted["Dimension"] = melted["Dimension"].replace(dimension_rename)
                    if attribute_rename:
                        melted["Attribute"] = melted["Attribute"].replace(attribute_rename)
                    
                    # Exclude rows with bad dimensions or attributes.
                    melted = exclude_dimension_attribute(melted, bad_dimensions, bad_attributes)
                    
                    # Create GroupKey: Dimension | NameID.
                    melted["GroupKey"] = (melted["Dimension"].astype(str).str.strip() +
                                          " | " +
                                          melted["NameID"].astype(str).str.strip())
                    
                    # Build the full key.
                    def build_full_key(row):
                        return f"{row['Dimension'].strip()} | {row['NameID'].strip()} | {row['Attribute'].strip()} | {str(row['Value']).strip()}"
                    melted["Key"] = melted.apply(build_full_key, axis=1)
                    
                    melted.drop_duplicates(inplace=True)
                    logging.info(f"[Gamma] Processed {txt_file} into {len(melted)} rows.")
                    all_dfs.append(melted)
                except Exception as inner_e:
                    logging.error(f"[Gamma] Error processing file {txt_file}: {inner_e}")
                    continue
        if all_dfs:
            df_gamma = pd.concat(all_dfs, ignore_index=True)
            logging.info(f"[Gamma] Combined transformed data has {len(df_gamma)} rows.")
            return df_gamma
        else:
            logging.warning("[Gamma] No valid data found in ZIP.")
            return pd.DataFrame()
    except Exception as e:
        logging.error(f"[Gamma] Error during transformation: {e}")
        return pd.DataFrame()

# ---------------------------
# 5) CREATE MISSING ITEMS EXCEL
# ---------------------------
def create_missing_items_excel(df_alfa: pd.DataFrame,
                               df_gamma: pd.DataFrame,
                               df_exceptions: pd.DataFrame,
                               output_path: Path) -> None:
    """
    Compares Alfa and Gamma records (grouped by Dimension and NameID) and outputs only keys
    (rows) that are missing in one source.
    
    For each group:
      - If the group is missing entirely in one source, the missing "Name" row from that source is reported.
      - If both groups exist but one source is missing the "Name" attribute, that missing row is reported
        (and differences on non-"Name" attributes are ignored).
      - Otherwise, only differences in non-"Name" attributes are reported.
    
    Optionally, the exceptions table (if provided) is merged (to hide rows or add comments).
    Color formatting is applied in the final Excel (blue for missing in Alfa, green for missing in Gamma).
    """
    try:
        logging.info("[Missing Items] Starting comparison between Alfa and Gamma.")
        
        # Ensure key columns are strings.
        for df in [df_alfa, df_gamma]:
            df["Dimension"] = df["Dimension"].fillna("").astype(str)
            df["NameID"] = df["NameID"].fillna("").astype(str)
            df["Attribute"] = df["Attribute"].fillna("").astype(str)
        
        # Get the set of all groups.
        groups_alfa = set(df_alfa["GroupKey"].unique())
        groups_gamma = set(df_gamma["GroupKey"].unique())
        all_groups = groups_alfa.union(groups_gamma)
        
        missing_items = []
        for group in all_groups:
            alfa_group = df_alfa[df_alfa["GroupKey"] == group]
            gamma_group = df_gamma[df_gamma["GroupKey"] == group]
            
            # If one source is missing the entire group, report the "Name" row from the other.
            if alfa_group.empty and not gamma_group.empty:
                name_rows = gamma_group[gamma_group["Attribute"].str.lower() == "name"]
                if not name_rows.empty:
                    row = name_rows.iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Alfa"
                    })
                continue
            if gamma_group.empty and not alfa_group.empty:
                name_rows = alfa_group[alfa_group["Attribute"].str.lower() == "name"]
                if not name_rows.empty:
                    row = name_rows.iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Gamma"
                    })
                continue
            
            # Both groups exist. Check for presence of the "Name" attribute.
            alfa_has_name = not alfa_group[alfa_group["Attribute"].str.lower() == "name"].empty
            gamma_has_name = not gamma_group[gamma_group["Attribute"].str.lower() == "name"].empty
            if not alfa_has_name or not gamma_has_name:
                if not alfa_has_name and gamma_has_name:
                    row = gamma_group[gamma_group["Attribute"].str.lower() == "name"].iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Alfa"
                    })
                if not gamma_has_name and alfa_has_name:
                    row = alfa_group[alfa_group["Attribute"].str.lower() == "name"].iloc[0]
                    missing_items.append({
                        "Key": row["Key"],
                        "Dimension": row["Dimension"],
                        "NameID": row["NameID"],
                        "Attribute": "Name",
                        "Value": row["Value"],
                        "Missing In": "Gamma"
                    })
                continue  # Skip further attribute comparisons for this group.
            
            # Both groups have a "Name" row; compare non-"Name" attributes.
            alfa_attrs = set(alfa_group[alfa_group["Attribute"].str.lower() != "name"]["Attribute"].str.lower())
            gamma_attrs = set(gamma_group[gamma_group["Attribute"].str.lower() != "name"]["Attribute"].str.lower())
            missing_in_gamma = alfa_attrs - gamma_attrs
            missing_in_alfa = gamma_attrs - alfa_attrs
            for attr in missing_in_gamma:
                row = alfa_group[alfa_group["Attribute"].str.lower() == attr].iloc[0]
                missing_items.append({
                    "Key": row["Key"],
                    "Dimension": row["Dimension"],
                    "NameID": row["NameID"],
                    "Attribute": row["Attribute"],
                    "Value": row["Value"],
                    "Missing In": "Gamma"
                })
            for attr in missing_in_alfa:
                row = gamma_group[gamma_group["Attribute"].str.lower() == attr].iloc[0]
                missing_items.append({
                    "Key": row["Key"],
                    "Dimension": row["Dimension"],
                    "NameID": row["NameID"],
                    "Attribute": row["Attribute"],
                    "Value": row["Value"],
                    "Missing In": "Alfa"
                })
        
        df_missing = pd.DataFrame(missing_items)
        logging.info(f"[Missing Items] Total missing items: {len(df_missing)}")
        
        if df_missing.empty:
            logging.info("[Missing Items] No missing items found.")
            return
        
        # Merge with exceptions table if provided.
        if not df_exceptions.empty:
            df_missing["Key"] = df_missing["Key"].astype(str).str.strip().str.lower()
            df_exceptions["Key"] = df_exceptions["Key"].astype(str).str.strip().str.lower()
            df_missing = df_missing.merge(df_exceptions, on="Key", how="left")
            df_missing["hide exception"] = df_missing["hide exception"].astype(str).str.lower().fillna("no")
            df_missing = df_missing[df_missing["hide exception"] != "yes"]
        
        # Add an Action Item column.
        df_missing["Action Item"] = ""
        final_columns = ["Key", "Dimension", "NameID", "Attribute", "Value",
                         "Comments_1", "Comments_2", "Action Item", "Missing In"]
        df_missing = df_missing.reindex(columns=final_columns)
        
        # Write to Excel.
        df_missing.to_excel(output_path, sheet_name="Missing_Items", index=False)
        logging.info(f"[Missing Items] Written output to {output_path}")
        
        # --- Apply Color Formatting ---
        try:
            wb = load_workbook(output_path)
            ws = wb["Missing_Items"]
            header_font = Font(bold=True)
            fill_header = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
            fill_gamma = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # green for Gamma
            fill_alfa = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")   # blue for Alfa

            header_row = next(ws.iter_rows(min_row=1, max_row=1))
            headers = {cell.value: cell.column for cell in header_row}
            for cell in header_row:
                cell.font = header_font
                cell.fill = fill_header

            required_cols = ["Dimension", "NameID", "Attribute", "Value", "Missing In"]
            if not all(col in headers for col in required_cols):
                logging.warning("[Missing Items] One or more required columns for coloring are missing.")
            else:
                for row_idx in range(2, ws.max_row + 1):
                    missing_in = str(ws.cell(row=row_idx, column=headers["Missing In"]).value).strip().lower()
                    if missing_in == "gamma":
                        fill_color = fill_gamma
                    elif missing_in == "alfa":
                        fill_color = fill_alfa
                    else:
                        continue
                    ws.cell(row=row_idx, column=headers["Dimension"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["NameID"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Attribute"]).fill = fill_color
                    ws.cell(row=row_idx, column=headers["Value"]).fill = fill_color

            ws.freeze_panes = ws["A2"]
            wb.save(output_path)
            logging.info("[Missing Items] Applied color formatting to Excel.")
        except Exception as e:
            logging.error(f"[Missing Items] Error applying color formatting: {e}")
    
    except Exception as e:
        logging.error(f"[Missing Items] Error during creation of missing items Excel: {e}")

# ---------------------------
# 6) READ EXCEPTION TABLE
# ---------------------------
def read_exception_table(exception_file: Path) -> pd.DataFrame:
    """
    Reads an Excel file containing exceptions.
    Expected columns: Key, Comments_1, Comments_2, hide exception.
    """
    try:
        if not exception_file.is_file():
            logging.warning(f"[Exception] Exception file {exception_file} not found.")
            return pd.DataFrame()
        df = pd.read_excel(exception_file, sheet_name="Sheet1")
        required_cols = {"Key", "Comments_1", "Comments_2", "hide exception"}
        if not required_cols.issubset(set(df.columns)):
            logging.warning(f"[Exception] Missing required columns: {required_cols - set(df.columns)}")
            return pd.DataFrame()
        logging.info(f"[Exception] Loaded {len(df)} exception rows.")
        return df
    except Exception as e:
        logging.error(f"[Exception] Error reading exception table: {e}")
        return pd.DataFrame()

# ---------------------------
# 7) MAIN FUNCTION
# ---------------------------
def main() -> None:
    try:
        log_file = Path("script.log")
        setup_logging(log_file)
        logging.info("Script started.")
        
        # Exception table (optional)
        exception_file = Path("Exception_Table.xlsx")
        df_exceptions = read_exception_table(exception_file)
        
        # --- ALFA configuration ---
        alfa_file = Path("AlfaData.xlsx")
        alfa_pre_exclude = [("ColA", ["X"]), ("ColB", ["Y"])]  # Pre-melt filtering rules.
        alfa_bad_dims = ["UnwantedDim"]        # Rows with these Dimension values will be dropped.
        alfa_bad_attrs = ["Debug"]               # Rows with these Attribute values will be dropped.
        alfa_dimension_rename = {"DimOld": "DimNew"}   # Rename dimensions if needed.
        alfa_attribute_rename = {"First": "Name"}        # Rename attributes if needed.
        df_alfa = transform_alfa(alfa_file,
                                  pre_melt_exclude_rules=alfa_pre_exclude,
                                  bad_dimensions=alfa_bad_dims,
                                  bad_attributes=alfa_bad_attrs,
                                  dimension_rename=alfa_dimension_rename,
                                  attribute_rename=alfa_attribute_rename,
                                  sheet_name="Sheet1",
                                  skip_rows=3)
        logging.info(f"[Alfa] Transformed data contains {len(df_alfa)} rows.")
        
        # --- GAMMA configuration ---
        gamma_zip = Path("GammaData.zip")
        gamma_pre_exclude = [("RawCol", ["Z"])]  # Pre-melt filtering rules.
        gamma_bad_dims = ["TestDim"]             # Rows with these Dimension values will be dropped.
        gamma_bad_attrs = ["BadAttr"]              # Rows with these Attribute values will be dropped.
        gamma_dimension_rename = {"GammaOld": "GammaNew"}
        gamma_attribute_rename = {"First": "Name"}
        df_gamma = transform_gamma(gamma_zip,
                                   pre_melt_exclude_rules=gamma_pre_exclude,
                                   bad_dimensions=gamma_bad_dims,
                                   bad_attributes=gamma_bad_attrs,
                                   dimension_rename=gamma_dimension_rename,
                                   attribute_rename=gamma_attribute_rename,
                                   delimiter=",",
                                   remove_substring="_ceaster.txt")
        logging.info(f"[Gamma] Transformed data contains {len(df_gamma)} rows.")
        
        # --- Create Missing Items Excel Report ---
        output_file = Path("Missing_Items.xlsx")
        create_missing_items_excel(df_alfa, df_gamma, df_exceptions, output_file)
        
        logging.info("Script completed successfully.")
    except Exception as e:
        logging.critical(f"[Main] Critical error: {e}")

if __name__ == "__main__":
    main()
